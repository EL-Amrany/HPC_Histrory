Uni.lu High Performance Computing (HPC) Tutorials

 Copyright (c) 2013-2021 UL HPC Team <hpc-team@uni.lu>
This repository holds a set of tutorials to help the users of the UL HPC platform to better understand or simply use our platform.

This is the main page of the documentation for this repository, which relies on MkDocs and the Read the Docs theme. In particular, the latest version of these tutorials is available online:

http://ulhpc-tutorials.rtfd.io

The list of proposed tutorials is continuously evolving. They used on a regular basis during the UL HPC School we organise.

ULHPC School

So far, the following tutorials are proposed:

Category	Description	Level
Pre-requisites and Setup instructions	beginners
Basic	Introduction to UNIX/Linux Shell and Command lines	beginners
Getting Started on the UL HPC platform	beginners
Monitoring & Profiling I: why, what, how, where to look	beginners
Emb. Parallel Jobs	GNU Parallel	beginners
HPC Management of Sequential and Embarrassingly Parallel Jobs	beginners
Scheduling	Advanced scheduling with SLURM	intermediate
Software Management	HPC Software Building: optimizing and complementing the ULHPC software set	beginners
The Spack package manager for supercomputers	beginners
Data Management	Data Management on UL HPC Facility	beginners
Debuging & Profiling	Know Your Bugs: Weapons for Efficient Debugging	intermediate
Advanced debugging on the UL HPC platform	intermediate
(OLD) Unified profiling and debugging with Allinea	intermediate
(OLD) Direct, Reverse and parallel Memory debugging with TotalView	intermediate
MPI	Scalable Science and Parallel computations with OpenMP/MPI	intermediate
OSU Micro-Benchmarks	intermediate
High-Performance Linpack (HPL) benchmarking on UL HPC platform	intermediate
HPCG benchmarking on UL HPC platform	intermediate
Python	Prototyping with Python	beginners
Python: Use Jupyter notebook on UL HPC	intermediate
Use Python Celery on UL HPC	advanced
Scalable computing with Dask	advanced
Parallel machine learning with scikit-learn	intermediate
Parallel evolutionary computing with Scoop/Deap	intermediate
Mathematics	MATLAB (interactive, passive and sequential jobs)	intermediate
Advanced MATLAB execution: checkpointing and parallel jobs	advanced
R / Statictical Computing	intermediate
Cplex / Gurobi	intermediate
Bioinformatics	Bioinformatics software on the UL HPC platform	intermediate
Galaxy Introduction Exercise: From Peaks to Genes	intermediate
CFD/MD/Chemistry	Scalable Science II (Advanced - Computational Physics, Chemistry & Engineering apps)	advanced
Big Data	Running Big Data Application using Apache Hadoop and Spark	intermediate
Machine/Deep Learning	Machine and Deep learning workflows	intermediate
Distributed Deep Learning with Horovod	intermediate
Distributed Large Language Model with HuggingFace and DeepSpeed	advanced
Containers	Singularity	advanced
Singularity with Infiniband	advanced
Reproducibility and HPC Containers	advanced
Virtualization	(OLD) Create and reproduce work environments using Vagrant	intermediate
Deploying virtual machines with Vm5k on Grid'5000	intermediate
GPU Programming	Introduction to GPU programming with CUDA (C/C++)	intermediate
Image Convolution with GPU and CUDA	advanced
Introduction to OpenCL	intermediate
Introduction to OpenACC Programming Model (C/C++ and Fortran)	intermediate
Solving the Laplace Equation on GPU with OpenAcc	advanced
IPU Programming	Tensorflow on Graphcore IPU	intermediate
Misc	Reproducible Research at the Cloud Era	intermediate
List of contributors

See docs/contacts.md.
In the advent where you want to contribute yourself to these tutorials, do not hesitate! See below for instructions.
Issues / Feature request

You can submit bug / issues / feature request using the ULHPC/tutorials Tracker.
See also docs/contributing/ for further information.
Contributing

If you want to contribute to the code, you shall be aware of the way this module is organized.
These elements are detailed on docs/contributing.md.
You are more than welcome to contribute to its development by sending a pull request.
Online Documentation

Read the Docs aka RTFD hosts documentation for the open source community and the ULHPC/sysadmins has its documentation (see the docs/ directly) hosted on readthedocs.
See docs/rtfd.md for more details.
Licence

This project and the sources proposed within this repository are released under the terms of the GPL-3.0 licence.

Latest UL HPC School (program)
Inaugural Keynote



The practical sessions of the Nov 2021 HPC School are listed below

PS	Description	Speaker	Duration
1	Introduction to UNIX/Linux Shell and Command lines	A. Olloh, T. Valette, H. Cartiaux	4h45
Pre-requisites and Setup instructions		
2	Preliminaries (SSH - OpenOnDemand)	A. Olloh, T. Valette	1h30
3	Getting Started 2.0 Introduction to the SLURM Job Scheduler, basic launchers	H. Cartiaux	1h45
4	HPC Management of Sequential and Embarrassingly parallel jobs	S. Varrette	1h30
Distributing embarrassingly parallel tasks GNU Parallel		
5	HPC Software Building: optimizing and complementing the ULHPC software set	S. Varrette	50min
6	Prototyping with python	S. Peter	2h30
7	HPC Containers with Singularity	E. Kieffer	45min
8	Advanced distributed computing with python and Dask	E. Kieffer	1h35
Parallel machine learning with scikit-learn		
Parallel evolutionary computing with Scoop/Deap		
9	Scalable Science with OpenMP/MPI	S. Varrette	1h20
Keynote / Advanced Performance Engineering for Efficient Parellel Debugging	X. Besseron	45min
10	Introduction to OpenACC Programming Model (C/C++ and Fortran)	E. Krishnasamy, L. Koutsantonis, T. Pessoa	1h45min
Solving the Laplace Equation on GPU with OpenACC		
Introduction to OpenCL Programming (C/C++)		
11	R - Statistical computing	A. Ginolac	2h30
Keynote / Data management (backup, security)	S. Peter	1h00
12	Big Data Analytics: Batch, Stream and Hybrid processing engines	S. Varrette	55min
Contributors

As initiator and coordinator of this event since the first edition in 2014, Dr. Varrette would like to express his deep appreciation and heartfelt thanks to the below contributors (in alphabetical order) which permit for this 11th edition to take place and (hopefully) to be yet another big success.

This directory holds the documentation to properly setup the ULHPC/tutorials on your machine or once connected on the UL HPC platform.

In particular, you should check the following resources:

Pre-requisites / Preliminary software to install the mandatory software.

This is only required for your personal laptop.
These steps are NOT applicable once connected on the UL HPC Platform
Instructions to clone and setup a local working copy of this repository

/!\ IMPORTANT: this is a set of hands-on tutorials: participants are expected to bring a laptop and pre-install software in advance to make the best use of time during the proposed tutorials. If for some reason you are unable to fulfill this pre-requisite, try to seat close to an attendee that is able to perform these tasks.

Note: in the following instructions, terminal commands are prefixed by a virtual prompt $>which obviously does not belong to the command.

Online accounts

Kindly create in advance the various accounts for the cloud services we might use, i.e.:

Github
Vagrant Cloud
Docker Hub
UL HPC account

You need to have an account on our platform. See https://hpc-docs.uni.lu/accounts/

Software List

The following software should be installed, depending on your running platform. Detailed instructions for each OS are depicted below.

Platform	Software	Description	Usage
Mac OS	Homebrew	The missing package manager for macOS	brew install ...
Mac OS	iTerm2	enhanced Terminal	
Windows	Chocolatey	Package Manager for Windows	choco install ...
Windows	Windows Subsystem for Linux (WSL)	Emulation-like translation of Linux kernel system calls	
Windows	Ubuntu over WSL	Linux Ubuntu on Windows (recommended)	
Windows	Windows Terminal		
Windows	MobaXTERM	Terminal with tabbed SSH client	
Windows	SourceTree	(optional) enhanced git GUI	
Windows/Linux	Virtual Box	Free hypervisor provider for Vagrant	
Windows/Linux	Vagrant	Reproducible environments made easy.	
Linux	Docker for Ubuntu	Lightweight Reproducible Containers	
Windows	Docker for Windows	Lightweight Reproducible Containers	
Follow the below custom instructions depending on your running platform and Operating System.

Microsoft Windows

Chocolatey: The Package Manager for Windows

Follow Installation instructions on https://chocolatey.org/ - install it as an administrator PowerShell. You'll probably need to reboot your laptop.

Chocolatey Installation

With PowerShell, you must ensure Get-ExecutionPolicy is not Restricted.

Right click on the Windows starting boutton and choose Windows PowerShell
Run these three commands
Get-ExecutionPolicy
### if it returns  Restricted then go to the next step
Set-ExecutionPolicy AllSigned   ## or Set-ExecutionPolicy Bypass -Scope Process
Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://chocolatey.org/install.ps1'))
Then use chocolatey to install a software
choco.exe install virtualbox
WSL and Ubuntu over Windows

The Windows Subsystem for Linux makes this all possible by providing emulation-like translation of Linux kernel system calls to the Windows kernel. So when a Linux app like Bash executes and makes system calls, the Windows Subsystem for Linux translates those calls into Windows system calls and Bash executes just like on Linux.

Resources:
Install the Windows Subsystem for Linux
Run Bash, SSH and other Linux Apps on Windows 10
Using WSL and MobaXterm to Create a Linux Dev Environment on Windows
WSL Installation

In the Start Menu: select "Turn Windows features on or off"

Tick "Windows Subsystem for Linux"
reboot to complete the install


Then Enable the Developer mode

In Start Menu: select "Developers Settings"
Turn on Developer Mode
Now you can Install Ubuntu within the Microsoft Store



Search for Ubuntu and install it
create a new UNIX username (prefer to follow the guidelines on ULHPC facility: first letter of your firstname, followed by your lastname, all in lower case. Ex for John Doe: jdoe)
add the new password
Your windows system drive are exposed in the /mnt/ directory
Ex: C:\Users\MyLogin\Downloads\File.txt will be located under /mnt/c/Users/MyLogin/Downloads/File.txt
BEWARE that your Linux file system is stored in a hidden folder, under %userprofile%\AppData\Local\Packages. In particular for Ubuntu, the files are located under the CanonicalGroupLimited.UbuntuonWindows_<hash>\LocalState\rootfs folder. Note that you're NOT supposed to tamper these files. But, if you need to view or back up some files, you'll find them stored in a hidden folder.

Ubuntu Image Customization

In the Ubuntu bash, you may want to install of-my-zsh and the Powerlevel10k prompt following the following guide:

Setting up Windows Subsystem for Linux with zsh + oh-my-zsh
You will need to enable by default the good font (top left window icon / Properties / Fonts)

Microsoft Terminal

You probably want to install then Windows Terminal It offers the ability to use multiple shell environment in one terminal.

Install the Windows Terminal from the Microsoft Store. Other useful link
Install the Cascadia Code PL font
Install the Meslo LGM NF font
Other useful link
Changing the Powershell prompt

Install posh-git and oh-my-posh:

Install-Module posh-git -Scope CurrentUser
Install-Module oh-my-posh -Scope CurrentUser
Install-Module -Name PSReadLine -Scoope CurrentUser -Force -SkipPublisherCheck 
Set-PoshPrompt -Theme Agnoster
If you get an error message "Running scripts is disabled on this system", you have to change the PowerShell execution policy which doesn't allow to run scripts:

Get-ExecutionPolicy -List
Set-ExecutionPolicy RemoteSigned -Scope CurrentUser
To make these changes permanent, append the following lines in the Powershell profile (run notepad $PROFILE):

Import-Module posh-git
Import-Module oh-my-posh
Set-PoshPrompt -Theme Agnoster
You will also need to enable the Cascadia Code PL font by adding into the Windows Terminal Parameters the following lines in the settings.json files under the default

"defaults":
{
    "font": 
            {
                "face": "MesloLGM NF"
            }
        },
        "list": 
        [
            {
                "commandline": "powershell.exe",
                "guid": "{61c54bbd-c2c6-5271-96e7-009a87ff44bf}",
                "hidden": false,
                "name": "Windows PowerShell"
                "fontFace": "MesloLGM NF",
            },
        ]
},
MobaXterm to Run Graphical Linux Apps

Now you can install MobaXterm using chocolatey (within an administrator Powershell -- You may need to enable the appropriate powerline font by defaults in the Properties of the Administrator Powershell):

$ choco.exe install mobaxterm          # Enhanced X11 Terminal for Windows
$ choco.exe install vcxsrv
Alternatively, you can consider using VcXsrv as an X-server yet our training sessions will assume you rely on MobaXterm. However if you wish to install VcXsrv to natively run X application from the windows terminal, proceed as follows:

Run XLaunch Wizard
Accept the default options BUT pay attention to Save the configuration file at the last step in %appdata%\Microsoft\Windows\Start Menu\Programs\Startup (eventually search for the 'Start Menu' directory in the explorer).
DENY (cancel) the Firewall request
Every time you run MobaXterm, make sure that the X server is running - check it by clicking on the associate button. Select the WSL/Ubuntu profile. We need to configure WSL to send the display of its graphical apps over to Windows. Failing to do so will result in the fact that graphical apps will attempt to load natively inside of WSL and nothing will show up.

To do that, we need to set the DISPLAY environment variable within WSL. Run nano ~/.profile to append the following lines:

# Check WSL version with 'wsl.exe -l -v'
# If using WSL 1:
export DISPLAY=:0
# If using WSL 2:
export DISPLAY="$(/sbin/ip route | awk '/default/ { print $3 }'):0"
IDE / Programming Editor

You probably want to install the following editors/IDE:

Sublime Text 3 -- see this configuration of Sublime Text inside of WSL

In MobaXterm, follow the instructions provided below in the Linux section
Test it with subl .
Check out this Sublime Text 3 package configuration
VSCode (Visual Studio Code)
PyCharm
Useful applications install

Then, while most of the below software are covered in the trainings, if you want a fast setup, once you have Chocolatey installed, run the following within an administrator Powershell:

$ choco.exe install git gitflow-avh    # (newer) Git stuff
$ choco.exe install mobaxterm          # Enhanced X11 Terminal for Windows
$ choco.exe install virtualbox         # install virtualbox -- see https://www.virtualbox.org/
$ choco.exe install vagrant            # install Vagrant    -- see https://www.vagrantup.com/downloads.html
$ choco.exe install docker-desktop     # install Docker -- https://docs.docker.com/engine/installation/mac/
Update to WSL 2

Better performances of your Linux subsystem can be obtained by migrating to WSL 2.

In an administrator Powershell, enable the Virtual Machine Platform

Enable-WindowsOptionalFeature -Online -FeatureName VirtualMachinePlatform
Then you can get the list of your WSL systems and convert them as follows:

$ wsl -l -v
  NAME     STATE     VERSION
* Ubuntu   Stopped   1
# Convert -- Adapt the Distribution name accordinaly
$ wsl --set-version Ubuntu 2
For setting all future distributions to use WSL 2, you will need to use the following command:

$ wsl --set-default-version 2
Finally, verify that your changes worked:

$ wsl -l -v    # OR wsl --list --verbose
  NAME     STATE     VERSION
* Ubuntu   Stopped   2
Mac OS X

Resources:
Configuring Mac OS
Install iterm2 and Homebrew Once you have Homebrew installed:

$ brew install git git-flow    # (newer) Git stuff
$ brew install mkdocs               # (optional) install mkdocs
$ brew install pyenv pyenv-virtualenv direnv # see https://varrette.gforge.uni.lu/tutorials/pyenv.html
$ brew install virtualbox      # install virtualbox -- see https://www.virtualbox.org/
$ brew install vagrant         # install Vagrant    -- see https://www.vagrantup.com/downloads.html
$ brew install vagrant-manager # see http://vagrantmanager.com/
$ brew install docker          # install Docker -- https://docs.docker.com/engine/installation/mac/
# Note that you probably want to install Firefox, Chrome etc. with brew
$ brew install firefox
$ brew install google-chrome
Note: later on, you might wish to use the following shell function to update the software installed using Homebrew.

bup () {
    echo "Updating your [Homebrew] system"
    brew update
    brew upgrade
    brew cu
    brew cleanup
    brew cask cleanup
}
Linux (Debian / Ubuntu)

# Adapt the package names (and package manager) in case you are using another Linux distribution.
$ sudo apt-get update
$ sudo apt-get install git git-flow build-essential
$ sudo apt-get install rubygems virtualbox vagrant virtualbox-dkms
For Docker, choose your distribution from https://docs.docker.com/engine/installation/linux/ and follow the instructions. You need a reasonably new kernel version (3.10 or higher). Here are detailed instuctions per OS:

Ubuntu
Debian
CentOS
You may want to install Sublime Text:

wget -qO - https://download.sublimetext.com/sublimehq-pub.gpg | sudo apt-key add -
sudo apt-get install apt-transport-https
echo "deb https://download.sublimetext.com/ apt/stable/" | sudo tee /etc/apt/sources.list.d/sublime-text.list
sudo apt-get update
sudo apt-get install sublime-text
Check out this Sublime Text 3 package configuration:

git clone https://github.com/nickjj/sublime-text-3-packages.git ~/.config/sublime-text-3
Then test the configuration with subl .

Post-Installations checks

Git:

(Eventually) Make yourself known to Git

$ git config –-global user.name  "Firstname LastName"              # Adapt accordingly
$ git config –-global user.email "Firstname.LastName@domain.org"   # Adapt with your mail
# Eventually, if you have a GPG key, use the public key to sign your commits/tags
$ git config --global user.helper osxkeychain       # Only on Mac OS
$ git config --global user.signingkey <fingerprint> # Ex: git config --global user.signingkey 5D08BCDD4F156AD7
# you can get your key fingerprint (prefixed by 0x) with 'gpg -K --fingerprint | grep sec'
To clone and install this repository, follow the installation instructions.

Vagrant

Ensure that vagrant is running and has the appropriate plugins from the command line

$ vagrant --version
Vagrant 2.2.13
Docker (only required for containers tutorials)

Launch the Docker app and then check that the Docker works:

$ docker info
Containers: 9
 Running: 0
 Paused: 0
 Stopped: 9
Images: 12
Server Version: 18.03.1-ce
[...]
Pull the docker containers we might need for the concerned tutorial
$ docker pull centos
Login onto you Docker hub account (take note of your Docker Hub ID and password).
With docker installed, run
$ docker login -u <your docker hub ID>
and enter your password.

Note that if the Docker installation fails, you can use http://play-with-docker.com/ to try Docker, but it won't work if all of us try it once! So use it only as a last resort, and it is up to you to use any important information (like the Docker hub account) inside it.

Mkdocs

It probably makes sense to install  mkdocs to be able to generate locally the current documentation.

Follow for that the instructions provided on the ../rtfd.md.

First of all, ensure you have installed the Pre-requisites / Preliminary software and followed the corresponding configuration.

Then this repository is hosted on Github. Assuming you have installed and configured git:

Cloning the ULHPC git repository

To clone this repository, proceed as follows (adapt accordingly):

$> mkdir -p ~/git/github.com/ULHPC
$> cd ~/git/github.com/ULHPC
$> git clone https://github.com/ULHPC/tutorials.git
$> cd tutorials
# /!\ IMPORTANT: run 'make setup' only **AFTER** Pre-requisites software are installed
To setup you local copy of this repository (after pre-requisites are satisfied), simply run:

$> make setup    # Under Mac OS / Linux
This will initiate the Git submodules of this repository (see .gitmodules) and setup the git flow layout for this repository. Later on, you can update your local branches by running:

 $> make up
If upon pulling the repository, you end in a state where another collaborator have upgraded the Git submodules for this repository, you'll end in a dirty state (as reported by modifications within the .submodules/ directory). In that case, just after the pull, you have to run make up to ensure consistency with regards the Git submodules.

Finally, you can upgrade the Git submodules to the latest version by running:

$> make upgrade
Python Virtualenv / Pyenv and Direnv

If you want to perform a local rendering of this documentation, a few Python packages are required to be installed (see requirements.txt). You will have to ensure you have installed direnv (configured by .envrc), pyenv and pyenv-virtualenv. This assumes also the presence of ~/.config/direnv/direnvrc from this page - for more details, see this blog post.

You can run the following command to setup your local machine in a compliant way:

make setup-direnv
make setup-pyenv
Adapt your favorite shell configuration as suggested. You may want to add the following:

for f in $XDG_CONFIG_HOME/*/init.sh; do
  . ${f}
done
Running direnv allow (this will have to be done only once), you should automatically enable the virtualenv ulhpc-docs based on the python version specified in .python-version. You'll eventually need to install the appropripriate Python version with pyenv:

pyenv versions   # Plural: show all versions
pyenv install $(head .python-version)
# Activate the virtualenv by reentering into the directory
cd ..
cd -
From that point, you should install the required packages using:

make setup-python

# OR (manually)
pip install --upgrade pip
pip install -r requirements.txt
You should now be able to preview the documentation locally (as rendered on readthedocs).

Documentation

See docs/.

The documentation for this project is handled by mkdocs. You might wish to generate locally the docs:

Install mkdocs and the mandatory package from the requirements.txt file (ideally within a virtual environment as above)
Preview your documentation from the project root by running mkdocs serve and visit with your favorite browser the URL http://localhost:8000
Alternatively, you can run make doc at the root of the repository.
(eventually) build the full documentation locally (in the site/ directory) by running mkdocs build.
Prepare for Tutorial sessions

To take the best out the tutorial sessions proposed during the HPC school, you probably wish on your homedir on the cluster to

clone (or update) the ULHPC/tutorials as instructed above
work in a separate directory structure when following a given event. Here is a suggested approach:
# First time: clone the repo under a meaningfull path
$> mkdir -p ~/git/github.com/ULHPC
$> cd ~/git/github.com/ULHPC
$> git clone https://github.com/ULHPC/tutorials.git
$> cd tutorials
$> make setup

# Next times: pull latest changes
$> cd ~/git/github.com/ULHPC/tutorials
$> make up     # update both branches (production and devel)

# Prepare a dedicated (separated) working directory
$> mkdir -p ~/tutorials/ULHPC-School-2021         # Adapt event name accordingly
$> cd ~/tutorials/ULHPC-School-2021
$> ln -s ~/git/github.com/ULHPC/tutorials  ref.d  # create a symbolic link pointing to the tutorial reference material
# Now  $HOME/tutorials/ULHPC-School-2021/ref.d/ points to reference training material

Introducing the UNIX/Linux Shell

 Copyright (c) 2013-2021 UL HPC Team <hpc-sysadmins@uni.lu>
The use of the shell is fundamental to a wide range of advanced computing tasks, including high-performance computing. These lessons will introduce you to this powerful tool. We will cover a range of basic commands that will help you navigate and explore, create files and directories, write script, read and concatenate, how to use command arguments/options and combine existing commands, as well as copy, move and remove files.



Very first steps on UNIX/Linux Shell and Command lines

If you're not already comfortable manipulating files and directories, searching for files with grep and find, and writing simple loops and scripts. Please follow this lesson Unix Shell from Software Carpentry (from which we made our slides).

You'll learn within a command shell to:

move around on your computer
see what files and directories you have
specify the location of a file or directory on your computer
create, copy, and delete files and directories
edit files
combine existing commands to do new things
perform the same actions on many different files
save and re-use commands
find files and find things in them
write scripts
Having this basic UNIX/Linux Shell knowledge is the first requirement to use efficiently an HPC facility.

Editors

Nano

$ nano <path/filename>

quit and save: CTRL+x
save: CTRL+o
highlight text: Alt+a
Cut the highlighted text: CTRL+k
Paste: CTRL+u
Vim

$ vim <path/filename>

There are 2 main modes:

Edition mode: press i or insert once
Command mode: press ESC once
Here is a short list of useful commands:

save: :w
save and quit: :wq
quit and discard changes: :q!
search: /<pattern>
search & replace: :%s/<pattern>/<replacement>/g
jump to line 100: :100
highlight text: CTRL+V
cut the highlighted text: d
cut one line: dd
paste: p
undo: u
Persistent Terminal Sessions using GNU Screen

GNU Screen is a tool to manage persistent terminal sessions. It becomes interesting since you will probably end at some moment with the following scenario:

you frequently program and run computations on the UL HPC platform i.e on a remote Linux/Unix computer, typically working in six different terminal logins to the access server from your office workstation, cranking up long-running computations that are still not finished and are outputting important information (calculation status or results), when you have 2 interactive jobs running... But it's time to catch the bus and/or the train to go back home.

Probably what you do in the above scenario is to

a. clear and shutdown all running terminal sessions

b. once at home when the kids are in bed, you're logging in again... And have to set up the whole environment again (six logins, 2 interactive jobs etc. )

c. repeat the following morning when you come back to the office.

Enter the long-existing and very simple, but totally indispensable GNU screen command. It has the ability to completely detach running processes from one terminal and reattach it intact (later) from a different terminal login.

Note that screen is not available anymore on modern system, especially when using the Aion cluster, you should use Tmux instead.

Pre-requisite: screen configuration file ~/.screenrc

While not mandatory, we advise you to rely on our customized configuration file for screen .screenrc available on Github.

Otherwise, simply clone the ULHPC dotfile repository and make a symbolic link ~/.screenrc targeting the file screen/screenrc of the repository.

Screen commands

You can start a screen session (i.e. creates a single window with a shell in it) with the screen command. Its main command-lines options are listed below:

screen: start a new screen
screen -ls: does not start screen, but prints a list of pid.tty.host strings identifying your current screen sessions.
screen -r: resumes a detached screen session
screen -x: attach to a not detached screen session. (Multi display mode i.e. when you and another user are trying to access the same session at the same time)
Once within a screen, you can invoke a screen command which consist of a "CTRL + a" sequence followed by one other character. The main commands are:

CTRL + a c: (create) creates a new Screen window. The default Screen number is zero.
CTRL + a n: (next) switches to the next window.
CTRL + a p: (prev) switches to the previous window.
CTRL + a d: (detach) detaches from a Screen
CTRL + a A: (title) rename the current window
CTRL + a 0-9: switches between windows 0 through 9.
CTRL + a k or CTRL + d: (kill) destroy the current window
CTRL + a ?: (help) display a list of all the command options available for Screen.
Persistent Terminal Sessions using Tmux

Tmux is a more modern equivalent to GNU screen.

Pre-requisite: screen configuration file ~/.tmuxrc

While not mandatory, we advise you to rely on our customized configuration file for tmux .tmuxrc available on Github.

Otherwise, simply clone the ULHPC dotfile repository and make a symbolic link ~/.tmuxrc targeting the file tmux/tmuxrc of the repository.

Tmux commands

You can start a tmux session (i.e. creates a single window with a shell in it) with the tmux command. Its main command-lines options are listed below:

tmux: start a new tmux session
tmux ls: does not start tmux, but print the list of the existing sessions.
tmux a: resumes a detached tmux session
Once within a tmux, you can invoke a tmux command which consist of a "CTRL + b" sequence followed by one other character. The main commands are:

CTRL + b c: (create) creates a new tmux window. The default tmux number is zero.
CTRL + b n: (next) switches to the next window.
CTRL + b p: (prev) switches to the previous window.
CTRL + b d: (detach) detaches from a session
CTRL + b ,: (title) rename the current window
CTRL + b 0-9: switches between windows 0 through 9.
CTRL + d: (kill) destroy the current window
CTRL + b ?: (help) display a list of all the command options available for tmux.

Preliminaries to ULHPC facility access

 Copyright (c) 2020-2021 UL HPC Team <hpc-team@uni.lu>


Welcome to the High Performance Computing (HPC) Facility of the University of Luxembourg (ULHPC)!

To take the best out of the different tutorials and practical sessions proposed in the training sessions organised by the University of Luxembourg, you have to follow several steps to configure your working environments. In particular, ensure you have followed the preliminary setup instructions for your laptop.

Convention

In the below tutorial, you'll proposed terminal commands where the prompt is denoted by $>.

In general, we will prefix to precise the execution context (i.e. your laptop, a cluster frontend or a node). Remember that # character is a comment. Example:

            # This is a comment
            $> hostname

            (laptop)$> hostname         # executed from your personal laptop / workstation

            (access-iris)$> hostname    # executed from access server of the Iris cluster
Git Installation

# Mac OS X, using Homebrew - https://brew.sh
brew install git git-gui git-flow gitk tig kdiff3

# Ubuntu / WSL / RedHat
{ sudo apt | dnf } install git-core git-flow tig gitk kdiff3

# Windows, using Chocolatey - https://chocolatey.org/
$> choco.exe install git gitflow-avh
Git Initial Setup

# /!\ To add to your bash/zsh profile (~/.profile)
# XDG Base Directory Specification
# See https://specifications.freedesktop.org/basedir-spec/latest/
(laptop)$> export XDG_CONFIG_HOME=$HOME/.config
(laptop)$> export XDG_CACHE_HOME=$HOME/.cache
(laptop)$> export XDG_DATA_HOME=$HOME/.local/share

# Create the directories
mkdir -p ~/.config/git $XDG_CACHE_HOME $XDG_DATA_HOME

# Basic Git defaults
(laptop)$> git config --global user.name "Firstname LastName"
(laptop)$> git config --global user.email "<email>@<domain>"
(laptop)$> git config --global user.signingkey <gpg-keyID>     # <-- Leave this part if you don't have a gpg-keyID
(laptop)$> git config --global color.ui true

# Set your default editor -- vim in this case
(laptop)$> git config ---global core.editor vim
Secure SHell (SSH)

Access / SSH Tutorial
The way SSH handles the keys and the configuration files is illustrated in the following figure:

SSH key management

Developed by SSH Communications Security Ltd., Secure Shell is a program to log into another computer over a network, to execute commands in a remote machine, and to move files from one machine to another in a secure way.

It provides strong authentication and secure communications over insecure channels. To use SSH, you have to generate a pair of keys, one public and the other private. The public key authentication is the most secure and flexible approach to ensure a multi-purpose transparent connection to a remote server. You will learn here how to generate an SSH key pair, authorize its public part on the ULHPC portal, allowing to connect and transfer data securely data toward the ULHPC facility.

SSH Key Generation

(Again) ensure your have followed the preliminary setup instructions as the below guideline is common to ALL platforms (including Windows assuming you have configure Ubuntu over WSL)

Open a Terminal. SSH is installed natively on your machine and the ssh command should be accessible from the command line:

(laptop)$> ssh -V
OpenSSH_8.8p1, OpenSSL 1.1.1l
SSH Key Management

You can check all available SSH keys on your computer by running the following command on your terminal:

(laptop)$> for key in ~/.ssh/id_*; do ssh-keygen -l -f "${key}"; done | uniq
Your SSH keys might use one of the following algorithms:

DSA: It's unsafe and even no longer supported since OpenSSH version 7, you need to upgrade it!
RSA: OK if the key size has 3072 or 4096-bit length -- 1024-bit length is considered unsafe.
Ed25519: It’s the most recommended public-key algorithm available today
Default RSA Key Pair

To generate an RSA SSH keys of 4096-bit length, just use the ssh-keygen command as follows:

(laptop)$> ssh-keygen -t rsa -b 4096 -a 100
Generating public/private rsa key pair.
Enter file in which to save the key (/home/user/.ssh/id_rsa):
Enter passphrase (empty for no passphrase):
Enter same passphrase again:
Your identification has been saved in /home/user/.ssh/id_rsa.
Your public key has been saved in /home/user/.ssh/id_rsa.pub.
The key fingerprint is:
fe:e8:26:df:38:49:3a:99:d7:85:4e:c3:85:c8:24:5b username@yourworkstation
The key's randomart image is:
+---[RSA 4096]----+
|                 |
|      . E        |
|       * . .     |
|      . o . .    |
|        S. o     |
|       .. = .    |
|       =.= o     |
|      * ==o      |
|       B=.o      |
+-----------------+
IMPORTANT: To ensure the security of your key-pair, you MUST protect your SSH keys with a passphrase!

After the execution of ssh-keygen command, the keys are generated and stored in the following files:

SSH RSA Private key: ~/.ssh/id_rsa. Again, NEVER EVER TRANSMIT THIS FILE
SSH RSA Public key:  ~/.ssh/id_rsa.pub. This file is the ONLY one SAFE to distribute
Ensure the access rights are correct on the generated keys using the ' ls -l ' command. The private key should be readable only by you:

(laptop)$> ls -l ~/.ssh/id_*
-rw------- 1 username groupname 751 Mar  1 20:16 /home/username/.ssh/id_rsa
-rw-r--r-- 1 username groupname 603 Mar  1 20:16 /home/username/.ssh/id_rsa.pub
ED25519 Key Pair

ED25519 is the most recommended public-key algorithm available today.

Repeat the procedure to generate a

(laptop)$> ssh-keygen -t ed25519 -a 100
[...]
$> ls -l ~/.ssh/id_*
-rw------- 1 username groupname 751 Mar  1 20:16 /home/username/.ssh/id_rsa
-rw-r--r-- 1 username groupname 603 Mar  1 20:16 /home/username/.ssh/id_rsa.pub     # Public  RSA key
-rw------- 1 username groupname 751 Mar  1 20:16 /home/username/.ssh/id_ed25519
-rw-r--r-- 1 username groupname 603 Mar  1 20:16 /home/username/.ssh/id_ed25519.pub # Public ED25519 key
Upload the keys on the ULHPC Identity Management Portal

You should now upload your public SSH keys *.pub to your user entry on the ULHPC Identity Management Portal.

Connect to the IdM portal and enter your ULHPC credentials.



First copy the content of the key you want to add

# Example with ED25519 **public** key
(laptop)$> cat ~/.ssh/id_ed25519.pub
ssh-ed25519 AAAA[...]
# OR the RSA **public** key
(laptop)$> cat ~/.ssh/id_rsa.pub
ssh-rsa AAAA[...]
Then on the portal:

Select Identity / Users.
Select your login entry
Under the Settings tab in the Account Settings area, click SSH public keys: Add.


Paste in the Base 64-encoded public key string, and click Set.



Click Save at the top of the page.

Hands-On/SSH & UL HPC access

Step 1a - Connect to UL HPC (Linux / Mac OS / Unix)

Run the following commands in a terminal (substituting yourlogin with the login name you received from us):

    (laptop)$> ssh -p 8022 yourlogin@access-aion.uni.lu
Now you probably want to avoid taping this long command to connect to the platform. You can customize SSH aliases for that. Edit the file ~/.ssh/config (create it if it does not already exist) and adding the following entries:

    Host aion-cluster
        Hostname access-aion.uni.lu
        User yourlogin
        Port 8022
        ForwardAgent no
Now you shall be able to issue the following (simpler) command to connect to the cluster and obtain the welcome banner:

    (laptop)$> ssh aion-cluster
We are now going to illustrate the quick configuration of SSH to facilitate access to the two instances.

(laptop)$> nano ~/.ssh/config    # OR subl vim emacs  etc...
Create the following content:

# Common options
Host *
    Compression yes
    ConnectTimeout 15

Host iris-cluster
    Hostname access-iris.uni.lu

Host aion-cluster
    Hostname access-aion.uni.lu

# /!\ ADAPT 'yourlogin' accordingly
Host *-cluster
    User yourlogin
    Port 8022
    ForwardAgent no
Now you can test the configuration with: ssh aion-cluster:

(laptop)$> ssh aion-cluster
==================================================================================
 Welcome to access1.aion-cluster.uni.lux
==================================================================================
                            _                         _
                           / \   ___ ___ ___  ___ ___/ |
                          / _ \ / __/ __/ _ \/ __/ __| |
                         / ___ \ (_| (_|  __/\__ \__ \ |
                        /_/   \_\___\___\___||___/___/_|
            __  _    _                ____ _           _          __
           / / / \  (_) ___  _ __    / ___| |_   _ ___| |_ ___ _ _\ \
          | | / _ \ | |/ _ \| '_ \  | |   | | | | / __| __/ _ \ '__| |
          | |/ ___ \| | (_) | | | | | |___| | |_| \__ \ ||  __/ |  | |
          | /_/   \_\_|\___/|_| |_|  \____|_|\__,_|___/\__\___|_|  | |
           \_\                                                    /_/
==================================================================================
 Atos BullSequana XH2000 Direct Liquid Cooling (DLC) supercomputer
                                             https://hpc-docs.uni.lu/systems/aion/
=== Computing Nodes ========================================= #RAM/n === #Cores ==
 aion-[0001-0318] 318 Atos X2410 AMD compute blade            256GB      40704
                      (2 AMD Epyc ROME 7H12 @ 2.6 GHz [64c/280W])
==================================================================================
 Fast interconnect using InfiniBand HDR 100 Gb/s technology
 Shared Storage with iris (raw capacity): 2180 TB (GPFS)+1300 TB (Lustre) = 3480TB

 Support (in this order!)                                 Platform notifications
   - *NEW* Technical Docs ...  https://hpc-docs.uni.lu         - Twitter: @ULHPC
   - FAQ .................... https://hpc-docs.uni.lu/support/
   - User Mailing-list ...... hpc-users@uni.lu  (moderated)
   - Helpdesk/Bug reports ... https://hpc.uni.lu/support (Service Now)
   - HPC Devops/Admins ...... hpc-team@uni.lu (OPEN TICKETS)
 ULHPC user guide is available on https://hpc-docs.uni.lu

    - 2020b software set released as default module environment
==================================================================================
 /!\ NEVER COMPILE OR RUN YOUR PROGRAMS FROM THIS FRONTEND !
     First reserve your nodes (using srun/sbatch(1))
 /!\ BEWARE of OS and architecture differences between Iris and Aion
     Identify the cluster used to compile your programs (Ex: <name>_<cluster>)
[yourlogin@access1 ~]$
In the following sections, we assume these aliases to be defined.

Step 1b - Optional - using SSH proxycommand setup to access the clusters despite port filtering (Linux / Mac OS / Unix)

It might happen that the port 8022 is filtered from your working place. You can easily bypass this firewall rule using an SSH proxycommand to setup transparently multi-hop connexions through one host (a gateway) to get to the access frontend of the cluster, as depited below:

[laptop] -----||--------> 22 [SSH gateway] ---------> 8022 [access-iris]
           firewall
The gateway can be any SSH server which have access to the access frontend of the cluster. You can use your personal NAS @ home etc. Then alter the SSH config on your laptop (in ~/.ssh/config typically) as follows:

create an entry to be able to connect to the gateway:
Alias for the gateway (not really needed, but convenient), below instantiated

Host gw
  User anotherlogin
  Hostname host.domain.org
  ForwardAgent no
Automatic connection to UL HPC from the outside via the gateway

Host *.ulhpc
  ProxyCommand ssh -q -x gw -W `basename %h .ulhpc`:%p
Ensure you can connect to the gateway:

(laptop)$> ssh gw
(gateway)$> exit # or CTRL-D
The .ulhpc suffix we mentioned in the previous configuration is an arbitrary suffix you will now specify in your command lines in order to access the UL HPC platform via the gateway as follows:

(laptop)$> ssh aion.ulhpc
Step 1c - Connect to UL HPC (Windows)

Download MobaXterm Installer edition
Install MobaXterm
Open the application Start > Program Files > MobaXterm
Change the default home directory for a persistent home directory instead of the default Temp directory. Go onto Settings > Configuration > General > Persistent home directory. Choose a location for your home directory.
load your private SSH key. Tools > Network > MobaKeyGen (SSH key generator) and choose Load (or create a new RSA key).
click on Session
In SSH Session:
Remote host: access-aion.uni.lu (or access-iris.uni.lu)
Check the Specify username box
Username: yourlogin
Port: 8022
In Advanced SSH Settings
Check Use private key box
Select your previously generated id_rsa.ppk
Click on OK
Step 2 - Hands-on/ Transferring files

Directories such as $HOME, $WORK or $SCRATCH are shared among the nodes of the cluster that you are using (including the front-end) via shared filesystems (NFS, Lustre) meaning that:

every file/directory pushed or created on the front-end is available on the computing nodes
every file/directory pushed or created on the computing nodes is available on the front-end
Step 2a - Linux / OS X / Unix command line tools

The two most common tools you can use for data transfers over SSH:

scp: for the full transfer of files and directories (only works fine for single files or directories of small/trivial size)
rsync: a software application which synchronizes files and directories from one location to another while minimizing data transfer as only the outdated or inexistent elements are transferred (practically required for lengthy complex transfers, which are more likely to be interrupted in the middle).
Of both, normally the second approach should be preferred, as more generic; note that, both ensure a secure transfer of the data, within an encrypted tunnel.

Create a new directory on your local machine and download a file to transfer (next-gen sequencing data from the NIH Roadmap Epigenomics Project):

(laptop)$> mkdir file_transfer
(laptop)$> cd file_transfer
(laptop)$> wget "ftp://ftp.ncbi.nlm.nih.gov/geo/samples/GSM409nnn/GSM409307/suppl/GSM409307_UCSD.H1.H3K4me1.LL228.bed.gz"
Transfer the file with scp:

(laptop)$> scp GSM409307_UCSD.H1.H3K4me1.LL228.bed.gz iris-cluster:
Connect to the cluster, check if the file is there and delete it.

(laptop)$> ssh iris-cluster
(access-iris)$> ls
(access-iris)$> rm GSM409307_UCSD.H1.H3K4me1.LL228.bed.gz
rm: remove regular file `GSM409307_UCSD.H1.H3K4me1.LL228.bed.gz'? y
(access-iris)$> exit
Transfer the directory with rsync:

(laptop)$> cd ..
(laptop)$> rsync -avzu file_transfer iris-cluster:
(laptop)$> rsync -e "ssh -F <path/to/config>" -avzu . <name>:path/to/remote/dir # Allow you to use a consistent naming across your server for either SSH or SCP/RSYNC data transfers.
Delete the file and retrieve it from the cluster:

(laptop)$> rm file_transfer/GSM409307_UCSD.H1.H3K4me1.LL228.bed.gz
(laptop)$> rsync -avzu iris-cluster:file_transfer .
Bonus: Check where the file is located on the cluster after the rsync.
You can get more information about these transfer methods in the file transfer documentation

Step 2b - Windows MobaXterm file transfer

If you are on Windows, you can directly use MobaXterm to transfer files. Connect to your session (see below on how to configure it). On the right panel you should see an SFTP panel opened.

SFTP on MobaXterm

You have just to drag and drop your files to this panel to transfer files to the cluster. You can try to upload this file ftp://ftp.ncbi.nlm.nih.gov/geo/samples/GSM409nnn/GSM409307/suppl/GSM409307_UCSD.H1.H3K4me1.LL228.bed.gz (next-gen sequencing data from the NIH Roadmap Epigenomics Project)

To retrieve a file from the cluster, you can right click on it and choose the Download option. Please refers to MobaXterm documentation for more informations on the available features.

SSH agent

If you are tired of typing your passphrase, use ssh-agent to load your private key

(laptop)$> ssh-add ~/.ssh/id_rsa
Enter passphrase for ~/.ssh/id_rsa:           # <-- enter your passphrase here
Identity added: ~/.ssh/id_rsa (<login>@<hostname>)

(laptop)$> ssh-add ~/.ssh/id_ed25519
Enter passphrase for ~/.ssh/id_ed25519:       # <-- enter your passphrase here
Identity added: ~/.ssh/id_ed25519 (<login>@<hostname>)
On Ubuntu/WSL, if you experience issues when using ssh-add, you should install the keychain package and use it as follows (eventually add it to your ~/.profile):

# Installation
(laptop)$> sudo apt install keychain

# Save your passphrase
/usr/bin/keychain --nogui ~/.ssh/id_ed25519    # (eventually) repeat with ~/.ssh/id_rsa
# Load the agent in your shell
source ~/.keychain/$(hostname)-sh
You are encouraged to ALWAYS simplify the SSH connections by embedding them in a dedicated Host entry, allowing to connect to your remote servers with

    ssh [-F <path/to/config>] <name>
# Now you can connect transparently to both instances
(laptop)$> ssh -F ~/.ssh/config iris-cluster
Welcome to access1.iris-cluster.uni.lux
[...]
[yourlogin@access1 ~]$ logout    # OR CTRL-D

(laptop)$> ssh -F ~/.ssh/config aion-cluster
Welcome to access1.aion-cluster.uni.lux
[...]
[yourlogin@access1 ~]$ logout    # OR CTRL-D
SOCKS 5 Proxy plugin (Optional)

Many Data Analytics framework involves a web interface (at the level of the master and/or the workers) you probably want to access in a relative transparent way.

For that, a convenient way is to rely on a SOCKS proxy, which is basically an SSH tunnel in which specific applications forward their traffic down the tunnel to the server, and then on the server end, the proxy forwards the traffic out to the general Internet. Unlike a VPN, a SOCKS proxy has to be configured on an app by app basis on the client machine, but can be set up without any specialty client agents.

Setting Up the Tunnel

To initiate such a SOCKS proxy using SSH (listening on localhost:1080 for instance), you simply need to use the -D 1080 command line option when connecting to a remote server:

(laptop)$> ssh -D 1080 -C <name>
-D: Tells SSH that we want a SOCKS tunnel on the specified port number (you can choose a number between 1025-65536)
-C: Compresses the data before sending it
-name: Server name
Configuring Firefox to Use the Tunnel

Now that you have an SSH tunnel, it's time to configure your web browser (in this case, Firefox) to use that tunnel. In particular, install the Foxy Proxy extension for Firefox and configure it to use your SOCKS proxy:

Right click on the fox icon
Options
Add a new proxy button
Name: ULHPC proxy
Informations > Manual configuration
Host IP: 127.0.0.1
Port: 1080
Check the Proxy SOCKS Option
Click on OK
Close
Open a new tab
Click on the Fox
Choose the ULHPC proxy
You can now access any web interface deployed on any service reachable from the SSH jump host i.e. the ULHPC login node.

Getting Started on the UL HPC platform

 Copyright (c) 2013-2021 UL HPC Team <hpc-sysadmins@uni.lu>


This tutorial will guide you through your first steps on the UL HPC platform.

Before proceeding:

make sure you have an account (if not, follow this procedure), and an SSH client.
take a look at the Getting Started
Follow the "Linux Shell" Tutorial
From a general perspective, the Support page describes how to get help during your UL HPC usage.

Convention

In the below tutorial, you'll proposed terminal commands where the prompt is denoted by $>.

In general, we will prefix to precise the execution context (i.e. your laptop, a cluster access server or a node). Remember that # character is a comment. Example:

    # This is a comment
    $> hostname

    (laptop)$> hostname         # executed from your personal laptop / workstation

    (access-iris)$> hostname    # executed from access server of the Iris cluster
Platform overview.

You can find a brief overview of the platform with key characterization numbers on this page.

The general organization of each cluster is depicted below:

UL HPC clusters general organization

Details on this organization can be found here

Discovering, visualizing and reserving UL HPC resources

In the following sections, replace <login> in the proposed commands with you login on the platform (ex: svarrette).

Step 1: the working environment

reference documentation After a successful login onto one of the access node (see Cluster Access), you end into your personal homedir $HOME which is shared over GPFS between the access node and the computing nodes.
Otherwise, you have to be aware of at least two directories:

$HOME: your home directory under NFS.
$SCRATCH: a non-backed up area put if possible under Lustre for fast I/O operations
Your homedir is under a regular backup policy. Therefore you are asked to pay attention to your disk usage and the number of files you store there.

Estimate file space usage and summarize disk usage of each FILE, recursively for directories using the ncdu command:

(access)$> ncdu
You can get an overview of the quotas and your current disk usage with the following command:

(access)$> df-ulhpc
You shall also pay attention to the number of files in your home directory. You can count them as follows:

(access)$> df-ulhpc -i
Step 2: web monitoring interfaces

Each cluster offers a set of web services to monitor the platform usage:

Ganglia

Ganglia is a scalable distributed monitoring system for high-performance computing systems such as clusters and Grids. Ganglia provides plots the system usage for each individual compute nodes (CPU, memory, I/O and network usage).

These information will help you identify and understand the behavior of your jobs on the cluster.

It is interesting to identify the limiting factor of your job:

Memory
Memory bound job on ganglia

CPU
CPU bound job on ganglia

Storage I/O
I/O bound job on ganglia

Network bound
Network bound job on ganglia

This is covered in the other tutorial Monitoring and profiling

Sample Usage on the UL HPC platform: Kernel compilation

We will illustrate the usage of tmux by performing a compilation of a recent linux kernel.

start a new tmux session

(access)$> tmux
rename the screen window "Frontend" (using CTRL+b ,)
create a new window and rename it "Compile"
within this new window, start a new interactive job over 1 node and 2 cores for 2 hours

(access)$> si --time 2:00:0 -N 1 -c 2
detach from this screen (using CTRL+b d)
kill your current SSH connection and your terminal
re-open your terminal and connect back to the cluster access server
list your running tmux sessions:

(access)$> tmux ls
0: 1 windows (created Mon Nov 15 17:48:58 2021) [316x46]
re-attach your previous screen session

(access)$> tmux a        # OR tmux attach-session -t 0:
in the "Compile" windows, go to the temporary directory and download the Linux kernel sources

(node)$> cd /tmp/
(node)$> curl -O https://cdn.kernel.org/pub/linux/kernel/v4.x/linux-4.19.163.tar.xz
IMPORTANT to avoid overloading the shared file system with the many small files involves in the kernel compilation (i.e. NFS and/or Lustre), we will perform the compilation in the local file system, i.e. either in /tmp or (probably more efficient) in /dev/shm (i.e in the RAM):

    (node)$> mkdir /dev/shm/PS1
    (node)$> cd /dev/shm/PS1
    (node)$> tar xf /tmp/linux-4.19.163.tar.xz
    (node)$> cd linux-4.19.163
    (node)$> make mrproper
    (node)$> make alldefconfig
    (node)$> make 2>&1 | tee /dev/shm/PS1/kernel_compile.log
You can now detach from the tmux session and have a coffee
The last compilation command make use of tee, a nice tool which read from standard input and write to standard output and files. This permits to save in a log file the message written in the standard output.

Question: why using the make 2>&1 sequence in the last command?

Question: why working in /dev/shm is more efficient?

Reattach from time to time to your tmux session to see the status of the compilation
Your compilation is successful if it ends with the sequence:

[...]
Kernel: arch/x86/boot/bzImage is ready  (#2)
Restart the compilation, this time using multiple cores and parallel jobs within the Makefile invocation (-j option of make)

(node)$> make clean
(node)$> time make -j $SLURM_CPUS_ON_NODE 2>&1 | tee /dev/shm/PS1/kernel_compile.2.log
The table below should convince you to always run make with the -j option whenever you can...

Context	time (make)	time (make -j 16)
Compilation in /tmp(HDD / chaos)	4m6.656s	0m22.981s
Compilation in /tmp(SSD / gaia)	3m52.895s	0m17.508s
Compilation in /dev/shm (RAM)	3m11.649s	0m17.990s
Use the Ganglia interface to monitor the impact of the compilation process on the node your job is running on.
Connect to your interactive job using the command sjoin <jobid>. Use the following system commands on the node during the compilation:

htop
top
free -m
uptime
ps aux

Job scheduling with SLURM

 Copyright (c) 2013-2021 UL HPC Team <hpc-sysadmins@uni.lu>
This page is part of the Getting started tutorial, and the follow-up of the "Overview" section.

reference documentation
The basics

Slurm is an open source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small Linux clusters. It is used on Iris UL HPC cluster.

It allocates exclusive or non-exclusive access to the resources (compute nodes) to users during a limited amount of time so that they can perform they work
It provides a framework for starting, executing and monitoring work
It arbitrates contention for resources by managing a queue of pending work.
It permits to schedule jobs for users on the cluster resource
Vocabulary

A user job is characterized by:

the number of nodes
the number of CPU cores
the memory requested
the walltime
the launcher script, which will initiate your tasks
Partition: group of compute nodes, with specific usage characteristics (time limits and maximum number of nodes per job
QOS: The quality of service associated with a job affects the way it is scheduled (priority, preemption, limits per user, etc).
Tasks: processes run in parallel inside the job
Hands on

We will now see the basic commands of Slurm.

Connect to aion-cluster or iris-cluster. You can request resources in interactive mode:
(access)$> si
Notice that with no other parameters, srun gave you one resource for 30 minutes. You were also directly connected to the node you reserved with an interactive shell. Now exit the reservation (exit command or CTRL-D)

When you run exit, you are disconnected and your reservation is terminated.

To avoid anticipated termination of your jobs in case of errors (terminal closed by mistake), you can reserve and connect in two steps using the job id associated to your reservation.

First run a passive job i.e. run a predefined command -- here sleep 4h to delay the execution for 4 hours -- on the first reserved node:
(access)$> sbatch --qos normal --wrap "sleep 4h"
Submitted batch job 390
You noticed that you received a job ID (in the above example: 390), which you can later use to connect to the reserved resource(s):

    (access)$> sjoin 390 # adapt the job ID accordingly ;)
    (node)$> ps aux | grep sleep
    <login> 186342  0.0  0.0 107896   604 ?        S    17:58   0:00 sleep 4h
    <login> 187197  0.0  0.0 112656   968 pts/0    S+   18:04   0:00 grep --color=auto sleep
    (node)$> exit             # or CTRL-D
Question: At which moment the job 390 will end?

a. after 10 days

b. after 2 hours

c. never, only when I'll delete the job

Question: manipulate the $SLURM_* variables over the command-line to extract the following information, once connected to your job

a. the list of hostnames where a core is reserved (one per line) * hint: man echo

b. number of reserved cores * hint: search for the NPROCS variable

c. number of reserved nodes * hint: search for the NNODES variable

d. number of cores reserved per node together with the node name (one per line) * Example of output:

        12 iris-11
        12 iris-15
hint: NPROCS variable or NODELIST
Job management with interactive jobs

Normally, the previously run job is still running.

You can check the status of your running jobs using squeue command:
(access)$> squeue             # list all jobs
(access)$> squeue -u <login> # list all your jobs
Then you can delete your job by running scancel command:

    (access)$> scancel 390
You can see your system-level utilization (memory, I/O, energy) of a running job using sstat $jobid:
(access)$> sstat 390
In all remaining examples of reservation in this section, remember to delete the reserved jobs afterwards (using scancel or CTRL-D)

You probably want to use more than one core, and you might want them for a different duration than one hour.

Reserve interactively 4 cores in one task on one node, for 30 minutes (delete the job afterwards)

(access)$> salloc -p interactive --time=0:30:0 -N 1 --ntasks-per-node=1 --cpus-per-task=4
Reserve interactively 4 tasks (system processes) with 2 nodes for 30 minutes (delete the job afterwards)

(access)$> salloc -p interactive --time=0:30:0 -N 2 --ntasks-per-node=4 --cpus-per-task=4
This command can also be written in a more compact way

    (access)$> si --time=0:30:0 -N2 -n4 -c2
You can stop a waiting job from being scheduled and later, allow it to be scheduled:
(access)$> scontrol hold $SLURM_JOB_ID
(access)$> scontrol release $SLURM_JOB_ID
Passive jobs submission

In the previous section, we have started interactive jobs using the srun command. When srun is used on the access server, it can be used to submit an interactive jobs, meaning that the srun will wait for the job to be scheduled before starting a new shell.

The other possibility is to submit a passive job with sbatch. In this case, resources can be specified on the command line as srun, or in the launcher script.

The launcher script first lines can contain headers, in the following case, in this case, the following launcher script will request a single task of one ore for 5 minutes, in the batch queue.

    #!/bin/bash -l
    #SBATCH -N 1
    #SBATCH --ntasks-per-node=1
    #SBATCH -c 1
    #SBATCH --time=0-00:05:00
    #SBATCH -p batch
    echo "Hello World!"
Save this content in a file named launcher.sh and submit it with sbatch

    (access)$> sbatch launcher.sh
Modify this example to:

request 6 single core tasks equally spread across two nodes for 3 hours
Request as many tasks as cores available on a single node in the batch queue for 3 hours
Request one sequential task requiring half the memory of a regular node for 1 day (use the header #SBATCH --mem=64GB on Iris or #SBATCH --mem=128GB)
Iris only - request one GPU tasks for 4 hours - dedicate 1/4 of available cores for its management (use the gpu partition and the header #SBATCH -G 1)

Using software modules

 Copyright (c) 2013-2020 UL HPC Team <hpc-sysadmins@uni.lu>
This page is part of the Getting started tutorial.

Environment Modules is a software package that allows us to provide a multitude of applications and libraries in multiple versions on the UL HPC platform. The tool itself is used to manage environment variables such as PATH, LD_LIBRARY_PATH and MANPATH, enabling the easy loading and unloading of application/library profiles and their dependencies.

We will have multiple occasion to use modules in the other tutorials so there is nothing special we foresee here. You are just encouraged to read the following resources:

Introduction to Environment Modules by Wolfgang Baumann
Modules tutorial @ NERSC
UL HPC documentation on modules
Commands

By loading appropriate environment modules, the user can select:

compilers,
libraries, e.g. the MPI library, or
other third party software packages.
An exhaustive list of the available software is proposed in this page.

On a node, using an interactive job, you can:

list all available softwares: module avail
search for one software: module spider <search terms>
"load" a software in your environment: module load <category>/<software>[/<version>]
list the currently loaded modules: module list
clean your environment, unload everything: module purge
Software sets

Currently, the ULHPC provides the software sets 2019a (default) and 2019b (devel). We encourage you to use right now 2019b, by redefining the MODULEPATH variable as explained in the next section, as it will soon be promoted as the next default environment.

The ULHPC team updates the software set every year based on the Easybuild releases.

Name	Type	2019b (legacy)	2020a	2020b (prod)	2021a (devel)
GCCCore	compiler	8.3.0	9.3.0	10.2.0	10.3.0
foss	toolchain	2019b	2020a	2020b	2021a
intel	toolchain	2019b	2020a	2020b	2021a
Python		3.7.4 (and 2.7.16)	3.8.2	3.8.6	3.9.2
Each environment provides different versions of softwares. The core of the software environment is the toolchain, a toolchain is a set of tools used to compile and run of the other programs of the software environment.

The ULHPC team provides two toolchain:

foss : based on open source software (GCC, binutils, OpenMPI, OpenBLAS, FFTW)
intel: based on the proprietary Intel compiler suite.
$MODULEPATH environment variable

The environment variable $MODULEPATH contains the path of the directory containing the modules. You can use this variable to change your software environment.

In the following command, we use the 2020b environment compiled for Broadwell CPUs on Iris.

export MODULEPATH=/opt/apps/resif/iris/2020b/broadwell/modules/all
This is equivalent to the command resif-load-swset-devel on Iris.

For backward compatibility reasons and for reproducibility, it is always possible to load the older environments, with the command resif-load-swset-legacy

In order to restore production settings, run the command resif-load-swset-prod

Examples

Choose one or two languages below, and try to run the hello world program on a compute node.

Matlab

Create a file named fibonacci.m in your home directory, copy-paste the following code in this file. This code will calculate the first N numbers of the Fibonacci sequence

N=1000;
fib=zeros(1,N);
fib(1)=1;
fib(2)=1;
k=3;
while k <= N
  fib(k)=fib(k-2)+fib(k-1);
  fprintf('%d\n',fib(k));
  pause(1);
  k=k+1;
end
Create a new interactive job
Look for the matlab module using the command module spider
Load the module base/MATLAB using the command module load
Execute the code using matlab

(node)$> matlab -nojvm -nodisplay -nosplash < path/to/fibonacci.m
R

Create a file named fibonacci.R in your home directory, copy-paste the following code in this file. This code will calculate the first N numbers of the Fibonacci sequence

N <- 130
fibvals <- numeric(N)
fibvals[1] <- 1
fibvals[2] <- 1
for (i in 3:N) {
     fibvals[i] <- fibvals[i-1]+fibvals[i-2]
     print( fibvals[i], digits=22)
     Sys.sleep(1)
}
Create a new interactive job
Look for the R module using the command module spider
Load the module lang/R using the command module load
Execute the code using R

(node)$> Rscript path/to/fibonacci.R
C

Create a new file called helloworld.c, containing the source code of a simple "Hello World" program written in C.

    #include<stdio.h>

    int main()
    {
        printf("Hello, world!");
        return 0;
    }
First, compile the program using the "FOSS" toolchain, containing the GNU C compiler

    (node)$> module load toolchain/foss
    (node)$> gcc helloworld.c -o helloworld
Then, compile the program using the Intel toolchain, containing the ICC compiler

    (node)$> module purge
    (node)$> module load toolchain/intel
    (node)$> icc helloworld.c -o helloworld
If you use Intel CPUs and ICC is available on the platform, it is advised to use ICC in order to produce optimized binaries and achieve better performance.

C++

Question: create a new file helloworld.cpp containing the following C++ source code, compile the following program, using GNU C++ compiler (g++ command), and the Intel compiler (icpc command).

    #include <iostream>

    int main() {
        std::cout << "Hello, world!" << std::endl;
    }
Fortran

Question: create a new file helloworld.f90 containing the following source code, compile the following program, using the GNU Fortran compiler (gfortran command), and ICC (ifort command).

    program hello
       print *, "Hello, World!"
    end program hello
MPI

MPI is a programming interface that enables the communication between processes of a distributed memory system.

We will create a simple MPI program where the MPI process of rank 0 broadcasts an integer (42) to all the other processes. Then, each process prints its rank, the total number of processes and the value he received from the process 0.

In your home directory, create a file mpi_broadcast.c and copy the following source code:

    #include <stdio.h>
    #include <mpi.h>
    #include <unistd.h>

    int main (int argc, char *argv []) {
           char hostname[257];
           int size, rank;
           int i, pid;
           int bcast_value = 1;

           gethostname(hostname, sizeof hostname);
           MPI_Init(&argc, &argv);
           MPI_Comm_rank(MPI_COMM_WORLD, &rank);
           MPI_Comm_size(MPI_COMM_WORLD, &size);
           if (!rank) {
                bcast_value = 42;
           }
           MPI_Bcast(&bcast_value, 1, MPI_INT, 0, MPI_COMM_WORLD);
           printf("%s\t- %d - %d - %d\n", hostname, rank, size, bcast_value);
           fflush(stdout);

           MPI_Barrier(MPI_COMM_WORLD);
           MPI_Finalize();
           return 0;
    }
Reserve 2 tasks of 1 core on two distinct nodes with Slurm

    (access-iris)$> si --time 1:00:0 -N 2 -n 2 -c 1
Load a toolchain and compile the code using mpicc

    (node)$> mpicc mpi_broadcast.c -o mpi_broadcast
With Slurm, you can use the srun command. Create an interactive job, with 2 nodes (-N 2), and at least 2 tasks (-n 2).

    (node)$> srun -n $SLURM_NTASKS ~/mpi_broadcast

    Using and Building [custom] software with EasyBuild on the UL HPC platform

    Copyright (c) 2014-2021 UL HPC Team hpc-sysadmins@uni.lu
    
    Authors: Emmanuel Kieffer, Sébastien Varrette and UL HPC Team hpc-team@uni.lu
    
    The objective of this tutorial is to show how tools such as EasyBuild can be used to ease, automate and script the build of software on the UL HPC platforms.
    
    Indeed, as researchers involved in many cutting-edge and hot topics, you probably have access to many theoretical resources to understand the surrounding concepts. Yet it should normally give you a wish to test the corresponding software. Traditionally, this part is rather time-consuming and frustrating, especially when the developers did not rely on a "regular" building framework such as CMake or the autotools (i.e. with build instructions as configure --prefix <path> && make && make install).
    
    And when it comes to have a build adapted to an HPC system, you are somehow forced to make a custom build performed on the target machine to ensure you will get the best possible performances. EasyBuild is one approach to facilitate this step.
    
    Moreover, later on, you probably want to recover a system configuration matching the detailed installation paths through a set of environmental variable (ex: JAVA_HOME, HADOOP_HOME etc.). At least you would like to see the traditional PATH, CPATH or LD_LIBRARY_PATH updated.
    
    Question: What is the purpose of the above mentioned environmental variable?
    
    For this second aspect, the solution came long time ago (in 1991) with the [Environment Modules](http://modules.sourceforge.net/ and LMod We will cover it in the first part of this tutorial, also to discover the ULHPC User Software set in place.
    
    Finally the last part will cover Easybuild usage on the ULHPC platform to build and complete the existing software environment.
    
    Pre-requisites
    
    Ensure you are able to connect to the UL HPC clusters.
    
    In particular, recall that the module command is not available on the access frontends.
    
    Now you'll need to pull the latest changes in your working copy of the ULHPC/tutorials you should have cloned in ~/git/github.com/ULHPC/tutorials (see "preliminaries" tutorial)
    
    (access)$> cd ~/git/github.com/ULHPC/tutorials
    (access)$> git pull
    Now configure a dedicated directory ~/tutorials/easybuild for this session
    
    # return to your home
    (access)$> mkdir -p ~/tutorials/easybuild
    (access)$> cd ~/tutorials/easybuild
    # create a symbolic link to the reference material
    (access)$> ln -s ~/git/github.com/ULHPC/tutorials/tools/easybuild easybuild
    For all tests and compilation with Easybuild, you MUST work on a computing node. Let's get one in interactive jobs:
    
    ### Access to ULHPC cluster (if not yet done)
    (laptop)$> ssh aion-cluster
    ### Have an interactive job, request 4 cores/thread for 2 hours
    # ... either directly
    (access)$> si --ntasks-per-node 1 -c 4 -t 2:00:00
    # ... or using the HPC School reservation 'hpcschool'if needed  - use 'sinfo -T' to check if active and its name
    # (access)$> si --reservation=hpcschool --ntasks-per-node 1 -c 4 -t 2:00:00
    (node)$>
    Part 1: Environment modules and LMod
    
    The ULHPC facility relied on the Environment Modules / LMod framework which provided the module utility on Compute nodes to manage nearly all software. There are two main advantages of the module approach:
    
    ULHPC can provide many different versions and/or installations of a single software package on a given machine, including a default version as well as several older and newer version.
    Users can easily switch to different versions or installations without having to explicitly specify different paths. With modules, the PATH and related environment variables (LD_LIBRARY_PATH, MANPATH, etc.) are automatically managed.
    Environment Modules in itself are a standard and well-established technology across HPC sites, to permit developing and using complex software and libraries build with dependencies, allowing multiple versions of software stacks and combinations thereof to co-exist. It brings the module command which is used to manage environment variables such as PATH, LD_LIBRARY_PATH and MANPATH, enabling the easy loading and unloading of application/library profiles and their dependencies.
    
    See https://hpc-docs.uni.lu/environment/modules/ for more details
    
    Command	Description
    module avail	Lists all the modules which are available to be loaded
    module spider <pattern>	Search for among available modules (Lmod only)
    module load <mod1> [mod2...]	Load a module
    module unload <module>	Unload a module
    module list	List loaded modules
    module purge	Unload all modules (purge)
    module display <module>	Display what a module does
    module use <path>	Prepend the directory to the MODULEPATH environment variable
    module unuse <path>	Remove the directory from the MODULEPATH environment variable
    At the heart of environment modules interaction resides the following components:
    
    the MODULEPATH environment variable, which defines the list of searched directories for modulefiles
    modulefile
    Take a look at the current values:
    
    # Example on Aion
    $ module -h
    $ echo $MODULEPATH
    /opt/apps/resif/aion/2020b/epyc/modules/all:/opt/apps/smc/modules
    $ module show toolchain/foss
    -----------------------------------------------------------------------------------------------------------------
       /opt/apps/resif/aion/2020b/epyc/modules/all/toolchain/foss/2020b.lua:
    -----------------------------------------------------------------------------------------------------------------
    help([[
    Description
    ===========
    GNU Compiler Collection (GCC) based compiler toolchain, including
     OpenMPI for MPI support, OpenBLAS (BLAS and LAPACK support), FFTW and ScaLAPACK.
    
    
    More information
    ================
     - Homepage: https://easybuild.readthedocs.io/en/master/Common-toolchains.html#foss-toolchain
    ]])
    whatis("Description: GNU Compiler Collection (GCC) based compiler toolchain, including
     OpenMPI for MPI support, OpenBLAS (BLAS and LAPACK support), FFTW and ScaLAPACK.")
    whatis("Homepage: https://easybuild.readthedocs.io/en/master/Common-toolchains.html#foss-toolchain")
    whatis("URL: https://easybuild.readthedocs.io/en/master/Common-toolchains.html#foss-toolchain")
    conflict("toolchain/foss")
    load("compiler/GCC/10.2.0")
    load("mpi/OpenMPI/4.0.5-GCC-10.2.0")
    load("numlib/OpenBLAS/0.3.12-GCC-10.2.0")
    load("numlib/FFTW/3.3.8-gompi-2020b")
    load("numlib/ScaLAPACK/2.1.0-gompi-2020b")
    setenv("EBROOTFOSS","/opt/apps/resif/aion/2020b/epyc/software/foss/2020b")
    setenv("EBVERSIONFOSS","2020b")
    setenv("EBDEVELFOSS","/opt/apps/resif/aion/2020b/epyc/software/foss/2020b/easybuild/toolchain-foss-2020b-easybuild-de
    vel")
    You have already access to a huge list of software:
    
    $ module avail       # OR 'module av'
    Now you can search for a given software using module spider <pattern>:
    
    $  module spider lang/Python
    ---------------------------------------------------------------------------------------------------------
      lang/Python:
    ---------------------------------------------------------------------------------------------------------
        Description:
          Python is a programming language that lets you work more quickly and integrate your systems more
          effectively.
    
         Versions:
            lang/Python/2.7.18-GCCcore-10.2.0
            lang/Python/3.8.6-GCCcore-10.2.0
    Let's see the effect of loading/unloading a module
    
    $> module list
    No modules loaded
    $> which python
    /usr/bin/python
    $> python --version       # System level python
    Python 2.7.8
    
    $> module load lang/Python    # use TAB to auto-complete
    $> which python
    /opt/apps/resif/aion/2020b/epyc/software/Python/3.8.6-GCCcore-10.2.0/bin/python
    $> python --version
    Python Python 3.8.6
    $> module purge
    ULHPC $MODULEPATH
    
    By default, the MODULEPATH environment variable holds a single searched directory holding the optimized builds prepared for you by the ULHPC Team. The general format of this directory is as follows:
    
    /opt/apps/resif/<cluster>/<version>/<arch>/modules/all
    where:
    
    <cluster> depicts the name of the cluster (iris or aion). Stored as $ULHPC_CLUSTER.
    <version> corresponds to the ULHPC Software set release (aligned with Easybuid toolchains release), i.e. 2019b, 2020a etc. It is stored as $RESIF_VERSION_{PROD,DEVEL,LEGACY}.
    <arch> is a lower-case strings that categorize the CPU architecture of the build host, and permits to easily identify optimized target architecture. It is stored as $RESIF_ARCH
    On Intel nodes: broadwell (default), skylake
    On AMD nodes: epyc
    On GPU nodes: gpu
    Cluster	Arch.	$MODULEPATH Environment variable
    Iris	broadwell (default)	/opt/apps/resif/iris/<version>/broadwell/modules/all
    Iris	skylake	/opt/apps/resif/iris/<version>/skylake/modules/all
    Iris	gpu	/opt/apps/resif/iris/<version>/gpu/modules/all
    Aion	epyc (default)	/opt/apps/resif/aion/<version>/{epyc}/modules/all
    
    
    Now let's assume that a given software you're looking at is not available, or not in the version you want. Before we continue, there are a set of local environmental variables defined on the ULHPC facility that you will be interested to use in the sequel.
    
    Environment Variable	Description	Example
    $ULHPC_CLUSTER	Current ULHPC supercomputer you're connected to	aion
    $RESIF_VERSION_{PROD,DEVEL,LEGACY}	Production / development / legacy ULHPC software set version	2020b
    $RESIF_ARCH	RESIF Architecture	epyc
    $MODULEPATH_{PROD,DEVEL,LEGACY}	Production / development / legacy MODULEPATH	/opt/apps/resif/aion/2021b/{epyc}/modules/all
    Part 2: Easybuild
    
    
    
    EasyBuild is a tool that allows to perform automated and reproducible compilation and installation of software. A large number of scientific software are supported (2995 supported software packages in the last release 4.7.1) -- see also What is EasyBuild?
    
    All builds and installations are performed at user level, so you don't need the admin (i.e. root) rights. The software are installed in your home directory under $EASYBUILD_PREFIX -- see https://hpc-docs.uni.lu/environment/easybuild/
    
    Default setting (local)	Recommended setting
    $EASYBUILD_PREFIX	$HOME/.local/easybuild	$HOME/.local/easybuild/${ULHPC_CLUSTER}/${RESIF_VERSION_PROD}/${RESIF_ARCH}
    built software are placed under ${EASYBUILD_PREFIX}/software/
    modules install path ${EASYBUILD_PREFIX}/modules/all
    Easybuild main concepts
    
    See also the official Easybuild Tutorial: "Maintaining a Modern Scientific Software Stack Made Easy with EasyBuild"
    
    EasyBuild relies on two main concepts: Toolchains and EasyConfig files.
    
    A toolchain corresponds to a compiler and a set of libraries which are commonly used to build a software. The two main toolchains frequently used on the UL HPC platform are the foss ("Free and Open Source Software") and the intel one.
    
    foss is based on the GCC compiler and on open-source libraries (OpenMPI, OpenBLAS, etc.).
    intel is based on the Intel compiler and on Intel libraries (Intel MPI, Intel Math Kernel Library, etc.).
    An EasyConfig file is a simple text file that describes the build process of a software. For most software that uses standard procedures (like configure, make and make install), this file is very simple. Many EasyConfig files are already provided with EasyBuild. By default, EasyConfig files and generated modules are named using the following convention: <Software-Name>-<Software-Version>-<Toolchain-Name>-<Toolchain-Version>. However, we use a hierarchical approach where the software are classified under a category (or class) -- see the CategorizedModuleNamingScheme option for the EASYBUILD_MODULE_NAMING_SCHEME environmental variable), meaning that the layout will respect the following hierarchy: <Software-Class>/<Software-Name>/<Software-Version>-<Toolchain-Name>-<Toolchain-Version>
    
    Additional details are available on EasyBuild website:
    
    EasyBuild homepage
    EasyBuild documentation
    What is EasyBuild?
    Toolchains
    EasyConfig files
    List of supported software packages
    Easybuild is provided to you as a software module.
    
    module load tools/EasyBuild
    In case you can't install the latest version yourself, please follow the official instructions. Nonetheless, we strongly recommand to use the provided module. Don't forget to setup your local Easybuild configuration first.
    
    What is important for the installation of Easybuild are the following variables:
    
    EASYBUILD_PREFIX: where to install local modules and software, i.e. $HOME/.local/easybuild
    EASYBUILD_MODULES_TOOL: the type of modules tool you are using, i.e. LMod in this case
    EASYBUILD_MODULE_NAMING_SCHEME: the way the software and modules should be organized (flat view or hierarchical) -- we're advising on CategorizedModuleNamingScheme
    /!\ IMPORTANT: Recall that you should be on a compute node to install Easybuild (otherwise the checks of the module command availability will fail.)
    
    Local Easybuild configuration
    
    If you prefer to extend/complement the ULHPC software set while taking into account the cluster $ULHPC_CLUSTER ("iris" or "aion"), the toolchain version <version> (Ex: 2019b, 2020b etc.) and eventually the architecture <arch>. In that case, you can use the following helper function defined at /etc/profile.d/ulhpc_resif.sh:
    
        resif-load-home-swset-prod
    The function sets all the Lmod and Easybuild variables to match the production software set:
    
    resif-load-home-swset-prod 
    => Set EASYBUILD_PREFIX to '/home/users/ekieffer/.local/easybuild/aion/2020b/epyc'
    => Enabling local RESIF home environment  (under /home/users/ekieffer) against '2020b' software set
       Current MODULEPATH=/home/users/ekieffer/.local/easybuild/aion/2020b/epyc/modules/all:/opt/apps/resif/aion/2020b/epyc/modules/all:/opt/apps/smc/modules/
    For a shared project directory <name> located under $PROJECTHOME/<name>, you can use the following following helper scripts:
    
    resif-load-project-swset-prod $PROJECTHOME/<name>
    This function is very helpfull if you wish to share your custom software set with members of your group:
    
    resif-load-project-swset-prod $PROJECTHOME/ulhpc-tutorials/
    => Set EASYBUILD_PREFIX to '/work/projects//ulhpc-tutorials//easybuild/aion/2020b/epyc'
    => Enabling local RESIF project environment  (under /work/projects//ulhpc-tutorials/) against '2020b' software set
       Current MODULEPATH=/work/projects//ulhpc-tutorials//easybuild/aion/2020b/epyc/modules/all:/opt/apps/resif/aion/2020b/epyc/modules/all:/opt/apps/smc/modules/
    Additonnaly the ULHPC provides multiple functions to help you switching between software sets.
    
    ekieffer@aion-0262(723109 -> 1:14:37) resif-<TAB>
    resif-info                       resif-load-home-swset-legacy     resif-load-project-swset-devel   resif-load-project-swset-prod    resif-load-swset-legacy          resif-reset-swset
    resif-load-home-swset-devel      resif-load-home-swset-prod       resif-load-project-swset-legacy  resif-load-swset-devel           resif-load-swset-prod            
    Install a missing software by complementing the ULHPC software set
    
    Let's try to install the missing software
    
    (node)$ module spider BCFtools   # Complementaty tool to SAMTools
    Lmod has detected the following error:  Unable to find: "BCFtools".
    
    # Use helper function to setup local easybuild configuration
    (node)$ resif-load-home-swset-prod 
    # Load Easybuild
    (node)$ module load tools/EasyBuild
    # Search for recipes for the missing software
    (node)$ eb -S BCFtools
    == found valid index for /opt/apps/resif/aion/2020b/epyc/software/EasyBuild/4.5.4/easybuild/easyconfigs, so using it...
    CFGS1=/opt/apps/resif/aion/2020b/epyc/software/EasyBuild/4.5.4/easybuild/easyconfigs
     * $CFGS1/b/BCFtools/BCFtools-1.2_extHTSlib_Makefile.patch
     * $CFGS1/b/BCFtools/BCFtools-1.3-foss-2016a.eb
     * $CFGS1/b/BCFtools/BCFtools-1.3-intel-2016a.eb
     * $CFGS1/b/BCFtools/BCFtools-1.3.1-foss-2016b.eb
     * $CFGS1/b/BCFtools/BCFtools-1.3_extHTSlib_Makefile.patch
     * $CFGS1/b/BCFtools/BCFtools-1.6-foss-2016b.eb
     * $CFGS1/b/BCFtools/BCFtools-1.6-foss-2017b.eb
     * $CFGS1/b/BCFtools/BCFtools-1.6-intel-2017b.eb
     * $CFGS1/b/BCFtools/BCFtools-1.8-GCC-6.4.0-2.28.eb
     * $CFGS1/b/BCFtools/BCFtools-1.9-foss-2018a.eb
     * $CFGS1/b/BCFtools/BCFtools-1.9-foss-2018b.eb
     * $CFGS1/b/BCFtools/BCFtools-1.9-iccifort-2019.1.144-GCC-8.2.0-2.31.1.eb
     * $CFGS1/b/BCFtools/BCFtools-1.9-intel-2018b.eb
     * $CFGS1/b/BCFtools/BCFtools-1.10.2-GCC-8.3.0.eb
     * $CFGS1/b/BCFtools/BCFtools-1.10.2-GCC-9.3.0.eb
     * $CFGS1/b/BCFtools/BCFtools-1.10.2-iccifort-2019.5.281.eb
     * $CFGS1/b/BCFtools/BCFtools-1.11-GCC-10.2.0.eb
     * $CFGS1/b/BCFtools/BCFtools-1.12-GCC-9.3.0.eb
     * $CFGS1/b/BCFtools/BCFtools-1.12-GCC-10.2.0.eb
     * $CFGS1/b/BCFtools/BCFtools-1.12-GCC-10.3.0.eb
     * $CFGS1/b/BCFtools/BCFtools-1.14-GCC-11.2.0.eb
    From this list, you should select the version matching the target toolchain version -- see ULHPC Toolchains and Software Set Versioning documentation In particular, for 2020b version, the GCC[core] is set to 10.2.0 so BCFtools-1.12-GCC-10.2.0.eb seems like a promising candidate .
    
    Once you pick a given recipy (for instance BCFtools-1.12-GCC-10.2.0.eb), install it with
    
       eb <name>.eb [-D] -r
    -D enables the dry-run mode to check what's going to be install -- ALWAYS try it first
    -r enables the robot mode to automatically install all dependencies while searching for easyconfigs in a set of pre-defined directories -- you can also prepend new directories to search for eb files (like the current directory $PWD) using the option and syntax --robot-paths=$PWD: (do not forget the ':'). See Controlling the robot search path documentation
    The $CFGS<n>/ prefix should be dropped unless you know what you're doing (and thus have previously defined the variable -- see the first output of the eb -S [...] command).
    Let's try to review the missing dependencies from a dry-run :
    
    # Select the one matching the target software set version
    (node)$ eb BCFtools-1.12-GCC-10.2.0.eb -Dr   # Dry-run
    == Temporary log file in case of crash /tmp/eb-xvjew6tq/easybuild-45mth_zy.log
    == found valid index for /opt/apps/resif/aion/2020b/epyc/software/EasyBuild/4.5.4/easybuild/easyconfigs, so using it...
    == found valid index for /opt/apps/resif/aion/2020b/epyc/software/EasyBuild/4.5.4/easybuild/easyconfigs, so using it...
    Dry run: printing build status of easyconfigs and dependencies
    CFGS=/opt/apps/resif/aion/2020b/epyc/software/EasyBuild/4.5.4/easybuild/easyconfigs
     * [ ] $CFGS/m/M4/M4-1.4.19.eb (module: devel/M4/1.4.19)
     * [ ] $CFGS/b/Bison/Bison-3.8.2.eb (module: lang/Bison/3.8.2)
     * [x] $CFGS/f/flex/flex-2.6.4.eb (module: lang/flex/2.6.4)
     * [x] $CFGS/z/zlib/zlib-1.2.11.eb (module: lib/zlib/1.2.11)
     * [x] $CFGS/b/binutils/binutils-2.35.eb (module: tools/binutils/2.35)
     * [x] $CFGS/g/GCCcore/GCCcore-10.2.0.eb (module: compiler/GCCcore/10.2.0)
     * [x] $CFGS/z/zlib/zlib-1.2.11-GCCcore-10.2.0.eb (module: lib/zlib/1.2.11-GCCcore-10.2.0)
     * [x] $CFGS/n/ncurses/ncurses-6.2.eb (module: devel/ncurses/6.2)
     * [x] $CFGS/g/gettext/gettext-0.21.eb (module: tools/gettext/0.21)
     * [x] $CFGS/h/help2man/help2man-1.47.16-GCCcore-10.2.0.eb (module: tools/help2man/1.47.16-GCCcore-10.2.0)
     * [x] $CFGS/m/M4/M4-1.4.18-GCCcore-10.2.0.eb (module: devel/M4/1.4.18-GCCcore-10.2.0)
     * [x] $CFGS/b/Bison/Bison-3.7.1-GCCcore-10.2.0.eb (module: lang/Bison/3.7.1-GCCcore-10.2.0)
     * [x] $CFGS/f/flex/flex-2.6.4-GCCcore-10.2.0.eb (module: lang/flex/2.6.4-GCCcore-10.2.0)
     * [x] $CFGS/b/binutils/binutils-2.35-GCCcore-10.2.0.eb (module: tools/binutils/2.35-GCCcore-10.2.0)
     * [x] $CFGS/c/cURL/cURL-7.72.0-GCCcore-10.2.0.eb (module: tools/cURL/7.72.0-GCCcore-10.2.0)
     * [x] $CFGS/g/GCC/GCC-10.2.0.eb (module: compiler/GCC/10.2.0)
     * [x] $CFGS/b/bzip2/bzip2-1.0.8-GCCcore-10.2.0.eb (module: tools/bzip2/1.0.8-GCCcore-10.2.0)
     * [x] $CFGS/x/XZ/XZ-5.2.5-GCCcore-10.2.0.eb (module: tools/XZ/5.2.5-GCCcore-10.2.0)
     * [x] $CFGS/g/GSL/GSL-2.6-GCC-10.2.0.eb (module: numlib/GSL/2.6-GCC-10.2.0)
     * [x] $CFGS/h/HTSlib/HTSlib-1.12-GCC-10.2.0.eb (module: bio/HTSlib/1.12-GCC-10.2.0)
     * [ ] $CFGS/b/BCFtools/BCFtools-1.12-GCC-10.2.0.eb (module: bio/BCFtools/1.12-GCC-10.2.0)
    == Temporary log file(s) /tmp/eb-xvjew6tq/easybuild-45mth_zy.log* have been removed.
    == Temporary directory /tmp/eb-xvjew6tq has been removed.
    Let's try to install it (remove the -D):
    
    # Select the one matching the target software set version
    (node)$ eb BCFtools-1.12-GCC-10.2.0.eb -r
    From now on, you should be able to see the new module.
    
    (node)$  module spider BCF
    
    -----------------------------------------------------------------------------------------------------
      bio/BCFtools: bio/BCFtools/1.12-GCC-10.2.0
    -----------------------------------------------------------------------------------------------------
        Description:
          Samtools is a suite of programs for interacting with high-throughput sequencing data. BCFtools -
          Reading/writing BCF2/VCF/gVCF files and calling/filtering/summarising SNP and short indel sequence
          variants
       This module can be loaded directly: module load bio/BCFtools/1.12-GCC-10.2.0
    
        Help:
    
          Description
          ===========
          Samtools is a suite of programs for interacting with high-throughput sequencing data.
           BCFtools - Reading/writing BCF2/VCF/gVCF files and calling/filtering/summarising SNP and short indel sequence
           variants
    
    
          More information
          ================
           - Homepage: https://www.htslib.org/
    Tips: When you load a module <NAME> generated by Easybuild, it is installed within the directory reported by the $EBROOT<NAME> variable. In the above case, you will find the generated binary in ${EBROOTBCFTOOLS}/.
    
    Install a missing software with a more recent toolchain
    
    The release of a new software set takes some time and you may wish to use the last toolchain provided by Easybuild.
    
    Let's say we wish to install BCFtools with GCC-12.2.0 (which is part of foss-2022b).
    
    First, you will need to add the following content to your $HOME/.bashrc. We recommend you to add it as a bash function to be able to switch between Easyconfig environement.
    
    load_local_easybuild(){
    # EASYBUILD_PREFIX: [basedir]/<cluster>/<environment>/<arch>
    # Ex: Default EASYBUILD_PREFIX in your home - Adapt to project directory if needed
    _VERSION="${1:-${RESIF_VERSION_PROD}}"
    _EB_PREFIX=$HOME/.local/easybuild
    # ... eventually complemented with cluster
    [ -n "${ULHPC_CLUSTER}" ] && _EB_PREFIX="${_EB_PREFIX}/${ULHPC_CLUSTER}"
    # ... eventually complemented with software set version
    _EB_PREFIX="${_EB_PREFIX}/${_VERSION}"
    # ... eventually complemented with arch
    [ -n "${RESIF_ARCH}" ] && _EB_PREFIX="${_EB_PREFIX}/${RESIF_ARCH}"
    export EASYBUILD_PREFIX="${_EB_PREFIX}"
    export LOCAL_MODULES=${EASYBUILD_PREFIX}/modules/all
    }
    Suppose also that we need the last easybuild version, i.e., v4.7.1. In order to avoid your local version to collide with the module one, we suggest you to install the newest easybuild inside a virtualenv as follows:
    
    # Purge all loaded modules
    module purge
    # use the system python and install Easybuild
    export EB_PYTHON=$(which python3)
    # load local easybuild version
    load_local_easybuild "2022b"
    # double-check the EASYBUILD_PREFIX
    echo ${EASYBUILD_PREFIX}
    # install easybuild
    python3 -m pip install easybuild==4.7.1 --user
    # double-check the installed version
    eb --version
    Now, let's try to install BCFtools with GCC-12.2.0. Using the command eb -S BCFtools, we can check the existing versions as previsouly
    
    # Search for recipes for the missing software
    (node)$ eb -S BCFtools
    == found valid index for /home/users/ekieffer/.local/easybuild/easyconfigs, so using it...
    CFGS1=/home/users/ekieffer/.local/easybuild/easyconfigs
     * $CFGS1/b/BCFtools/BCFtools-1.2_extHTSlib_Makefile.patch
     * $CFGS1/b/BCFtools/BCFtools-1.3-foss-2016a.eb
     * $CFGS1/b/BCFtools/BCFtools-1.3-intel-2016a.eb
     * $CFGS1/b/BCFtools/BCFtools-1.3.1-foss-2016b.eb
     * $CFGS1/b/BCFtools/BCFtools-1.3_extHTSlib_Makefile.patch
     * $CFGS1/b/BCFtools/BCFtools-1.6-foss-2016b.eb
     * $CFGS1/b/BCFtools/BCFtools-1.6-foss-2017b.eb
     * $CFGS1/b/BCFtools/BCFtools-1.6-intel-2017b.eb
     * $CFGS1/b/BCFtools/BCFtools-1.8-GCC-6.4.0-2.28.eb
     * $CFGS1/b/BCFtools/BCFtools-1.9-foss-2018a.eb
     * $CFGS1/b/BCFtools/BCFtools-1.9-foss-2018b.eb
     * $CFGS1/b/BCFtools/BCFtools-1.9-iccifort-2019.1.144-GCC-8.2.0-2.31.1.eb
     * $CFGS1/b/BCFtools/BCFtools-1.9-intel-2018b.eb
     * $CFGS1/b/BCFtools/BCFtools-1.10.2-GCC-8.3.0.eb
     * $CFGS1/b/BCFtools/BCFtools-1.10.2-GCC-9.3.0.eb
     * $CFGS1/b/BCFtools/BCFtools-1.10.2-iccifort-2019.5.281.eb
     * $CFGS1/b/BCFtools/BCFtools-1.11-GCC-10.2.0.eb
     * $CFGS1/b/BCFtools/BCFtools-1.12-GCC-9.3.0.eb
     * $CFGS1/b/BCFtools/BCFtools-1.12-GCC-10.2.0.eb
     * $CFGS1/b/BCFtools/BCFtools-1.12-GCC-10.3.0.eb
     * $CFGS1/b/BCFtools/BCFtools-1.14-GCC-11.2.0.eb
     * $CFGS1/b/BCFtools/BCFtools-1.15.1-GCC-11.3.0.eb
     * $CFGS1/b/BCFtools/BCFtools-1.17-GCC-12.2.0.eb
    
    Note: 4 matching archived easyconfig(s) found, use --consider-archived-easyconfigs to see them
    We are going to install BCFtools-1.17-GCC-12.2.0.eb but let's try with the dry-run option first. We should see that all dependencies are not satisfied.
    
    eb -D BCFtools-1.17-GCC-12.2.0.eb -r
    == Temporary log file in case of crash /tmp/eb-xasu197x/easybuild-up2jaclx.log
    == found valid index for /home/users/ekieffer/.local/easybuild/easyconfigs, so using it...
    == found valid index for /home/users/ekieffer/.local/easybuild/easyconfigs, so using it...
    Dry run: printing build status of easyconfigs and dependencies
    CFGS=/home/users/ekieffer/.local/easybuild/easyconfigs
     * [ ] $CFGS/m/M4/M4-1.4.19.eb (module: devel/M4/1.4.19)
     * [ ] $CFGS/b/Bison/Bison-3.8.2.eb (module: lang/Bison/3.8.2)
     * [ ] $CFGS/f/flex/flex-2.6.4.eb (module: lang/flex/2.6.4)
     * [ ] $CFGS/z/zlib/zlib-1.2.12.eb (module: lib/zlib/1.2.12)
     * [ ] $CFGS/b/binutils/binutils-2.39.eb (module: tools/binutils/2.39)
     * [ ] $CFGS/g/GCCcore/GCCcore-12.2.0.eb (module: compiler/GCCcore/12.2.0)
     * [ ] $CFGS/z/zlib/zlib-1.2.12-GCCcore-12.2.0.eb (module: lib/zlib/1.2.12-GCCcore-12.2.0)
     * [ ] $CFGS/n/ncurses/ncurses-6.3.eb (module: devel/ncurses/6.3)
     * [ ] $CFGS/g/gettext/gettext-0.21.1.eb (module: tools/gettext/0.21.1)
     * [ ] $CFGS/h/help2man/help2man-1.49.2-GCCcore-12.2.0.eb (module: tools/help2man/1.49.2-GCCcore-12.2.0)
     * [ ] $CFGS/m/M4/M4-1.4.19-GCCcore-12.2.0.eb (module: devel/M4/1.4.19-GCCcore-12.2.0)
     * [ ] $CFGS/p/pkgconf/pkgconf-1.8.0.eb (module: devel/pkgconf/1.8.0)
     * [ ] $CFGS/b/Bison/Bison-3.8.2-GCCcore-12.2.0.eb (module: lang/Bison/3.8.2-GCCcore-12.2.0)
     * [ ] $CFGS/o/OpenSSL/OpenSSL-1.1.eb (module: system/OpenSSL/1.1)
     * [ ] $CFGS/f/flex/flex-2.6.4-GCCcore-12.2.0.eb (module: lang/flex/2.6.4-GCCcore-12.2.0)
     * [ ] $CFGS/b/binutils/binutils-2.39-GCCcore-12.2.0.eb (module: tools/binutils/2.39-GCCcore-12.2.0)
     * [ ] $CFGS/c/cURL/cURL-7.86.0-GCCcore-12.2.0.eb (module: tools/cURL/7.86.0-GCCcore-12.2.0)
     * [ ] $CFGS/g/GCC/GCC-12.2.0.eb (module: compiler/GCC/12.2.0)
     * [ ] $CFGS/b/bzip2/bzip2-1.0.8-GCCcore-12.2.0.eb (module: tools/bzip2/1.0.8-GCCcore-12.2.0)
     * [ ] $CFGS/x/XZ/XZ-5.2.7-GCCcore-12.2.0.eb (module: tools/XZ/5.2.7-GCCcore-12.2.0)
     * [ ] $CFGS/g/GSL/GSL-2.7-GCC-12.2.0.eb (module: numlib/GSL/2.7-GCC-12.2.0)
     * [ ] $CFGS/h/HTSlib/HTSlib-1.17-GCC-12.2.0.eb (module: bio/HTSlib/1.17-GCC-12.2.0)
     * [ ] $CFGS/b/BCFtools/BCFtools-1.17-GCC-12.2.0.eb (module: bio/BCFtools/1.17-GCC-12.2.0)
    == Temporary log file(s) /tmp/eb-xasu197x/easybuild-up2jaclx.log* have been removed.
    == Temporary directory /tmp/eb-xasu197x has been removed.
    Now, let's install it with eb BCFtools-1.17-GCC-12.2.0.eb -r. It takes a bit of time to complete since all dependencies need to be installed first.
    
    Build software using a customized EasyConfig file
    
    There are multiple ways to amend an EasyConfig file. Check the --try-* option flags for all the possibilities.
    
    Generally you want to do that when the up-to-date version of the software you want is not available as a recipy within Easybuild. For instance, let's consider the most recent version, i.e., 20230422, of the GNU parallel.
    
    It is not available as module, so let's build it.
    
    First let's check for available easyconfigs recipies if one exist for the expected version:
    
    (node)$> eb -S parallel
    == found valid index for /home/users/ekieffer/.local/easybuild/easyconfigs, so using it...
    CFGS1=/home/users/ekieffer/.local/easybuild/easyconfigs
    [...]
     * $CFGS1/p/parallel/parallel-20141122-GCC-4.9.2.eb
     * $CFGS1/p/parallel/parallel-20150322-GCC-4.9.2.eb
     * $CFGS1/p/parallel/parallel-20150822-GCC-4.9.2.eb
     * $CFGS1/p/parallel/parallel-20160622-foss-2016a.eb
     * $CFGS1/p/parallel/parallel-20170822-intel-2017a.eb
     * $CFGS1/p/parallel/parallel-20171022-intel-2017b.eb
     * $CFGS1/p/parallel/parallel-20171122-foss-2017b.eb
     * $CFGS1/p/parallel/parallel-20171122-intel-2017b.eb
     * $CFGS1/p/parallel/parallel-20180422-intel-2018a.eb
     * $CFGS1/p/parallel/parallel-20180822-foss-2018b.eb
     * $CFGS1/p/parallel/parallel-20181222-intel-2018b.eb
     * $CFGS1/p/parallel/parallel-20190222-GCCcore-7.3.0.eb
     * $CFGS1/p/parallel/parallel-20190622-GCCcore-8.2.0.eb
     * $CFGS1/p/parallel/parallel-20190922-GCCcore-8.3.0.eb
     * $CFGS1/p/parallel/parallel-20200422-GCCcore-9.3.0.eb
     * $CFGS1/p/parallel/parallel-20200522-GCCcore-9.3.0.eb
     * $CFGS1/p/parallel/parallel-20210322-GCCcore-10.2.0.eb
     * $CFGS1/p/parallel/parallel-20210622-GCCcore-10.3.0.eb
     * $CFGS1/p/parallel/parallel-20210722-GCCcore-11.2.0.eb
     * $CFGS1/p/parallel/parallel-20220722-GCCcore-11.3.0.eb
     * $CFGS1/r/R/DMCfun-1.3.0_fix-parallel-detect.patch
     * $CFGS1/w/WRF/WRF_parallel_build_fix.patch
     * $CFGS1/x/Xmipp/Xmipp-3.19.04-Apollo_add_missing_pthread_to_XmippParallel.patch
    
    Note: 7 matching archived easyconfig(s) found, use --consider-archived-easyconfigs to see them
    As you can see, parallel-20220722-GCCcore-11.3.0.eb is a good candidate to start with. It is the most recent easybuild existing in the repository.
    
    We are going to reuse one of the latest EasyConfig available, for instance lets copy $CFGS1/p/parallel/parallel-20220722-GCCcore-11.3.0.eb as it was the most recent. We'll have to make it match the toolchain/compiler available defined previously i.e. GCCcore-12.2.0.
    
    # Work in a dedicated directory
    (node)$> mkdir -p ~/software/parallel
    (node)$> cd ~/software/parallel
    
    # collect the definition of the CFGS1 variable
    (node)$> CFGS1=/home/users/ekieffer/.local/easybuild/easyconfigs
    (node)$> cp $CFGS1/p/parallel/parallel-20220722-GCCcore-11.3.0.eb .
    # Adapt the filename with the target version and your default building environement 
    (node)$> mv parallel-20220722-GCCcore-11.3.0.eb parallel-20230422-GCCcore-12.2.0.eb  
    Please open parallel-20230422-GCCcore-12.2.0.eb with your preferred editor (e.g., vim, nano, emacs). You should see the following:
    
    easyblock = 'ConfigureMake'
    
    name = 'parallel'
    version = '20220722'
    
    homepage = 'https://savannah.gnu.org/projects/parallel/'
    description = """parallel: Build and execute shell commands in parallel"""
    
    toolchain = {'name': 'GCCcore', 'version': '11.3.0'}
    
    source_urls = [GNU_SOURCE]
    sources = [SOURCELOWER_TAR_BZ2]
    checksums = ['0e4083ac0d850c434598c6dfbf98f3b6dd2cc932a3af9269eb1f9323e43af019']
    
    builddependencies = [('binutils', '2.38')]
    
    dependencies = [('Perl', '5.34.1')]
    
    sanity_check_paths = {
        'files': ['bin/parallel'],
        'dirs': []
    }
    
    sanity_check_commands = ["parallel --help"]
    
    moduleclass = 'tools'
    We will need to update the following fields:
    
    version
    toolchain
    builddependencies
    dependencies
    Let's first update the (build)dependencies to match the new GCCcore-12.2.0.
    
    To find the appropriate version for the dependencies, we need to search the available versions for binutils and Perl regarding GCCcore-12.2.0.
    
    # Searching binutils version
    eb -S binutils
    [...]
    * $CFGS1/b/binutils/binutils-2.39-GCCcore-12.2.0.eb
    [...]
    # Searching Perl version
    eb -S Perl
    [...]
    * $CFGS1/p/Perl/Perl-5.36.0-GCCcore-12.2.0.eb
    [...]
    Once the version of all dependencies have been found, we are now able to update parallel-20230422-GCCcore-12.2.0.eb. Before diplaying the last diff between the old easybuild and the new one, we also need to update the checksum. For this purpose, Easybuild implements a nice functionnality which injects directly the right checksum into the eb file.
    
    # Injecting the checksum
    eb --inject-checksums='sha256'  --force parallel-20230422-GCCcore-12.2.0.eb
    == Temporary log file in case of crash /tmp/eb-ed9gqepv/easybuild-c1u3btwk.log
    == found valid index for /home/users/ekieffer/.local/easybuild/easyconfigs, so using it...
    == found valid index for /home/users/ekieffer/.local/easybuild/easyconfigs, so using it...
    == injecting sha256 checksums in /mnt/irisgpfs/users/ekieffer/software/parallel/parallel-20230422-GCCcore-12.2.0.eb
    == fetching sources & patches for parallel-20230422-GCCcore-12.2.0.eb...
    
    WARNING: Found existing checksums in parallel-20230422-GCCcore-12.2.0.eb, overwriting them (due to use of --force)...
    
    == backup of easyconfig file saved to /mnt/irisgpfs/users/ekieffer/software/parallel/parallel-20230422-GCCcore-12.2.0.eb.bak_20230503200710_3555416...
    == injecting sha256 checksums for sources & patches in parallel-20230422-GCCcore-12.2.0.eb...
    == * parallel-20230422.tar.bz2: 9106593d09dc4de0e094b7b14390a309d8fcb1d27104a53814d16937dcbae3c2
    == Temporary log file(s) /tmp/eb-ed9gqepv/easybuild-c1u3btwk.log* have been removed.
    == Temporary directory /tmp/eb-ed9gqepv has been removed.
    Before updating the checksums inside the eb file, Easybuild creates a backup. We are now able to display a diff between the original and the new eb files with the following command diff -u --color parallel-20220722-GCCcore-11.3.0.eb parallel-20230422-GCCcore-12.2.0.eb.
    
    You can also find the new easybuild recipy in the .../tutorials/tools/easybuild/ folder.
    --- parallel-20220722-GCCcore-11.3.0.eb 2023-05-03 20:01:58.538044000 +0200
    +++ parallel-20230422-GCCcore-12.2.0.eb 2023-05-03 20:07:10.034852000 +0200
    @@ -1,20 +1,20 @@
     easyblock = 'ConfigureMake'
    
     name = 'parallel'
    -version = '20220722'
    +version = '20230422'
    
     homepage = 'https://savannah.gnu.org/projects/parallel/'
     description = """parallel: Build and execute shell commands in parallel"""
    
    -toolchain = {'name': 'GCCcore', 'version': '11.3.0'}
    +toolchain = {'name': 'GCCcore', 'version': '12.2.0'}
    
     source_urls = [GNU_SOURCE]
     sources = [SOURCELOWER_TAR_BZ2]
    -checksums = ['0e4083ac0d850c434598c6dfbf98f3b6dd2cc932a3af9269eb1f9323e43af019']
    +checksums = ['9106593d09dc4de0e094b7b14390a309d8fcb1d27104a53814d16937dcbae3c2']
    
    -builddependencies = [('binutils', '2.38')]
    +builddependencies = [('binutils', '2.39')]
    
    -dependencies = [('Perl', '5.34.1')]
    +dependencies = [('Perl', '5.36.0')]
    
     sanity_check_paths = {
         'files': ['bin/parallel'],
    
    You can now build it
    
    (node)$> eb parallel-20230422-GCCcore-12.2.0.eb -Dr 
    == Temporary log file in case of crash /tmp/eb-j2gcfbzy/easybuild-2uaaamz_.log
    == found valid index for /home/users/ekieffer/.local/easybuild/easyconfigs, so using it...
    == found valid index for /home/users/ekieffer/.local/easybuild/easyconfigs, so using it...
    Dry run: printing build status of easyconfigs and dependencies
     * [x] /home/users/ekieffer/.local/easybuild/easyconfigs/m/M4/M4-1.4.19.eb (module: devel/M4/1.4.19)
     * [x] /home/users/ekieffer/.local/easybuild/easyconfigs/b/Bison/Bison-3.8.2.eb (module: lang/Bison/3.8.2)
     * [x] /home/users/ekieffer/.local/easybuild/easyconfigs/f/flex/flex-2.6.4.eb (module: lang/flex/2.6.4)
     * [x] /home/users/ekieffer/.local/easybuild/easyconfigs/z/zlib/zlib-1.2.12.eb (module: lib/zlib/1.2.12)
     * [x] /home/users/ekieffer/.local/easybuild/easyconfigs/b/binutils/binutils-2.39.eb (module: tools/binutils/2.39)
     * [x] /home/users/ekieffer/.local/easybuild/easyconfigs/g/GCCcore/GCCcore-12.2.0.eb (module: compiler/GCCcore/12.2.0)
     * [x] /home/users/ekieffer/.local/easybuild/easyconfigs/z/zlib/zlib-1.2.12-GCCcore-12.2.0.eb (module: lib/zlib/1.2.12-GCCcore-12.2.0)
     * [x] /home/users/ekieffer/.local/easybuild/easyconfigs/h/help2man/help2man-1.49.2-GCCcore-12.2.0.eb (module: tools/help2man/1.49.2-GCCcore-12.2.0)
     * [x] /home/users/ekieffer/.local/easybuild/easyconfigs/m/M4/M4-1.4.19-GCCcore-12.2.0.eb (module: devel/M4/1.4.19-GCCcore-12.2.0)
     * [x] /home/users/ekieffer/.local/easybuild/easyconfigs/b/Bison/Bison-3.8.2-GCCcore-12.2.0.eb (module: lang/Bison/3.8.2-GCCcore-12.2.0)
     * [x] /home/users/ekieffer/.local/easybuild/easyconfigs/f/flex/flex-2.6.4-GCCcore-12.2.0.eb (module: lang/flex/2.6.4-GCCcore-12.2.0)
     * [x] /home/users/ekieffer/.local/easybuild/easyconfigs/b/binutils/binutils-2.39-GCCcore-12.2.0.eb (module: tools/binutils/2.39-GCCcore-12.2.0)
     * [ ] /home/users/ekieffer/.local/easybuild/easyconfigs/g/groff/groff-1.22.4-GCCcore-12.2.0.eb (module: tools/groff/1.22.4-GCCcore-12.2.0)
     * [ ] /home/users/ekieffer/.local/easybuild/easyconfigs/e/expat/expat-2.4.9-GCCcore-12.2.0.eb (module: tools/expat/2.4.9-GCCcore-12.2.0)
     * [ ] /home/users/ekieffer/.local/easybuild/easyconfigs/n/ncurses/ncurses-6.3-GCCcore-12.2.0.eb (module: devel/ncurses/6.3-GCCcore-12.2.0)
     * [ ] /home/users/ekieffer/.local/easybuild/easyconfigs/l/libreadline/libreadline-8.2-GCCcore-12.2.0.eb (module: lib/libreadline/8.2-GCCcore-12.2.0)
     * [x] /home/users/ekieffer/.local/easybuild/easyconfigs/p/pkgconf/pkgconf-1.8.0.eb (module: devel/pkgconf/1.8.0)
     * [x] /home/users/ekieffer/.local/easybuild/easyconfigs/o/OpenSSL/OpenSSL-1.1.eb (module: system/OpenSSL/1.1)
     * [ ] /home/users/ekieffer/.local/easybuild/easyconfigs/d/DB/DB-18.1.40-GCCcore-12.2.0.eb (module: tools/DB/18.1.40-GCCcore-12.2.0)
     * [ ] /home/users/ekieffer/.local/easybuild/easyconfigs/p/Perl/Perl-5.36.0-GCCcore-12.2.0.eb (module: lang/Perl/5.36.0-GCCcore-12.2.0)
     * [ ] /mnt/irisgpfs/users/ekieffer/software/parallel/parallel-20230422-GCCcore-12.2.0.eb (module: tools/parallel/20230422-GCCcore-12.2.0)
    == Temporary log file(s) /tmp/eb-j2gcfbzy/easybuild-2uaaamz_.log* have been removed.
    == Temporary directory /tmp/eb-j2gcfbzy has been removed.
    (node)$> eb parallel-20230422-GCCcore-12.2.0.eb -r
    Check the result:
    
    (node)$> module av parallel
    
    ------------------------------------------------------------------------------------ /home/users/ekieffer/.local/easybuild/aion/2022b/epyc/modules/all -------------------------------------------------------------------------------------
       tools/parallel/20230422-GCCcore-12.2.0
    
    If the avail list is too long consider trying:
    
    "module --default avail" or "ml -d av" to just list the default modules.
    "module overview" or "ml ov" to display the number of modules for each name.
    
    Use "module spider" to find all possible modules and extensions.
    Use "module keyword key1 key2 ..." to search for all possible modules matching any of the "keys".
    
    
    That's all ;-)
    
    Final remaks
    
    This workflow (copying an existing recipy, adapting the filename, the version and the source checksum) covers most of the test cases. Yet sometimes you need to work on a more complex dependency check, in which case you'll need to adapt many eb files. In this case, for each build, you need to instruct Easybuild to search for easyconfigs also in the current directory, in which case you will use:
    
    $> eb <filename>.eb --robot=$PWD:$EASYBUILD_ROBOT -D
    $> eb <filename>.eb --robot=$PWD:$EASYBUILD_ROBOT
    While modifying easyconfig recipes, the following commands can be handy:
    
    eb --avail-module-tools: List all available tools for installing and managing modules.
    eb --list-easyblocks: List Python easyblock files available to execute an easybuild recipe. Select by setting the easyblock parameter in the recipe file. For example, setting easyblock = CMakeMake builds a recipe with Cmake+Make, and easyblock = CMakeNinja builds with CMake+Ninja.
    eb --avail-easyconfig-params --easyblock <block name>: List the setting available for recipe files using the given easyblock.
    Submitting working Easyconfigs to easybuilders
    
    Follow the Official documentations:
    Integration with Github
    Submitting pull requests (--new-pr)
    Uploading test reports (--upload-test-report)
    Updating existing pull requests (--update-pr)
    To go further (to update)
    
    ULHPC/sw: RESIF 3 sources
    EasyBuild homepage
    EasyBuild documentation
    Getting started
    Using EasyBuild
    Step-by-step guide


    The Spack package manager on the UL HPC platform

    Copyright (c) 2014-2023 UL HPC Team hpc-sysadmins@uni.lu
    
    Authors: Emmanuel Kieffer and UL HPC Team hpc-team@uni.lu
    
    Spack
    
    Like EasyBuild, Spack can be used to install softwares on the UL HPC platforms. Spack is a multi platform package manager that builds and installs multiple versions and configurations of software. Spack resolve dependencies and install them like any other package manager you can find on linux platform.
    
    The definition provided by the official documentation is the following one:
    
    "Spack is a multi-platform package manager that builds and installs multiple versions and configurations of software. It works on Linux, macOS, and many supercomputers. Spack is non-destructive: installing a new version of a package does not break existing installations, so many configurations of the same package can coexist.
    
    Spack offers a simple "spec" syntax that allows users to specify versions and configuration options. Package files are written in pure Python, and specs allow package authors to write a single script for many different builds of the same package. With Spack, you can build your software all the ways you want to".
    
    Pre-requisites
    
    Ensure you are able to connect to the UL HPC clusters.
    
    Now you'll need to pull the latest changes in your working copy of the ULHPC/tutorials you should have cloned in ~/git/github.com/ULHPC/tutorials (see "preliminaries" tutorial)
    
    (access)$ export SPACK_ROOT=${HOME}/.spack
    (access)$ git clone -c feature.manyFiles=true https://github.com/spack/spack.git ${SPACK_ROOT}
    (access)$ cat  << EOF >> ${HOME}/.bashrc
    export SPACK_ROOT=\${HOME}/.spack
    if [[ -f \${SPACK_ROOT}/share/spack/setup-env.sh && -n \${SLURM_JOB_ID} ]]; then
        source \${SPACK_ROOT}/share/spack/setup-env.sh
    fi
    EOF
    Now, it's time to get a interative job allocation
    
    (access)$> si --ntasks-per-node 1 -c 4 -t 2:00:00
    # ... or using the HPC School reservation 'hpcschool'if needed  - use 'sinfo -T' to check if active and its name
    # (access)$> si --reservation=hpcschool --ntasks-per-node 1 -c 4 -t 2:00:00
    # Double check if Spack is loaded
    (node)$> 
    spack
    usage: spack [-hkV] [--color {always,never,auto}] COMMAND ...
    
    A flexible package manager that supports multiple versions,
    configurations, platforms, and compilers.
    
    These are common spack commands:
    
    query packages:
      list                  list and search available packages
      info                  get detailed information on a particular package
      find                  list and search installed packages
    
    build packages:
      install               build and install packages
      uninstall             remove installed packages
      gc                    remove specs that are now no longer needed
      spec                  show what would be installed, given a spec
    
    configuration:
      external              manage external packages in Spack configuration
    
    environments:
      env                   manage virtual environments
      view                  project packages to a compact naming scheme on the filesystem.
    
    create packages:
      create                create a new package file
      edit                  open package files in $EDITOR
    
    system:
      arch                  print architecture information about this machine
      audit                 audit configuration files, packages, etc.
      compilers             list available compilers
    
    user environment:
      load                  add package to the user environment
      module                generate/manage module files
      unload                remove package from the user environment
    
    optional arguments:
      --color {always,never,auto}
                            when to colorize output (default: auto)
      -V, --version         show version number and exit
      -h, --help            show this help message and exit
      -k, --insecure        do not check ssl certificates when downloading
    
    more help:
      spack help --all       list all commands and options
      spack help <command>   help on a specific command
      spack help --spec      help on the package specification syntax
      spack docs             open https://spack.rtfd.io/ in a browser
    For all tests and compilation with Spack, you MUST work on a computing node.
    
    Simple package installation with Spack
    
    Installing the last version of a package is as simple as :(node)$ spack install zlib. This will install the most recent version of zlib. You can load it with (node)$ spack load zlib
    
    Spack becomes more complex and more complete when you need to install a specific version with specific dependencies. In fact, you have a large number of possible combinations helping you to be more reproducible.
    
    Some of Spack's features are:
    
    Custom versions & configurations
    Custom dependencies
    Non-destructive installs
    Packages can peacefully coexist
    Creating packages is easy
    Virtual environments
    In this tutorial, we will only consider package installation with custom versions and dependencies. Before starting, we will configure spack to use /dev/shm as build directory to improve compilation performance.
    In order to do it, please open ${SPACK_ROOT}/etc/spack/defaults/config.yaml with you favorite text editor, e.g., vim, emacs, nano.
    Go to the build_cache section (~ line 69) and :
    add first - /dev/shm/$user/spack-stage
    comment - $user_cache_path/stage
    Now, Spack will try to use /dev/shm as first build cache directory. You should see something like this:
    
      build_stage:
        - /dev/shm/$user/spack-stage
        - $tempdir/$user/spack-stage
      #  - $user_cache_path/stage
      # - $spack/var/spack/stage
    Last check, use the command spack config get config | grep "\/dev\/shm" to ensure that the configuration has been updated accordingly.
    
    Part 1: Custom versions and configurations
    
    The spack command spack help --spec is a very useful command to help you remember how to configure your package installation.
    First, check if the package is not already installed with spack find -vpl <package>. If not, you can check if Spack's available packages with spack list <package>
    (node)$ spack find -vpl hdf5
    ==> No package matches the query: hdf5
    (node)$ spack list hdf5  
    hdf5  hdf5-blosc  hdf5-vfd-gds  hdf5-vol-async  hdf5-vol-cache  hdf5-vol-external-passthrough  hdf5-vol-log  r-hdf5array  r-hdf5r  r-rhdf5  r-rhdf5filters  r-rhdf5lib
    ==> 12 packages
    Once you find the package listed in the output of the spack list command, you can get more info on the package, e.g., hdf5
    
    (node)$ spack info hdf5
    CMakePackage:   hdf5
    
    Description:
        HDF5 is a data model, library, and file format for storing and managing
        data. It supports an unlimited variety of datatypes, and is designed for
        flexible and efficient I/O and for high volume and complex data.
    
    Homepage: https://portal.hdfgroup.org
    
    Preferred version:
        1.14.0           https://support.hdfgroup.org/ftp/HDF5/releases/hdf5-1.14/hdf5-1.14.0/src/hdf5-1.14.0.tar.gz
    
    Safe versions:
        develop-1.15     [git] https://github.com/HDFGroup/hdf5.git on branch develop
        develop-1.14     [git] https://github.com/HDFGroup/hdf5.git on branch hdf5_1_14
        develop-1.12     [git] https://github.com/HDFGroup/hdf5.git on branch hdf5_1_12
        develop-1.10     [git] https://github.com/HDFGroup/hdf5.git on branch hdf5_1_10
        develop-1.8      [git] https://github.com/HDFGroup/hdf5.git on branch hdf5_1_8
        1.14.0           https://support.hdfgroup.org/ftp/HDF5/releases/hdf5-1.14/hdf5-1.14.0/src/hdf5-1.14.0.tar.gz
        1.13.3           https://support.hdfgroup.org/ftp/HDF5/releases/hdf5-1.13/hdf5-1.13.3/src/hdf5-1.13.3.tar.gz
        1.13.2           https://support.hdfgroup.org/ftp/HDF5/releases/hdf5-1.13/hdf5-1.13.2/src/hdf5-1.13.2.tar.gz
        1.12.2           https://support.hdfgroup.org/ftp/HDF5/releases/hdf5-1.12/hdf5-1.12.2/src/hdf5-1.12.2.tar.gz
        1.12.1           https://support.hdfgroup.org/ftp/HDF5/releases/hdf5-1.12/hdf5-1.12.1/src/hdf5-1.12.1.tar.gz
        1.12.0           https://support.hdfgroup.org/ftp/HDF5/releases/hdf5-1.12/hdf5-1.12.0/src/hdf5-1.12.0.tar.gz
        1.10.9           https://support.hdfgroup.org/ftp/HDF5/releases/hdf5-1.10/hdf5-1.10.9/src/hdf5-1.10.9.tar.gz
        1.10.8           https://support.hdfgroup.org/ftp/HDF5/releases/hdf5-1.10/hdf5-1.10.8/src/hdf5-1.10.8.tar.gz
        1.10.7           https://support.hdfgroup.org/ftp/HDF5/releases/hdf5-1.10/hdf5-1.10.7/src/hdf5-1.10.7.tar.gz
        1.10.6           https://support.hdfgroup.org/ftp/HDF5/releases/hdf5-1.10/hdf5-1.10.6/src/hdf5-1.10.6.tar.gz
        1.10.5           https://support.hdfgroup.org/ftp/HDF5/releases/hdf5-1.10/hdf5-1.10.5/src/hdf5-1.10.5.tar.gz
        1.10.4           https://support.hdfgroup.org/ftp/HDF5/releases/hdf5-1.10/hdf5-1.10.4/src/hdf5-1.10.4.tar.gz
        1.10.3           https://support.hdfgroup.org/ftp/HDF5/releases/hdf5-1.10/hdf5-1.10.3/src/hdf5-1.10.3.tar.gz
        1.10.2           https://support.hdfgroup.org/ftp/HDF5/releases/hdf5-1.10/hdf5-1.10.2/src/hdf5-1.10.2.tar.gz
        1.10.1           https://support.hdfgroup.org/ftp/HDF5/releases/hdf5-1.10/hdf5-1.10.1/src/hdf5-1.10.1.tar.gz
        1.10.0-patch1    https://support.hdfgroup.org/ftp/HDF5/releases/hdf5-1.10/hdf5-1.10.0-patch1/src/hdf5-1.10.0-patch1.tar.gz
        1.10.0           https://support.hdfgroup.org/ftp/HDF5/releases/hdf5-1.10/hdf5-1.10.0/src/hdf5-1.10.0.tar.gz
        1.8.22           https://support.hdfgroup.org/ftp/HDF5/releases/hdf5-1.8/hdf5-1.8.22/src/hdf5-1.8.22.tar.gz
        1.8.21           https://support.hdfgroup.org/ftp/HDF5/releases/hdf5-1.8/hdf5-1.8.21/src/hdf5-1.8.21.tar.gz
        1.8.19           https://support.hdfgroup.org/ftp/HDF5/releases/hdf5-1.8/hdf5-1.8.19/src/hdf5-1.8.19.tar.gz
        1.8.18           https://support.hdfgroup.org/ftp/HDF5/releases/hdf5-1.8/hdf5-1.8.18/src/hdf5-1.8.18.tar.gz
        1.8.17           https://support.hdfgroup.org/ftp/HDF5/releases/hdf5-1.8/hdf5-1.8.17/src/hdf5-1.8.17.tar.gz
        1.8.16           https://support.hdfgroup.org/ftp/HDF5/releases/hdf5-1.8/hdf5-1.8.16/src/hdf5-1.8.16.tar.gz
        1.8.15           https://support.hdfgroup.org/ftp/HDF5/releases/hdf5-1.8/hdf5-1.8.15/src/hdf5-1.8.15.tar.gz
        1.8.14           https://support.hdfgroup.org/ftp/HDF5/releases/hdf5-1.8/hdf5-1.8.14/src/hdf5-1.8.14.tar.gz
        1.8.13           https://support.hdfgroup.org/ftp/HDF5/releases/hdf5-1.8/hdf5-1.8.13/src/hdf5-1.8.13.tar.gz
        1.8.12           https://support.hdfgroup.org/ftp/HDF5/releases/hdf5-1.8/hdf5-1.8.12/src/hdf5-1.8.12.tar.gz
        1.8.10           https://support.hdfgroup.org/ftp/HDF5/releases/hdf5-1.8/hdf5-1.8.10/src/hdf5-1.8.10.tar.gz
    
    Deprecated versions:
        None
    
    Variants:
        Name [Default]                 When                              Allowed values          Description
        ===========================    ==============================    ====================    ============================================
    
        api [default]                  --                                default, v116, v114,    Choose api compatibility for earlier version
                                                                         v112, v110, v18, v16
        build_system [cmake]           --                                cmake                   Build systems supported by the package
        build_type [RelWithDebInfo]    [build_system=cmake]              Debug, Release,         CMake build type
                                                                         RelWithDebInfo,
                                                                         MinSizeRel
        cxx [off]                      --                                on, off                 Enable C++ support
        fortran [off]                  --                                on, off                 Enable Fortran support
        generator [make]               [build_system=cmake]              ninja,make              the build system generator to use
        hl [off]                       --                                on, off                 Enable the high-level library
        ipo [off]                      [build_system=cmake               on, off                 CMake interprocedural optimization
                                       ^cmake@3.9:]
        java [off]                     [@1.10:]                          on, off                 Enable Java support
        map [off]                      [@1.14:]                          on, off                 Enable MAP API support
        mpi [on]                       --                                on, off                 Enable MPI support
        shared [on]                    --                                on, off                 Builds a shared version of the library
        szip [off]                     --                                on, off                 Enable szip support
        threadsafe [off]               --                                on, off                 Enable thread-safe capabilities
        tools [on]                     --                                on, off                 Enable building tools
    
    Build Dependencies:
        cmake  gmake  java  mpi  ninja  szip  zlib
    
    Link Dependencies:
        mpi  szip  zlib
    
    Run Dependencies:
        java  pkgconfig
    The spack command spack info <package> provides information regarding the different variants, i.e., possible configurations, and all dependencies. You can then choose to install the default version or customize your installation.
    
    Let's check what the default configuration will install:
    
    # show what would be installed, given a spec
    (node)$ spack spec -Il hdf5
    Input spec
    --------------------------------
     -   hdf5
    
    Concretized
    --------------------------------
     -   be7kscq  hdf5@1.14.0%gcc@8.5.0~cxx~fortran~hl~ipo~java~map+mpi+shared~szip~threadsafe+tools api=default build_system=cmake build_type=RelWithDebInfo generator=make patches=0b5dd6f arch=linux-rhel8-zen
     -   hiorsyv      ^cmake@3.26.3%gcc@8.5.0~doc+ncurses+ownlibs~qt build_system=generic build_type=Release arch=linux-rhel8-zen
     -   3qm6oyl          ^ncurses@6.4%gcc@8.5.0~symlinks+termlib abi=none build_system=autotools arch=linux-rhel8-zen
     -   mwndgpr          ^openssl@1.1.1t%gcc@8.5.0~docs~shared build_system=generic certs=mozilla arch=linux-rhel8-zen
     -   fhqdfoi              ^ca-certificates-mozilla@2023-01-10%gcc@8.5.0 build_system=generic arch=linux-rhel8-zen
     -   nin2wpc      ^gmake@4.4.1%gcc@8.5.0~guile build_system=autotools arch=linux-rhel8-zen
     -   n2mjwkw      ^openmpi@4.1.5%gcc@8.5.0~atomics~cuda~cxx~cxx_exceptions~gpfs~internal-hwloc~java~legacylaunchers~lustre~memchecker~orterunprefix+romio+rsh~singularity+static+vt+wrapper-rpath build_system=autotools fabrics=none schedulers=none arch=linux-rhel8-zen
     -   lqrfa5n          ^hwloc@2.9.1%gcc@8.5.0~cairo~cuda~gl~libudev+libxml2~netloc~nvml~oneapi-level-zero~opencl+pci~rocm build_system=autotools libs=shared,static arch=linux-rhel8-zen
     -   6w6lkzs              ^libpciaccess@0.17%gcc@8.5.0 build_system=autotools arch=linux-rhel8-zen
     -   cnzqq23                  ^util-macros@1.19.3%gcc@8.5.0 build_system=autotools arch=linux-rhel8-zen
     -   xylxzqj              ^libxml2@2.10.3%gcc@8.5.0~python build_system=autotools arch=linux-rhel8-zen
     -   wv3hjx4                  ^libiconv@1.17%gcc@8.5.0 build_system=autotools libs=shared,static arch=linux-rhel8-zen
     -   r2g2ajd                  ^xz@5.4.1%gcc@8.5.0~pic build_system=autotools libs=shared,static arch=linux-rhel8-zen
     -   jeic37n          ^numactl@2.0.14%gcc@8.5.0 build_system=autotools patches=4e1d78c,62fc8a8,ff37630 arch=linux-rhel8-zen
     -   wbv247s              ^autoconf@2.69%gcc@8.5.0 build_system=autotools patches=35c4492,7793209,a49dd5b arch=linux-rhel8-zen
     -   t7feejp              ^automake@1.16.5%gcc@8.5.0 build_system=autotools arch=linux-rhel8-zen
     -   je4mbwr              ^libtool@2.4.7%gcc@8.5.0 build_system=autotools arch=linux-rhel8-zen
     -   5lv5lpk              ^m4@1.4.19%gcc@8.5.0+sigsegv build_system=autotools patches=9dc5fbd,bfdffa7 arch=linux-rhel8-zen
     -   eoimqmc                  ^diffutils@3.9%gcc@8.5.0 build_system=autotools arch=linux-rhel8-zen
     -   7yewilg                  ^libsigsegv@2.14%gcc@8.5.0 build_system=autotools arch=linux-rhel8-zen
     -   ixliisx          ^openssh@9.3p1%gcc@8.5.0+gssapi build_system=autotools arch=linux-rhel8-zen
     -   ct754qa              ^krb5@1.20.1%gcc@8.5.0+shared build_system=autotools arch=linux-rhel8-zen
     -   5vjavy3                  ^bison@3.8.2%gcc@8.5.0 build_system=autotools arch=linux-rhel8-zen
     -   sllixs3                  ^gettext@0.21.1%gcc@8.5.0+bzip2+curses+git~libunistring+libxml2+tar+xz build_system=autotools arch=linux-rhel8-zen
     -   5pqwl7d                      ^tar@1.34%gcc@8.5.0 build_system=autotools zip=pigz arch=linux-rhel8-zen
     -   4eucki5                          ^pigz@2.7%gcc@8.5.0 build_system=makefile arch=linux-rhel8-zen
     -   jnerfu4                          ^zstd@1.5.5%gcc@8.5.0+programs build_system=makefile compression=none libs=shared,static arch=linux-rhel8-zen
     -   vg5yhf2              ^libedit@3.1-20210216%gcc@8.5.0 build_system=autotools arch=linux-rhel8-zen
     -   qll3x2r              ^libxcrypt@4.4.33%gcc@8.5.0~obsolete_api build_system=autotools arch=linux-rhel8-zen
     -   y5vo5t2          ^perl@5.36.0%gcc@8.5.0+cpanm+open+shared+threads build_system=generic arch=linux-rhel8-zen
     -   uw5w4yh              ^berkeley-db@18.1.40%gcc@8.5.0+cxx~docs+stl build_system=autotools patches=26090f4,b231fcc arch=linux-rhel8-zen
     -   ymcs7ce              ^bzip2@1.0.8%gcc@8.5.0~debug~pic+shared build_system=generic arch=linux-rhel8-zen
     -   seklrqi              ^gdbm@1.23%gcc@8.5.0 build_system=autotools arch=linux-rhel8-zen
     -   zgvmdyi                  ^readline@8.2%gcc@8.5.0 build_system=autotools patches=bbf97f1 arch=linux-rhel8-zen
     -   2jwl6bm          ^pmix@4.2.3%gcc@8.5.0~docs+pmi_backwards_compatibility~python~restful build_system=autotools arch=linux-rhel8-zen
     -   bomqowi              ^libevent@2.1.12%gcc@8.5.0+openssl build_system=autotools arch=linux-rhel8-zen
     -   vr36zlh      ^pkgconf@1.8.0%gcc@8.5.0 build_system=autotools arch=linux-rhel8-zen
     -   426hs7t      ^zlib@1.2.13%gcc@8.5.0+optimize+pic+shared build_system=makefile arch=linux-rhel8-zen
    Spack builds an installation tree: * package already existing will have a [+] in the first column * missing packages but installed upstream will have [^] * unknown packages not found by Spack should be provided as external to Spack
    
    Since we start we a fresh installation, we will need to install everything, i.e, all packages are marked as [-]. The ouput of the spec can be quite complex to read but is really exhaustive. You see all compilations options as well as the architecture and the compiler versions. There is no notion of toolchains (see for Easybuild). Spack automatically detects the architecture. You can explicitely obtain it with the command spack arch, i.e., on Aion you will get linux-rhel8-zen.
    
    Spack will also rely the system gcc compiler, i.e., gcc-8.5.0, to install all packages. You may want to have a newer compiler. Let's say gcc 12.2.0 ... In order to list all compilers seen by Spack, just use the command spack compilers.
    
    (node)$ spack compilers
    ==> Available compilers
    -- gcc rhel8-x86_64 ---------------------------------------------
    gcc@8.5.0
    Let's check first which gcc versions are available ...
    
    (node)$ spack info gcc
    AutotoolsPackage:   gcc
    
    Description:
        The GNU Compiler Collection includes front ends for C, C++, Objective-C,
        Fortran, Ada, and Go, as well as libraries for these languages.
    
    Homepage: https://gcc.gnu.org
    
    Preferred version:  
        13.1.0    https://ftpmirror.gnu.org/gcc/gcc-13.1.0/gcc-13.1.0.tar.xz
    
    Safe versions:  
        master    [git] git://gcc.gnu.org/git/gcc.git on branch master
        13.1.0    https://ftpmirror.gnu.org/gcc/gcc-13.1.0/gcc-13.1.0.tar.xz
        12.2.0    https://ftpmirror.gnu.org/gcc/gcc-12.2.0/gcc-12.2.0.tar.xz
        12.1.0    https://ftpmirror.gnu.org/gcc/gcc-12.1.0/gcc-12.1.0.tar.xz
        11.3.0    https://ftpmirror.gnu.org/gcc/gcc-11.3.0/gcc-11.3.0.tar.xz
        11.2.0    https://ftpmirror.gnu.org/gcc/gcc-11.2.0/gcc-11.2.0.tar.xz
        11.1.0    https://ftpmirror.gnu.org/gcc/gcc-11.1.0/gcc-11.1.0.tar.xz
        10.4.0    https://ftpmirror.gnu.org/gcc/gcc-10.4.0/gcc-10.4.0.tar.xz
        10.3.0    https://ftpmirror.gnu.org/gcc/gcc-10.3.0/gcc-10.3.0.tar.xz
        10.2.0    https://ftpmirror.gnu.org/gcc/gcc-10.2.0/gcc-10.2.0.tar.xz
        10.1.0    https://ftpmirror.gnu.org/gcc/gcc-10.1.0/gcc-10.1.0.tar.xz
        9.5.0     https://ftpmirror.gnu.org/gcc/gcc-9.5.0/gcc-9.5.0.tar.xz
        9.4.0     https://ftpmirror.gnu.org/gcc/gcc-9.4.0/gcc-9.4.0.tar.xz
        9.3.0     https://ftpmirror.gnu.org/gcc/gcc-9.3.0/gcc-9.3.0.tar.xz
        9.2.0     https://ftpmirror.gnu.org/gcc/gcc-9.2.0/gcc-9.2.0.tar.xz
        9.1.0     https://ftpmirror.gnu.org/gcc/gcc-9.1.0/gcc-9.1.0.tar.xz
        8.5.0     https://ftpmirror.gnu.org/gcc/gcc-8.5.0/gcc-8.5.0.tar.xz
        8.4.0     https://ftpmirror.gnu.org/gcc/gcc-8.4.0/gcc-8.4.0.tar.xz
        8.3.0     https://ftpmirror.gnu.org/gcc/gcc-8.3.0/gcc-8.3.0.tar.xz
        8.2.0     https://ftpmirror.gnu.org/gcc/gcc-8.2.0/gcc-8.2.0.tar.xz
        8.1.0     https://ftpmirror.gnu.org/gcc/gcc-8.1.0/gcc-8.1.0.tar.xz
        7.5.0     https://ftpmirror.gnu.org/gcc/gcc-7.5.0/gcc-7.5.0.tar.xz
        7.4.0     https://ftpmirror.gnu.org/gcc/gcc-7.4.0/gcc-7.4.0.tar.xz
        7.3.0     https://ftpmirror.gnu.org/gcc/gcc-7.3.0/gcc-7.3.0.tar.xz
        7.2.0     https://ftpmirror.gnu.org/gcc/gcc-7.2.0/gcc-7.2.0.tar.xz
        7.1.0     https://ftpmirror.gnu.org/gcc/gcc-7.1.0/gcc-7.1.0.tar.bz2
    [...]
    As expected, gcc-12.2.0 can be install through Spack.
    
    # On Aion, you will be able to speedup compilation with the `-j128` option
    (node)$ spack install -j128 gcc@12.2.0
    (node)$ spack find -vpl gcc
    -- linux-rhel8-zen / gcc@8.5.0 ----------------------------------
    e77dbi5 gcc@12.2.0+binutils+bootstrap~graphite~nvptx~piclibs~profiled~strip build_system=autotools build_type=RelWithDebInfo languages=c,c++,fortran  /mnt/irisgpfs/users/ekieffer/.spack/opt/spack/linux-rhel8-zen/gcc-8.5.0/gcc-12.2.0-e77dbi53lyselilz6paitdkv7osdrpfy
    ==> 1 installed package
    Notice the symbol @ to specify the version you are expecting to be installed.
    
    Now, we need to tell Spack that this package is a compiler which can be used to build new packages
    
    (node)$ spack compiler add "$(spack location -i gcc@12.2.0)"
    ==> Added 1 new compiler to /home/users/ekieffer/.spack/linux/compilers.yaml
        gcc@12.2.0
    ==> Compilers are defined in the following files:
        /home/users/ekieffer/.spack/linux/compilers.yaml
    Now, we are able to built hdf5 with gcc-12.2.0
    
    (node)$ spack install -j128 hdf5 %gcc@12.2.0
    Spack rely on a Domain-Specific Language (DSL) for describing the build parameters and dependencies with which a package was, or should be, built. For more informartion, plase have a look at the Spack Specs and Dependencies documentation. The example below provided by the NERSC documentation is good illustration of this DSL.
    
    NERSC
    
    Package versions:
    
    @version : defines a single version
    @min:max : defines version range (inclusive)
    @min: : defines version min or higher
    @:max : defines up to version max (inclusive)
    
    
    Compilers:
    %compiler : defines the compiler
    %compiler@version : defines the compiler version
    %compiler@min:max : defines the version range for the compiler
    
    
    Compiler flags:
    cflags="flags" : cppflags, cflags, cxxflags, fflags, ldflags, ldlibs
    == : propagate flags to package dependencies
    
    
    Variants:
    +variant : enable a specific variant
    -variant or ~variant : disable a specific variant
    variant=value : set non-boolean variant to value
    variant=value1,value2,value3 : set multi-value variant values
    ++, --, ~~, == : propagate variants to package dependencies
    
    
    Architecture variants:
    platform=platform : linux, darwin, cray, etc.
    os=operating_system : specific operating system
    target=target : specific target processor
    arch=platform-os-target : shortcut for all three above
    
    
    Dependencies:
    ^dependency [constraints] : specify constraints on dependencies
    ^/hash : build with a specific installed dependency
    Now, let's try to built hdf5 with mvapich2 as mpi dependencies. By default, hdf5 built openmpi.
    
    (node)$ spack install hdf5+hl+mpi ^mvapich2
    Check now the difference between the two installed version.
    
    (node)$ spack find -vld  hdf5
    -- linux-rhel8-zen2 / gcc@12.2.0 --------------------------------
    3siutgo hdf5@1.14.0~cxx~fortran~hl~ipo~java~map+mpi+shared~szip~threadsafe+tools api=default build_system=cmake build_type=RelWithDebInfo generator=make patches=0b5dd6f
    6ctbjak     cmake@3.26.3~doc+ncurses+ownlibs~qt build_system=generic build_type=Release
    tzovucj         ncurses@6.4~symlinks+termlib abi=none build_system=autotools
    li4zb27         openssl@1.1.1t~docs~shared build_system=generic certs=mozilla
    sswrjht             ca-certificates-mozilla@2023-01-10 build_system=generic
    jpezcpe     gmake@4.4.1~guile build_system=autotools
    pxe6qhc     openmpi@4.1.5~atomics~cuda~cxx~cxx_exceptions~gpfs~internal-hwloc~java~legacylaunchers~lustre~memchecker~orterunprefix+romio+rsh~singularity+static+vt+wrapper-rpath build_system=autotools fabrics=none schedulers=none
    p45xi47         hwloc@2.9.1~cairo~cuda~gl~libudev+libxml2~netloc~nvml~oneapi-level-zero~opencl+pci~rocm build_system=autotools libs=shared,static
    dx6tnq3             libpciaccess@0.17 build_system=autotools
    ntnwias                 util-macros@1.19.3 build_system=autotools
    porg3lf             libxml2@2.10.3~python build_system=autotools
    zd5q5lz                 libiconv@1.17 build_system=autotools libs=shared,static
    p35gqye                 xz@5.4.1~pic build_system=autotools libs=shared,static
    553vtif         numactl@2.0.14 build_system=autotools patches=4e1d78c,62fc8a8,ff37630
    l4b4gbz             autoconf@2.69 build_system=autotools patches=35c4492,7793209,a49dd5b
    vi3mzkl             automake@1.16.5 build_system=autotools
    m7rnule             libtool@2.4.7 build_system=autotools
    7zr3ffl             m4@1.4.19+sigsegv build_system=autotools patches=9dc5fbd,bfdffa7
    s44lf7l                 diffutils@3.9 build_system=autotools
    ptpchf4                 libsigsegv@2.14 build_system=autotools
    lgvzskp         openssh@9.3p1+gssapi build_system=autotools
    qd626oc             krb5@1.20.1+shared build_system=autotools
    grlxl6k                 bison@3.8.2 build_system=autotools
    fwasmmb                 gettext@0.21.1+bzip2+curses+git~libunistring+libxml2+tar+xz build_system=autotools
    zi3agmq                     tar@1.34 build_system=autotools zip=pigz
    lje7caa                         pigz@2.7 build_system=makefile
    4mz3drs                         zstd@1.5.5+programs build_system=makefile compression=none libs=shared,static
    tyly5bo             libedit@3.1-20210216 build_system=autotools
    yz2pcts             libxcrypt@4.4.33~obsolete_api build_system=autotools
    wntkobh         perl@5.36.0+cpanm+open+shared+threads build_system=generic
    nhoc6pi             berkeley-db@18.1.40+cxx~docs+stl build_system=autotools patches=26090f4,b231fcc
    ms3w5jd             bzip2@1.0.8~debug~pic+shared build_system=generic
    bd7mntw             gdbm@1.23 build_system=autotools
    uz662wp                 readline@8.2 build_system=autotools patches=bbf97f1
    rdll4qk         pmix@4.2.3~docs+pmi_backwards_compatibility~python~restful build_system=autotools
    wed77jw             libevent@2.1.12+openssl build_system=autotools
    q2t65hw     pkgconf@1.8.0 build_system=autotools
    mou7cd5     zlib@1.2.13+optimize+pic+shared build_system=makefile
    
    oxayojp hdf5@1.14.0~cxx~fortran+hl~ipo~java~map+mpi+shared~szip~threadsafe+tools api=default build_system=cmake build_type=RelWithDebInfo generator=make patches=0b5dd6f
    6ctbjak     cmake@3.26.3~doc+ncurses+ownlibs~qt build_system=generic build_type=Release
    tzovucj         ncurses@6.4~symlinks+termlib abi=none build_system=autotools
    li4zb27         openssl@1.1.1t~docs~shared build_system=generic certs=mozilla
    sswrjht             ca-certificates-mozilla@2023-01-10 build_system=generic
    wntkobh             perl@5.36.0+cpanm+open+shared+threads build_system=generic
    nhoc6pi                 berkeley-db@18.1.40+cxx~docs+stl build_system=autotools patches=26090f4,b231fcc
    ms3w5jd                 bzip2@1.0.8~debug~pic+shared build_system=generic
    bd7mntw                 gdbm@1.23 build_system=autotools
    uz662wp                     readline@8.2 build_system=autotools patches=bbf97f1
    jpezcpe     gmake@4.4.1~guile build_system=autotools
    fnh3qls     mvapich2@2.3.7~alloca~cuda~debug~hwlocv2+regcache+wrapperrpath build_system=autotools ch3_rank_bits=32 fabrics=mrail file_systems=auto process_managers=auto threads=multiple
    grlxl6k         bison@3.8.2 build_system=autotools
    s44lf7l             diffutils@3.9 build_system=autotools
    7zr3ffl             m4@1.4.19+sigsegv build_system=autotools patches=9dc5fbd,bfdffa7
    ptpchf4                 libsigsegv@2.14 build_system=autotools
    3nkcsyz         findutils@4.9.0 build_system=autotools patches=440b954
    dx6tnq3         libpciaccess@0.17 build_system=autotools
    m7rnule             libtool@2.4.7 build_system=autotools
    ntnwias             util-macros@1.19.3 build_system=autotools
    porg3lf         libxml2@2.10.3~python build_system=autotools
    zd5q5lz             libiconv@1.17 build_system=autotools libs=shared,static
    p35gqye             xz@5.4.1~pic build_system=autotools libs=shared,static
    6wpwqr5         rdma-core@41.0~ipo+static build_system=cmake build_type=RelWithDebInfo generator=make
    kezio66             libnl@3.3.0 build_system=autotools
    mnpw6rx                 flex@2.6.3+lex~nls build_system=autotools
    cqa2b2a             py-docutils@0.19 build_system=python_pip
    ef5nrmp                 py-pip@23.0 build_system=generic
    2ajcrnb                 py-setuptools@67.6.0 build_system=generic
    7mvi3qw                 py-wheel@0.37.1 build_system=generic
    z7gsvzc                 python@3.10.10+bz2+crypt+ctypes+dbm~debug+libxml2+lzma~nis~optimizations+pic+pyexpat+pythoncmd+readline+shared+sqlite3+ssl~tkinter+uuid+zlib build_system=generic patches=0d98e93,7d40923,f2fd060
    vnnaacu                     expat@2.5.0+libbsd build_system=autotools
    ffgqjcw                         libbsd@0.11.7 build_system=autotools
    27arg6b                             libmd@1.0.4 build_system=autotools
    fwasmmb                     gettext@0.21.1+bzip2+curses+git~libunistring+libxml2+tar+xz build_system=autotools
    zi3agmq                         tar@1.34 build_system=autotools zip=pigz
    lje7caa                             pigz@2.7 build_system=makefile
    4mz3drs                             zstd@1.5.5+programs build_system=makefile compression=none libs=shared,static
    hmewqfx                     libffi@3.4.4 build_system=autotools
    yz2pcts                     libxcrypt@4.4.33~obsolete_api build_system=autotools
    jx26hkw                     sqlite@3.40.1+column_metadata+dynamic_extensions+fts~functions+rtree build_system=autotools
    kr76dbm                     util-linux-uuid@2.38.1 build_system=autotools
    q2t65hw     pkgconf@1.8.0 build_system=autotools
    mou7cd5     zlib@1.2.13+optimize+pic+shared build_system=makefile
    The installation is not destructive. Each version has its own hash value displayed on the first column.
    
    Part 2: Interaction with Schedulers (slurm)
    
    Slurm is the ULHPC supported batch scheduler, this package should never be installed by spack. The ULHPC's slurm has been built with pmix and both tools should be external dependencies for spack.
    
    Why is it so important ?
    
    If you don't specify to Spack the scheduler, you will NOT benefit from the slurm support to start distributed jobs. You will need to use the legacy approach, i.e.,mpirun -np ${SLURM_NTASKS} --hostfile hosts.
    
    In order to add slurm and pmix as externals dependencies, please use the follow bash script.
    
    cat << EOF >> $SPACK_ROOT/etc/spack/packages.yaml
    packages:
        slurm:
            externals:
            - spec: slurm@22.05.5
              prefix: /usr
            buildable: False
        libevent:
            externals:
            - spec: libevent@2.1.8
              prefix: /usr
            buildable: False
        pmix:
            externals:
            - spec: pmix@4.2.3 
              prefix: /usr
            buildable: False
        hwloc:
            externals:
            - spec: hwloc@2.2.0
              prefix: /usr
            buildable: False
    EOF
    Now, let's try to install openmpi with slurm support.
    
    (node)$ spack install -j128 openmpi@4.0.5 +pmi schedulers=slurm  ^pmix@4.2.3 ^hwloc@2.2.0
    (node)$ spack find -vpl openmpi
    Let's try if the slurm support works properly.
    
    cat << EOF >> launcher_osu.sh
    #!/bin/bash -l
    #SBATCH --job-name=mpi_job_test      # Job name
    #SBATCH --cpus-per-task=1            # Number of cores per MPI task
    #SBATCH --nodes=2                    # Maximum number of nodes to be allocated
    #SBATCH --ntasks-per-node=128        # Maximum number of tasks on each node
    #SBATCH --output=mpi_test_%j.log     # Path to the standard output and error files relative to the working directory
    #SBATCH --exclusive
    #SBATCH -p batch
    
    export SRUN_CPUS_PER_TASK=\${SLURM_CPUS_PER_TASK}
    OSU_VERSION="7.1-1"
    OSU_ARCHIVE="osu-micro-benchmarks-\${OSU_VERSION}.tar.gz"
    OSU_URL="https://mvapich.cse.ohio-state.edu/download/mvapich/\${OSU_ARCHIVE}"
    
    if [[ ! -f \${OSU_ARCHIVE} ]];then 
        wget https://mvapich.cse.ohio-state.edu/download/mvapich/osu-micro-benchmarks-7.1-1.tar.gz
        tar -xvf \${OSU_ARCHIVE} 
    fi
    
    # We use the hash since we could have different variants of the same openmpi version
    # Adapt with your hash version
    spack load /xgcbqft
    
    # cd into the extracted folder 
    cd \${OSU_ARCHIVE//.tar.gz/}
    
    # configure
    ./configure CC=\$(which mpicc) CXX=\$(which mpicxx)
    make
    cd ..
    
    srun  \${OSU_ARCHIVE//.tar.gz/}/c/mpi/collective/blocking/osu_alltoall
    EOF
    Start the previous script with the following command sbatch launcher_osu.sh
    You should see something like this
    
    [...]
    # OSU MPI All-to-All Personalized Exchange Latency Test v7.1
    # Datatype: MPI_CHAR.
    # Size       Avg Latency(us)
    1                     478.18
    2                     699.57
    4                    1048.96
    8                    1858.79
    16                   3638.70
    32                   7154.35
    64                  14922.24
    128                 31227.27
    256                 70394.74
    That is all for now :)
    
    Reference
    
    Spack documentation
    Reference for the slurm support
    Spack environments
    
    Spack environments are a feature of the Spack package manager, which is an open-source tool for managing software installations and dependencies on HPC (High-Performance Computing) systems, clusters, and other Unix-based environments. Spack environments provide a way to describe, reproduce, and manage sets of packages and their dependencies in a single, self-contained configuration.
    
    Spack environments have the following key features:
    
    Isolation: Environments create isolated spaces where packages and their dependencies can be installed without interfering with each other or the global Spack installation.
    Reproducibility: Environments can be defined using a YAML configuration file (spack.yaml or spack.lock), which lists the desired packages and their versions. This makes it easy to share and reproduce software installations across different systems or users.
    Flexibility: Users can create multiple environments to manage different sets of packages or software stacks for different projects or tasks.
    Version control: The configuration files can be placed under version control, enabling tracking of changes, collaboration, and easy rollback to previous configurations if needed.
    Integration: Spack environments can be integrated with other tools and workflows, such as continuous integration systems, containerization tools, or job schedulers in HPC environments.
    In summary, Spack environments provide a convenient and flexible way to manage software installations and dependencies, enhancing productivity and collaboration in research and development, particularly in the HPC community.
    
    If you have followed the beginning of this tutorial, you should already have some spack packages:
    
    (node)$ spack find
    -- linux-rhel8-zen / gcc@=8.5.0 ---------------------------------
    autoconf@2.69                ca-certificates-mozilla@2023-01-10  krb5@1.20.1           libtool@2.4.7     openmpi@4.0.5   parallel@20220522  slurm@22.05.5       zlib@1.2.13
    autoconf-archive@2023.02.20  diffutils@3.9                       libedit@3.1-20210216  libxcrypt@4.4.33  openmpi@4.0.5   perl@5.36.0        slurm@22.05.5       zstd@1.5.5
    automake@1.16.5              gdbm@1.23                           libfabric@1.18.0      libxml2@2.10.3    openmpi@4.1.5   pigz@2.7           tar@1.34
    berkeley-db@18.1.40          gettext@0.21.1                      libiconv@1.17         m4@1.4.19         openmpi@4.1.5   pkgconf@1.8.0      ucx@1.14.0
    bison@3.8.2                  gmake@4.4.1                         libpciaccess@0.17     ncurses@6.4       openssh@9.3p1   pmix@4.2.3         util-macros@1.19.3
    bzip2@1.0.8                  hwloc@2.2.0                         libsigsegv@2.14       numactl@2.0.14    openssl@1.1.1t  readline@8.2       xz@5.4.1
    ==> 44 installed packages
    Let's create now an isolated environement
    (node)$ spack env create myenv
    (node)$ spack env list
    ==> 1 environments
        myenv
    An environment is similar as a virtualized Spack instance used to collect and aggregate package installations for a specific project. The concept is very close to virtual environments in python.
    
    The myenvenvironment can be now activated using the following spack command: spack env activate -p myproject. Using -p option will add the activated environment name to your prompt.
    
    You can check in which environment you are located using spack env status
    If you try now spack find, you should not see any installed packages.
    Let's try now to install some packages...
    
    (node)$ spack install zlib
    ==> Error: Cannot install 'zlib' because no matching specs are in the current environment. You can add specs to the environment with 'spack add zlib', or as part of the install command with 'spack install --add zlib'
    In order to install packages, you need first to add specs. Internally, spack will queue the specs to be installed together. Let's add other packages to our environment:
    
    (node)$ spack add tcl 
    ==> Adding tcl to environment myenv
    (node)$ spack add eigen
    ==> Adding eigen to environment myenv
    # Now we can install the packages
    (node)$ spack install
    ==> Concretized tcl
     -   l5ue6gx  tcl@=8.6.12%gcc@=8.5.0 build_system=autotools arch=linux-rhel8-zen
    [+]  426hs7t      ^zlib@=1.2.13%gcc@=8.5.0+optimize+pic+shared build_system=makefile arch=linux-rhel8-zen
    
    ==> Concretized eigen
     -   g3mjusn  eigen@=3.4.0%gcc@=8.5.0~ipo build_system=cmake build_type=RelWithDebInfo generator=make arch=linux-rhel8-zen
     -   3zmanxo      ^cmake@=3.26.3%gcc@=8.5.0~doc+ncurses+ownlibs~qt build_system=generic build_type=Release arch=linux-rhel8-zen
    [+]  3qm6oyl          ^ncurses@=6.4%gcc@=8.5.0~symlinks+termlib abi=none build_system=autotools arch=linux-rhel8-zen
    [+]  vr36zlh              ^pkgconf@=1.8.0%gcc@=8.5.0 build_system=autotools arch=linux-rhel8-zen
    [+]  fd7rnqx          ^openssl@=1.1.1t%gcc@=8.5.0~docs~shared build_system=generic certs=mozilla arch=linux-rhel8-zen
    [+]  fhqdfoi              ^ca-certificates-mozilla@=2023-01-10%gcc@=8.5.0 build_system=generic arch=linux-rhel8-zen
    [+]  yvgdvqo              ^perl@=5.36.0%gcc@=8.5.0+cpanm+open+shared+threads build_system=generic arch=linux-rhel8-zen
    [+]  uw5w4yh                  ^berkeley-db@=18.1.40%gcc@=8.5.0+cxx~docs+stl build_system=autotools patches=26090f4,b231fcc arch=linux-rhel8-zen
    [+]  ymcs7ce                  ^bzip2@=1.0.8%gcc@=8.5.0~debug~pic+shared build_system=generic arch=linux-rhel8-zen
    [+]  eoimqmc                      ^diffutils@=3.9%gcc@=8.5.0 build_system=autotools arch=linux-rhel8-zen
    [+]  wv3hjx4                          ^libiconv@=1.17%gcc@=8.5.0 build_system=autotools libs=shared,static arch=linux-rhel8-zen
    [+]  seklrqi                  ^gdbm@=1.23%gcc@=8.5.0 build_system=autotools arch=linux-rhel8-zen
    [+]  zgvmdyi                      ^readline@=8.2%gcc@=8.5.0 build_system=autotools patches=bbf97f1 arch=linux-rhel8-zen
    [+]  426hs7t              ^zlib@=1.2.13%gcc@=8.5.0+optimize+pic+shared build_system=makefile arch=linux-rhel8-zen
    [+]  nin2wpc      ^gmake@=4.4.1%gcc@=8.5.0~guile build_system=autotools arch=linux-rhel8-zen
    
    [+] /mnt/irisgpfs/users/ekieffer/.spack/opt/spack/linux-rhel8-zen/gcc-8.5.0/zlib-1.2.13-426hs7tsxcfpebed5uqlogma32dbuvj5
    [+] /mnt/irisgpfs/users/ekieffer/.spack/opt/spack/linux-rhel8-zen/gcc-8.5.0/ncurses-6.4-3qm6oylywjcvizw7xyqbkxg33vqtgppp
    [+] /mnt/irisgpfs/users/ekieffer/.spack/opt/spack/linux-rhel8-zen/gcc-8.5.0/gmake-4.4.1-nin2wpcxfbi5jfu56hh2zah4giapsll7
    ==> Installing tcl-8.6.12-l5ue6gxjdrpsoix4fdn6sgcl3bma6qaj
    ==> No binary for tcl-8.6.12-l5ue6gxjdrpsoix4fdn6sgcl3bma6qaj found: installing from source
    ==> Fetching https://mirror.spack.io/_source-cache/archive/26/26c995dd0f167e48b11961d891ee555f680c175f7173ff8cb829f4ebcde4c1a6.tar.gz
    ==> No patches needed for tcl
    ==> tcl: Executing phase: 'autoreconf'
    ==> tcl: Executing phase: 'configure'
    ==> tcl: Executing phase: 'build'
    ==> tcl: Executing phase: 'install'
    ==> tcl: Successfully installed tcl-8.6.12-l5ue6gxjdrpsoix4fdn6sgcl3bma6qaj
      Stage: 1.38s.  Autoreconf: 0.00s.  Configure: 5.56s.  Build: 46.11s.  Install: 4.96s.  Total: 58.11s
    [+] /mnt/irisgpfs/users/ekieffer/.spack/opt/spack/linux-rhel8-zen/gcc-8.5.0/tcl-8.6.12-l5ue6gxjdrpsoix4fdn6sgcl3bma6qaj
    [+] /mnt/irisgpfs/users/ekieffer/.spack/opt/spack/linux-rhel8-zen/gcc-8.5.0/openssl-1.1.1t-fd7rnqxs5w7u6hdple4jdxobefeceevs
    ==> Installing cmake-3.26.3-3zmanxoso62q3bnzejrlnmpna4gas4bk
    ==> No binary for cmake-3.26.3-3zmanxoso62q3bnzejrlnmpna4gas4bk found: installing from source
    ==> Fetching https://github.com/Kitware/CMake/releases/download/v3.26.3/cmake-3.26.3.tar.gz
    ==> No patches needed for cmake
    ==> cmake: Executing phase: 'bootstrap'
    ==> cmake: Executing phase: 'build'
    ==> cmake: Executing phase: 'install'
    ==> cmake: Successfully installed cmake-3.26.3-3zmanxoso62q3bnzejrlnmpna4gas4bk
      Stage: 3.11s.  Bootstrap: 55.36s.  Build: 56.02s.  Install: 4.00s.  Total: 1m 58.79s
    [+] /mnt/irisgpfs/users/ekieffer/.spack/opt/spack/linux-rhel8-zen/gcc-8.5.0/cmake-3.26.3-3zmanxoso62q3bnzejrlnmpna4gas4bk
    ==> Installing eigen-3.4.0-g3mjusnpkiwnt2xu4fhhv4szbocwdfym
    ==> No binary for eigen-3.4.0-g3mjusnpkiwnt2xu4fhhv4szbocwdfym found: installing from source
    ==> Fetching https://mirror.spack.io/_source-cache/archive/85/8586084f71f9bde545ee7fa6d00288b264a2b7ac3607b974e54d13e7162c1c72.tar.gz
    ==> No patches needed for eigen
    ==> eigen: Executing phase: 'cmake'
    ==> eigen: Executing phase: 'build'
    ==> eigen: Executing phase: 'install'
    ==> eigen: Successfully installed eigen-3.4.0-g3mjusnpkiwnt2xu4fhhv4szbocwdfym
      Stage: 1.09s.  Cmake: 6.13s.  Build: 0.23s.  Install: 0.85s.  Total: 8.44s
    [+] /mnt/irisgpfs/users/ekieffer/.spack/opt/spack/linux-rhel8-zen/gcc-8.5.0/eigen-3.4.0-g3mjusnpkiwnt2xu4fhhv4szbocwdfym
    ==> Updating view at /mnt/irisgpfs/users/ekieffer/.spack/var/spack/environments/myenv/.spack-env/view
    When you activate a spack environment, all packages are automatically loaded and accessible. You do not need to call spack load.
    
    "When you install packages into an environment, they are, by default, linked into a single prefix, or view. Activating the environment with spack env activate results in subdirectories from the view being added to PATH, MANPATH, CMAKE_PREFIX_PATH, and other environment variables. This makes the environment easier to use."
    
    (node)$ which tclsh
    /mnt/irisgpfs/users/ekieffer/.spack/var/spack/environments/myenv/.spack-env/view/bin/tclsh
    In order to remove a package, use spack uninstall --remove <package>.
    The contents of environments is tracked by two files:
    spack.yaml: holds the environment configuration (abstract specs to install)
    spack.lock: generated during concretization (full concrete specs)
    You can use both files to transfer your software environement to another one.
    # Copy configuration from the myenv environment
    (node)$ cp ${SPACK_ROOT}/var/spack/environments/myenv/spack.yaml .
    # Create a new environment from the configuration file
    (node)$ spack env create myenv2 spack.yaml
    (node)$ spack env activate myenv2
    # Installing software ... What do you observe ?
    (node)$ spack install
    ==> Concretized tcl
    [+]  l5ue6gx  tcl@=8.6.12%gcc@=8.5.0 build_system=autotools arch=linux-rhel8-zen
    [+]  426hs7t      ^zlib@=1.2.13%gcc@=8.5.0+optimize+pic+shared build_system=makefile arch=linux-rhel8-zen
    
    ==> Concretized eigen
    [+]  g3mjusn  eigen@=3.4.0%gcc@=8.5.0~ipo build_system=cmake build_type=RelWithDebInfo generator=make arch=linux-rhel8-zen
    [+]  3zmanxo      ^cmake@=3.26.3%gcc@=8.5.0~doc+ncurses+ownlibs~qt build_system=generic build_type=Release arch=linux-rhel8-zen
    [+]  3qm6oyl          ^ncurses@=6.4%gcc@=8.5.0~symlinks+termlib abi=none build_system=autotools arch=linux-rhel8-zen
    [+]  vr36zlh              ^pkgconf@=1.8.0%gcc@=8.5.0 build_system=autotools arch=linux-rhel8-zen
    [+]  fd7rnqx          ^openssl@=1.1.1t%gcc@=8.5.0~docs~shared build_system=generic certs=mozilla arch=linux-rhel8-zen
    [+]  fhqdfoi              ^ca-certificates-mozilla@=2023-01-10%gcc@=8.5.0 build_system=generic arch=linux-rhel8-zen
    [+]  yvgdvqo              ^perl@=5.36.0%gcc@=8.5.0+cpanm+open+shared+threads build_system=generic arch=linux-rhel8-zen
    [+]  uw5w4yh                  ^berkeley-db@=18.1.40%gcc@=8.5.0+cxx~docs+stl build_system=autotools patches=26090f4,b231fcc arch=linux-rhel8-zen
    [+]  ymcs7ce                  ^bzip2@=1.0.8%gcc@=8.5.0~debug~pic+shared build_system=generic arch=linux-rhel8-zen
    [+]  eoimqmc                      ^diffutils@=3.9%gcc@=8.5.0 build_system=autotools arch=linux-rhel8-zen
    [+]  wv3hjx4                          ^libiconv@=1.17%gcc@=8.5.0 build_system=autotools libs=shared,static arch=linux-rhel8-zen
    [+]  seklrqi                  ^gdbm@=1.23%gcc@=8.5.0 build_system=autotools arch=linux-rhel8-zen
    [+]  zgvmdyi                      ^readline@=8.2%gcc@=8.5.0 build_system=autotools patches=bbf97f1 arch=linux-rhel8-zen
    [+]  426hs7t              ^zlib@=1.2.13%gcc@=8.5.0+optimize+pic+shared build_system=makefile arch=linux-rhel8-zen
    [+]  nin2wpc      ^gmake@=4.4.1%gcc@=8.5.0~guile build_system=autotools arch=linux-rhel8-zen
    
    ==> All of the packages are already installed
    ==> Updating view at /mnt/irisgpfs/users/ekieffer/.spack/var/spack/environments/myenv2/.spack-env/view
    
    # To deactivate
    (node)$ spack env deactivate
    We can modify packages from an environment without affecting other environments. Environments only link to those existing installations.
    For more details on Spack, please refer to the official documentation.

    An introduction to Conda and environment management

    Copyright (c) 2023 UL HPC Team hpc-sysadmins@uni.lu
    Author: Georgios Kafanas
    
    The objective of this tutorial is to cover the basics of package management with Conda. Conda environments can simultaneously install dependencies from multiple software distributions, such as Python and R. Package management systems native to distributions of Python, R, and Julia will also be covered to demonstrate how Conda can interface with such systems.
    
    In this tutorial the users will learn to:
    
    use Conda environments to manage the software and package dependencies of projects,
    document and exchange Conda environment setups for reproducibility,
    determine which is the best environment management tool given the requirements of a project, and
    instal packages using the facilities available in R, Python, and Julia when these packages are not available in Conda.
    Pre-requisites
    
    This tutorial focuses on generic aspects of package management. It is assumed that you have some basic knowledge of how to use packages in R or Python. The main package management framework used is Conda, although there will be mentions to tools native to R, Python, and Julia. You can use the techniques covered here both in your personal machine and on the UL HPC clusters. If you would like to setup environments in the UL HPC clusters, please ensure that you are able to connect first.
    
    A brief introduction to Conda
    
    You must be familiar with a few concepts to start working with Conda. In brief, these concepts are package managers which are the programs used to create and manage environments, channels which are the repositories that contain the packages from which environments are composed, and distributions which are systems for shipping package managers.
    
    Package managers
    
    Package managers are the programs that install and manage the Conda environments. There are multiple package managers, such as conda, mamba, and micromamba.
    
    The UL HPC centre supports the use of micromamba for the creation and management of personal Conda environments.
    
    Channels
    
    Conda channels are the locations where packages are stored. There are also multiple channels, with some important channels being:
    
    defaults, the default channel,
    anaconda, a mirror of the default channel,
    bioconda, a distribution of bioinformatics software, and
    conda-forge, a community-led collection of recipes, build infrastructure, and distributions for the conda package manager.
    The most useful channel that comes pre-installed in all distributions, is Conda-Forge. Channels are usually hosted in the official Anaconda page, but in some rare occasions custom channels may be used. For instance the default channel is hosted independently from the official Anaconda page. Many channels also maintain web pages with documentation both for their usage and for packages they distribute:
    
    Default Conda channel
    Bioconda
    Conda-Forge
    Distributions
    
    Quite often, the package manager is not distributed on its own, but with a set of packages that are required for the package manager to work, or even with some additional packages that required for most applications. For instance, the conda package manager is distributed with the Miniconda and Anaconda distributions. Miniconda contains the bare minimum packages for the conda package manager to work, and Anaconda contains multiple commonly used packages and a graphical user interface. The relation between these distributions and the package manager is depicted in the following diagram.
    
    
    
    The situation is similar for Mamba distributions. Mamba distributions are supported by Conda-Forge, and their default installation options set-up conda-forge as the default and only channel during installation. The defaults or its mirror anaconda must be explicitly added if required. The distribution using the Mamba package manager was originally distributed as Mambaforge and was recently renamed to Miniforge. Miniforge comes with a minimal set of python packages required by the Mamba package manager. The distribution using the Micromamba package manager ships no accompanying packages, as Micromamba is a standalone executable with no dependencies. Micromamba is using libmamba, a C++ library implementing the Conda API.
    
    The Micromamba package manager
    
    
    The Micromaba package manager is a minimal yet fairly complete implementation of the Conda interface in C++, which is shipped as a standalone executable. The package manager operates strictly on the user-space and thus it requires no special permissions to install packages. It maintains all its files in a couple of places, so uninstalling the package manager itself is also easy. Finally, the package manager is also lightweight and fast.
    
    UL HPC provides support only for the Micromamba package manager.
    
    Installation
    
    A complete guide regarding Micromamba installation can be found in the official documentation. To install micromamaba in the HPC clusters, log in to Aion or Iris. Working on a login node, run the installation script,
    
    "${SHELL}" <(curl -L micro.mamba.pm/install.sh)
    which will install the executable and setup the environment. There are 4 options to select during the installation of Micromamba:
    
    The directory for the installation of the binary file:  Micromamba binary folder? [~/.local/bin] Leave empty and press enter to select the default displayed within brackets. Your .bashrc script should include ~/.local/bin in the $PATH by default.
    The option to add to the environment autocomplete options for micromamba:  Init shell (bash)? [Y/n] Press enter to select the default option Y. This will append a clearly marked section in the .bashrc shell. Do not forget to remove this section when uninstalling Micromamba.
    The option to configure the channels by adding conda-forge:  Configure conda-forge? [Y/n] Press enter to select the default option Y. This will setup the ~/.condarc file with conda-forge as the default channel. Note that Mamba and Micromamba will not use the defaults channel if it is not present in ~/.condarc like conda.
    The option to select the directory where environment information and packages will be stored: Prefix location? [~/micromamba] Press enter to select the default option displayed within brackets.
    To activate the new environment log-out and log-in again. You now can use micromamba in the login and compute nodes, including the auto-completion feature.
    
    Managing environments
    
    As an example we consider the creation and use of an environment for R jobs. The command,
    
    $ micromamba create --name R-project
    creates an environment named R-project. The environment is activated with the command
    
    $ micromamba activate R-project
    anywhere in the file system.
    
    Next, install the base R environment package that contains the R program, and any R packages required by the project. To install packages, first ensure that the R-project environment is active, and then install any package with the command
    
    $ micromamba install <package_name>
    all the required packages. Quite often, the channel where Conda should first look for the package must also be specified. Using the syntax
    
    $ micromamba install --chanell <chanell_1> --channels <chanell_2> <package_name>
    channels are listed in a series of --channel <channel_name> entries and the channels are searched in the order they appear. Using the syntax
    
    $ micromamba install <chanell>::<package_name>
    packages are searched in the specified channel only. Available packages can be found by searching the conda-forge channel.
    
    For instance, the basic functionality of the R software environment is contained in the r-base package. Calling
    
    micromamba install --channel conda-forge r-base
    or
    
    micromamba install conda-forge::r-base
    will install all the components required to run standalone R scripts. More involved scripts use functionality defined in various packages. The R packages are prepended with a prefix 'r-'. Thus, plm becomes r-plm and so on. After all the required packages have been installed, the environment is ready for use.
    
    Packages in the conda-forge channel come with instructions for their installation. Quite often the channel is specified in the installation instructions, conda-forge::<package name> or even -c conda-forge or --channel conda-forge. While the Micromamba installer sets-up conda-forge as the default channel, latter modification in ~/.condarc may change the channel priority. Thus it is a good practice to explicitly specify the source channel when installing a package.
    
    After work in an environment is complete, deactivate the environment,
    
    $ micromamba deactivate
    to ensure that it does not interfere with any other operations. In contrast to modules, Conda is designed to operate with a single environment active at a time. Create one environment for each project, and Conda will ensure that any package that is shared between multiple environments is installed once.
    
    Micromamba supports almost all the subcommands of Conda. For more details see the official documentation.
    
    Using environments in submission scripts
    
    Since all computationally heavy operations must be performed in compute nodes, Conda environments are also used in jobs submitted to the queuing system. You can activate and deactivate environment in various sections of your script.
    
    Returning to the R example, a submission script running a single core R job can use the R-project_name environment as follows:
    
    #SBATCH --job-name R-test-job
    #SBATCH --nodes 1
    #SBATCH --ntasks-per-node 1
    #SBATCH --cpus-per-task 1
    #SBATCH --time=0-02:00:00
    #SBATCH --partition batch
    #SBATCH --qos normal
    
    micromamba activate R-project
    
    echo "Launched at $(date)"
    echo "Job ID: ${SLURM_JOBID}"
    echo "Node list: ${SLURM_NODELIST}"
    echo "Submit dir.: ${SLURM_SUBMIT_DIR}"
    echo "Numb. of cores: ${SLURM_CPUS_PER_TASK}"
    
    export SRUN_CPUS_PER_TASK="${SLURM_CPUS_PER_TASK}"
    export OMP_NUM_THREADS=1
    srun Rscript --no-save --no-restore script.R
    
    micromamba deactivate
    Environment activations in Conda are stacked, and unlike modules, only one environment is active at a time with the rest being pushed down the stack. Consider the following script excerpt.
    
    # Initialization code
    
    micromabma activate python-project
    
    # Code to run a simulation and generate output with Python
    
    micromabma activate R-project
    
    # Code to perform statistical analysis and ploting with R
    
    micromamba deactivate
    
    # Code to save data with Python
    These script creates the following environment stack.
    
    (base)
    |
    | # No software is available here
    |
    +-(python-project) # micromabma activate python-project
    | |
    | | # Only Python is available here
    | |
    | +-(R-project) # micromabma activate R-project
    | | |
    | | | # Only R is available here
    | | |
    | +-+ # micromamba deactivate
    | |
    | | # Only Python is available here
    | |
    We can see that the Python environment (python-project) remains in the stack while the R environment (R-project) is active, and will be broght forth as soon as the R environment is deactivated.
    
    Useful scripting resources
    
    Formatting submission scripts for R (and other systems)
    Exporting and importing environment specifications
    
    An important feature of Conda is that it allows you to export and version control you environment specifications, and recreate the environment on demand.
    
    A description of the software installed in the Conda environment can be exported on demand to a text file.
    In turn, a specification file can be used to populate a new environment, in effect recreating the environment.
    The environment reproducibility is particularly important when you want to have reproducible results, like for instance in a scientific simulation. You can setup and test your application in your local machine, save the environment, and later load the environment in an HPC system, and be sure that the application will behave identically. Conda in the background will ensure that identical packages will be installed.
    
    In Micromaba, you can export the specifications of an environment using the command:
    
    $ micromaba env export --name <environment name>
    By default the command prints to the standard output, but you can redirect the output to a file:
    
    $ micromaba env export --name <environment name> > <environment name>.yaml
    To recreate an environment from a specification file, pass the file as argument to the create command with the --file flag:
    
    $ micromamba env create --name <environment name> --file <environment name>.yaml
    This workflow demonstrates the use of simple text files to store specifications, but Micormamba supports various specification file types. All specification files are text files and can be version controlled with a tool such as Git.
    
    Sources
    
    Micromamba User Guide: Specification files
    Example: Installing Jupyter and managing the dependencies of a notebook with Micromamba
    
    In this example we will create an environment, install Jupyter, and install all the dependencies for our notebooks with Micromamba. Start by creating an environment:
    
    micromamba env create --name jupyter
    Next, install Jupyter in the environment. Have a look at the page for jupyterlab in the conda-forge channel. To install it in your environment call:
    
    micromamba install --name jupyter conda-forge::jupyterlab
    Now activate the environment, create a working directory for your notebooks, and launch Jypyter:
    
    micromamba activate jupyter
    mkdir ~/Documents/notebooks && cd ~/Documents/notebooks
    jupyter lab
    If a webpage appears with the Jupyter lab, the installation worked succeeded!
    
    You may need some Python package in your Jupyter notebook. You can make packages available in your notebook by installing the appropriate package in the Conda environment. For instance, assume that you need pandas and numpy. Searching the conda-forge channel, we can find the package name and installation instruction. With the jupyter environment active, run the command:
    
    micromamba install conda-forge::numpy conda-forge::pandas
    You should now be able to import numpy and pandas in your notebook!
    
    After completing your work, close down the notebook with the command C-c, and deactivate the jupyter Conda environment:
    
    micromamba deactive
    You should now be in your normal operating system environment.
    
    Self management of work environments in UL HPC with Conda
    
    Conda is one of the systems for providing software in UL HPC systems, along with modules and containers. When starting a new project it is important to select the appropriate system.
    
    Before installing any software yourself in user space you should contact the ULHPC High Level Support Team in the service portal [Home > Research > HPC > Software environment > Request expertise] to check if we can install the software in our system. A system wide installation will not consume any of your storage quota, and it will be configured and tested to provide optimal efficiency.
    
    When a Conda environment is useful
    
    There are three aspects of environment management that you should consider when selecting the method with which you will manage your software.
    
    Ease of use: Many software systems whose performance is not critical and are used by relatively few users are not provided though the standard distribution channels of modules or containers. In such cases the easiest installation option is a user side installation with Conda or some similar package management system.
    Reproducibility: Conda and containers can both create reproducible environments, with descriptions of the environment exported and version controlled in text files. However, containers require significant amount of manual configuration to create a reproducible environment and to perform well in a wide range of systems. If your aim is an easily reproducible environment Conda is the superior choice.
    Performance: Conda provides precompiled executables. Even thought multiple configurations are supported, you will not always find an executable tailored to your target system. Modules and containers provided by UL UPC are optimized to ensure performance and stability in our systems, so prefer them.
    Storage limitations in UL HPC
    
    Regardless of installation method, when you install software in user space you are using up your storage quota. Conda environment managers download and store a sizable amount of data to provided packages to the various environments. Even though the package data are shared between the various environments, they still consume space in your or your project's account. There are limits in the storage space and number of files that are available to projects and users in the cluster. Since Conda packages are self managed, you need to clean unused data yourself.
    
    Cleaning up package data
    
    There are two main sources of unused data, compressed archives of packages that Conda stores in its cache when downloading a new package, and data of packages no longer used in any environment. All unused data in Micromoamba can be removed with the command
    
    micromamba clean --all --yes
    where the flag --yes suppresses an interactive dialogue with details about the operations performed. In general you can use the default options with --yes, unless you have manually edited any files in you package data directory (default location ~/micromamba) and you would like to preserve your changes.
    
    Updating environments to remove old package versions
    
    As we create new environments, we often install the latest version of each package. However, if the environments are not updated regularly, we may end up with different versions of the same package across multiple environments. If we have the same version of a package installed in all environments, we can save space by removing unused older versions.
    
    To update a package across all environments, use the command
    
    for e in $(micromamba env list | awk 'FNR>2 {print $1}'); do micromamba update --yes --name $e <package name>; done
    and to update all packages across all environments
    
    for e in $(micromamba env list | awk 'FNR>2 {print $1}'); do micromamba update --yes --name $e --all; done
    where FNR>2 removes the headers in the output of micromamba env list, and is thus sensitive to changes in the user interface of Micromamba.
    
    After updating packages, the clean command can be called to removed the data of unused older package versions.
    
    Sources
    
    Oficial Conda clean documentation
    Understanding Conda clean
    A note about internal workings of Conda
    
    In general, Conda packages are stored in a central directory, and hard links are created in the library directories of any environment that requires the package. Since hard links do not consume space and inodes, Conda is very efficient in its usage of storage space.
    
    Consider for instance the MPFR package used in some environment gaussian_regression. Looking into the Conda installation managed by Micromamba, these are the installed library files:
    
    gkaf@ulhpc-laptop:~/micromamba$ ls -lahFi pkgs/mpfr-4.2.1-h9458935_0/lib/
    total 1.3M
    5286432 drwxr-xr-x 1 gkaf gkaf   94 Oct 25 13:59 ./
    5286426 drwxr-xr-x 1 gkaf gkaf   38 Oct 25 13:59 ../
    5286436 lrwxrwxrwx 1 gkaf gkaf   16 Oct 22 21:47 libmpfr.so -> libmpfr.so.6.2.1*
    5286441 lrwxrwxrwx 1 gkaf gkaf   16 Oct 22 21:47 libmpfr.so.6 -> libmpfr.so.6.2.1*
    5286433 -rwxrwxr-x 7 gkaf gkaf 1.3M Oct 22 21:47 libmpfr.so.6.2.1*
    5286442 drwxr-xr-x 1 gkaf gkaf   14 Oct 25 13:59 pkgconfig/
    Looking into the libraries of the gaussian_regression environment, there is a hard link to the MPFR library:
    
    gkaf@ulhpc-laptop:~/micromamba$ ls -lahFi envs/gaussian_regression/lib/libmpfr.so.6.2.1 
    5286433 -rwxrwxr-x 7 gkaf gkaf 1.3M Oct 22 21:47 envs/gaussian_regression/lib/libmpfr.so.6.2.1*
    You can use the -i flag in ls to print the inode number of a file. Hard links have the same inode number, meaning that they are essentially the same file.
    
    Conda will not automatically check if the files in the pkgs directories must be removed. For instance, when you uninstall a package from an environment, when you delete an environment, or when a package is updated in an environment, only the hard link in the environment directory will change. The files in pkgs will remain even if they are no longer used in any environment. The relevant clean routines check which packages are actually used and remove the unused files.
    
    Environment management best practices
    
    Environment management systems are diverse but support a simple common set of features, which are:
    
    the ability to create and reproduce software environments,
    isolation between environments and between each environment and the system, and
    easy access to multiple sources of software packages.
    The environment of a software system can be categorized in 2 components,
    
    the system comprising of installed software components and environment settings, and
    the packages added to various software components of the environment.
    The system is a superset of the packages in the environment, however, it makes sense to consider them separately as many software distributions provide native managers for the packages they ship.
    
    
    
    Environment management systems usually focus on the management of the system, installing software, setting environment variables and so on. Package management systems on the other hand usually focus on managing the packages installed for some specific software component. The distinction albeit useful is not always clear, and usually environment managers that can also manage the packages of some software systems.
    
    Furthermore, both environment and package management systems can be further subdivided according to the extend of the target environment. The management systems can target
    
    the whole system (system environment management tools), or
    the environment within a single project directory (project environment management tools).
    All project environment management tools and most system environment management tools provide methods to
    
    store the environment setup in a text file that can be version controlled, and
    to recreate the environment from the description in the text file.
    The project environment management tools in particular often automate the function of updating the text file describing the environment in order to automatically initialize the environment when initializing a new instance of the project. Thus project environments are very useful in storing a distributing research projects, since they automate the reproducibility of the project setup.
    
    The overarching theory for environment management tools such as Conda is simple. However, there are implementation details which affect how environment management tools are used. In the following section we present some case studies about practical issues you may encounter with few environment management tools.
    
    Using pip for Python package management
    
    The official package installer for Python is pip. You can use pip to install packages from the Python Package Index (PyPI) and other indexes. With pip you can install packages in 3 modes,
    
    system-wide installation, where a package is available to all users,
    user-wide installation, where a package are installed in a special directory in the user home directory and are available to the user only, and
    environment installation where a package is only available inside the environment where it was installed.
    Python is now part of many Linux distributions, such as Debian. This means now that users cannot (or should not) install packages system-wide using pip. User-wide and venv environment installations are still possible though.
    
    To install a package user-wide use the command:
    
    $ pip install --user <package_name>
    User-wide installations install packages in a directory located at the home directory of the user. Python searches for packages in the user installation path first. This packages are meant to be installed with the system Python and run without root privileges. Thus user-wide installation is appropriate for packages that extend the system functionality for a single user.
    
    User-wide installation is not recommended for all but the system Python environment. User-wide installations rely on system packages, so if you install any package using a Conda environment the package will depend on the Conda environment. Thus, packages installed user-wide with a Conda environment will may be incompatible with the system environment or with any other Conda environment.
    
    Python venv virtual environments
    
    For most applications, functionality should be installed in a virtual environment. To use a venv environment, first initialize the environment in some directory. The official distribution of Python comes packaged with the venv module that can create a virtual environment with the command:
    
    $ python -m venv <path_to_directory>
    You activate the environment with
    
    $ source <path_to_directory>/bin/activate
    and deactivate with:
    
    $ deactivate
    When the environment is active, you can install packages with the command:
    
    $ pip install <package_name>
    This is the same command installing packages system-wide, but because the environment is active the package is installed in the environment directory.
    
    The pip package manager also provides the functionality required to export environment setups in text files and recreate environments from a text file description of the environment setup. To export the environment, activate the environment and use the freeze command:
    
    $ source <path_to_directory>/bin/activate
    (<environment name>) $ pip freeze > <environment name>.yml
    To recreate the environment, create an empty environment,
    
    python -m venv ~/environments/<environment name>
    activate the environment,
    
    source ~/environments/<environment name>/bin/activate
    and install all the required packages from the requirement file:
    
    (<environment name>) $ pip install --requirement /path/to/<environment name>.yml
    When managing an environment, it is often required to upgrade a package to access new functionality or fix a bug. To upgrade a package with pip, simply install/reinstall the package with the --upgrade flag:
    
    (<environment name>) $ pip install --upgrade <package name>
    Note that in Python environments managed with pip, you have to update each package individually. Package management can thus be quote laborious, but there are some methods to speed up the process.
    
    Finally, to delete a virtual environment installation simply deactivate the environment and remove its directory. Thus, assuming that an environment was installed in ~/environments/<environment name>, remove it with:
    
    $ rm -r ~/environments/<environment name>
    With all the features that pip provides, it is a very attractive method for creating and managing environments. However, is often necessary even when you are managing your environments with pip!
    
    Creating a venv environment with Python installed in a Conda environment
    
    In most distributions venv comes packages together with python, however, in some distributions venv is provided as an independent package. In Debian for instance, the system Python package, python3, does not contain the venv module. The module is distributed through the python3-venv package. You may want to avoid using the system package for a variety of reasons:
    
    The PyPI package you want to install may require a version of Python that is not provided by your system.
    You may be in a constrained system where installing a package such as python3-venv is not possible, either because you don't have the rights, or because you are working with some high performance computer where installing a package required installing the package in hundreds of machines.
    You may not want to pollute your system installation. In accordance to the UNIX philosophy use the system ensures that your machine operates correctly, and use the Conda to run the latest application!
    The trick is to install a version of Python in a Conda environment, and use the Python installed in the Conda environment to create venv environments. Start by creating a python environment with the required version for python. For instance:
    
    $ micromamba env create --name <environment name>-python
    $ micromamba install --name <environment name>-python conda-forge::python
    The Python distribution from conda-forge comes with venv installed. Use venv to create your virtual environment:
    
    $ micromamba run --name <environment name>-python python -m venv ~/environments/<environment name>
    It is a good idea to use some standard location to install global environments; a good choice is some directory in your home directory such as:
    
    ~/environments
    Now an environment has been installed with the required version of python.
    
    Any other tool requires the activation of the Conda environment before you are able to activate an environment created by an internal tool, such as venv. In fact, you can activate your venv as follows:
    
    $ micromaba activate <environment name>-python
    (<environment name>-python) $ source ~/environments/<environment name>/bin/activate
    (<environment name>) (<environment name>-python) $
    However, activating the Conda environment is redundant with a venv environment. Yow you can activate the environment simply with:
    
    source ~/environments/<environment name>/bin/active
    (<environment name>) $
    After creating a venv environment you no longer need to interact with the Conda environment, <environment name>-python, except for updating Python itself. This is because pip environments are completely isolated from the system, except for the Python executable. If you browse the environment definition file,
    
    $ cat ~/environments/<environment name>/pyvenv.cfg
    home = /home/gkaf/micromamba/envs/<environment name>-python/bin
    include-system-site-packages = false
    version = 3.8.18
    you can see that system package usage is disabled by default, with the option include-system-site-packages = false. Also, note that the binary directory of the Conda environment that provides the Python executable is noted in the home entry. This is important, as the only package that pip cannot install is python itself. The python executable is selected the environment is created with the command:
    
    $ python -m venv ~/environments/<environment name>
    In fact, listing the contents of ~/environments/<environment name>/bin, there are the symbolic links
    
    python -> /home/gkaf/micromamba/envs/<environmen name>-python/bin/python
    python3 -> python
    that point to the Python installed in our Conda environment. Every other executable and library package installed in our environment, is installed locally, and as the options include-system-site-packages = false suggests, it will shadow any other package in the Conda environment.
    
    Example: Installing Jupyter and managing the dependencies of a notebook with pip
    
    In this example we create an environment, install Jupyter, and install all the dependencies for our notebooks with pip. Start by creating a Conda environment for the Python used in the venv environment:
    
    micromamba env create --name jupyter-python
    Next, install Python in the environment. Have a look at the page for Python in the conda-forge channel. To install it in your environment call:
    
    micromamba install --name jupyter-python conda-forge::python
    Now use the python of the jupyter-python environment to create your venv environment. We use the run command of Conda to execute a single shot command:
    
    micromamba run --name jupyter-python python -m venv ~/environments/jupyter
    We are now ready to activate our venv environment and install our main dependencies:
    
    $ source ~/environments/jupyter/bin/activate
    (jupyter) $ pip install jupyterlab
    Now create a working directory for your notebooks, and launch Jupyter:
    
    (jupyter) $ mkdir ~/Documents/notebooks && cd ~/Documents/notebooks
    (jupyter) $ jupyter lab
    If a webpage appears with the Jupyter lab, the installation worked succeeded!
    
    You may need some Python package in your Jupyter notebook. You can install new packages at your venv environment. For instance, assume that you need pandas and numpy. Activate the environment, and install the dependencies with the command:
    
    (jupyter) $ pip install numpy pandas
    You should now be able to import numpy and pandas in your notebook!
    
    After completing your work, close down the notebook with the command C-c, and deactivate the jupyter environment:
    
    (jupyter) $ deactivate
    You should now be in your normal operating system environment.
    
    Combining Conda with other package management tools
    
    In some cases Conda is used to manage the environment and other tools are used to install pacakges. There are a few reasons why you may want to manage packages with different tools.
    
    You may want to use a project environment management tool. For instance, you may install Python with Conda and use project environments managed with Virtualenv, Pipenv and Poetry. In the case of R you may install R with Conda and manage project environments with Packrat.
    In some cases packages are not available through Conda, but they may be available through other source code or binary distributions. A typical example is Julia where packages are only available trough the Pkg package manager. Similarly, many less popular packages are available through PyPI and can be installed with pip, but they are not available through a Conda channel.
    A list of case studies follows. In these case studies you can find how Conda and other package managers can be combined to install software.
    
    Installing packages with pip in a Conda environment
    
    In this example Conda and pip are used to create an environment for working with with MkDocs. We assume that we want to work with a custom python distribution installed in a Conda environment and we want to install 2 packages,
    
    mkdocs, and
    mkdocs-minify-plugin.
    While mkdocs is available though the conda-forge Conda channel, at the time of witting mkdocs-minify-plugin is only available through PyPI. At this point we have 3 options,
    
    install mkdocs and mkdocs-minify-plugin in a venv environment,
    install mkdocs and mkdocs-minify-plugin in the Conda environment from PyPI with pip, or
    install mkdocs in a Conda and mkdocs-minify-plugin with pip, by installing mkdocs-minify-plugin either
    directly in the Conda environment, or
    in a venv environment end enabling venv to include system packages to access mkdocs.
    To begin with, create the base Conda environment which will provide the Python executable and related tools such as pip and venv.
    
    micromamba env create --name mkdocs-python
    Exporting the environment,
    
    $ micromamba env export --name mkdocs-python
    name: mkdocs-python
    channels:
    dependencies:
    we can see that the environment is empty. Start by installing Python:
    
    micromamba install --name mkdocs-python conda-forge::python
    Exporting the environment specifications,
    
    $ micromamba env export --name mkdocs-python
    name: mkdocs-python
    channels:
    - conda-forge
    dependencies:
    - _libgcc_mutex=0.1=conda_forge
    - _openmp_mutex=4.5=2_gnu
    - bzip2=1.0.8=hd590300_5
    - ca-certificates=2024.2.2=hbcca054_0
    - ld_impl_linux-64=2.40=h41732ed_0
    - libexpat=2.6.2=h59595ed_0
    - libffi=3.4.2=h7f98852_5
    - libgcc-ng=13.2.0=h807b86a_5
    - libgomp=13.2.0=h807b86a_5
    - libnsl=2.0.1=hd590300_0
    - libsqlite=3.45.2=h2797004_0
    - libuuid=2.38.1=h0b41bf4_0
    - libxcrypt=4.4.36=hd590300_1
    - libzlib=1.2.13=hd590300_5
    - ncurses=6.4.20240210=h59595ed_0
    - openssl=3.2.1=hd590300_1
    - pip=24.0=pyhd8ed1ab_0
    - python=3.12.2=hab00c5b_0_cpython
    - readline=8.2=h8228510_1
    - setuptools=69.2.0=pyhd8ed1ab_0
    - tk=8.6.13=noxft_h4845f30_101
    - tzdata=2024a=h0c530f3_0
    - wheel=0.43.0=pyhd8ed1ab_0
    - xz=5.2.6=h166bdaf_0
    we can see that some version of pip (pip=24.0=pyhd8ed1ab_0) is installed, along with other packages.
    
    In all cases we will use pip, the PyPI package manager, that comes packages with Python. Make sure that in each case you start with an empty environment with only Python installed.
    
    Installing all dependencies on a venv environment
    
    The simplest option is to install all the required packages in a venv virtual environment. Start by creating an environment with the venv module of the Python installed in the mkdocs-python environment:
    
    micromamba run --name mkdocs-python python -m venv ~/environments/mkdocs
    Activate the environment,
    
    $ source ~/environment/mkdocs/bin/active
    and export the environment specification,
    
    (mkdocs) $ pip freeze
    which should result in no output since we have not installed ant packages yet. Some infrastructure packages are installed in the environment by default, all installed packages can be printed with the command pip list,
    
    (mkdocs) $ pip list --format=freeze
    pip==24.0
    where the --format option selected ensures that the output if produced in a YAML file format. We can see that in the environment there is the pip package installed. Infrastructure packages are not version tracked as they are required only for pip, and not for packages provided by the environment.
    
    With the venv environment active, install the packages:
    
    (mkdocs) $ pip install --upgrade mkdocs mkdocs-minify-plugin
    Now the pip freeze command should print all the installed packages:
    
    (mkdocs) $ pip freeze
    click==8.1.7
    csscompressor==0.9.5
    ghp-import==2.1.0
    htmlmin2==0.1.13
    Jinja2==3.1.3
    jsmin==3.0.1
    Markdown==3.6
    MarkupSafe==2.1.5
    mergedeep==1.3.4
    mkdocs==1.5.3
    mkdocs-minify-plugin==0.8.0
    packaging==24.0
    pathspec==0.12.1
    platformdirs==4.2.0
    python-dateutil==2.9.0.post0
    PyYAML==6.0.1
    pyyaml_env_tag==0.1
    six==1.16.0
    watchdog==4.0.0
    Again, the only difference with the pip list command is that the package for pip is self is not listed.
    
    Overall, installing the required packages in a separate venv virtual Python environment is the simplest solution. There is a distinction between the packages managed by the Conda package manager and pip, with Conda managing only the Python distribution, and pip managing the environment packages and the pip installation. However, this distinction comes at the cost of replicating package installations, as unlike Conda, package files are not share in venv virtual Python environments.
    
    Installing all dependencies from PyPI with pip on the Conda environment
    
    The pip package manager is in fact able to install packages directly on the Conda environment! When the Conda environment is active, the system environment for pip is the Conda environment. Activate the Conda environment, and list the packages with pip:
    
    $ micromamba activate mkdocs-coda
    (mkdocs-conda) $ pip list --format=freeze
    pip==24.0 
    setuptools==69.2.0
    wheel==0.43.0
    Note that we use the list command instead of the freeze command to export all packages. We observe that the packages
    
    pip,
    setuptools, and
    wheel,
    are installed with the Conda package manager, and are also visible in the output of micromamba env export.
    
    Do not mix package management tools! Packages installed with the Conda package manager should be managed with Conda, and package installed with pip should be managed with pip. All the packages listed so far are installed with the Conda package manager and should not be updated or otherwise altered with pip.
    
    Now install the mkdocs and mkdocs-minify-plugin with pip while the mkdocs-conda environment is active:
    
    (mkdocs-conda) $ pip install --upgrade mkdocs mkdocs-minify-plugin
    Now all the dependencies are installed in the Conda environment, and can be listed with:
    
    (mkdocs-conda) $ pip list --format=freeze
    $ pip list --format=freeze
    click==8.1.7
    csscompressor==0.9.5
    ghp-import==2.1.0
    htmlmin2==0.1.13
    Jinja2==3.1.3
    jsmin==3.0.1
    Markdown==3.6
    MarkupSafe==2.1.5
    mergedeep==1.3.4
    mkdocs==1.5.3
    mkdocs-minify-plugin==0.8.0
    packaging==24.0
    pathspec==0.12.1
    pip==24.0
    platformdirs==4.2.0
    python-dateutil==2.9.0.post0
    PyYAML==6.0.1
    pyyaml_env_tag==0.1
    setuptools==69.2.0
    six==1.16.0
    watchdog==4.0.0
    wheel==0.43.0
    Note that Conda is aware of which packages is meant to manage, listing the Conda packages
    
    (mkdics-conda) $ micromamba env export --name mkdocs-conda
    name: mkdocs-conda
    channels:
    - conda-forge
    dependencies:
    - _libgcc_mutex=0.1=conda_forge
    - _openmp_mutex=4.5=2_gnu
    - bzip2=1.0.8=hd590300_5
    - ca-certificates=2024.2.2=hbcca054_0
    - ld_impl_linux-64=2.40=h41732ed_0
    - libexpat=2.6.2=h59595ed_0
    - libffi=3.4.2=h7f98852_5
    - libgcc-ng=13.2.0=h807b86a_5
    - libgomp=13.2.0=h807b86a_5
    - libnsl=2.0.1=hd590300_0
    - libsqlite=3.45.2=h2797004_0
    - libuuid=2.38.1=h0b41bf4_0
    - libxcrypt=4.4.36=hd590300_1
    - libzlib=1.2.13=hd590300_5
    - ncurses=6.4.20240210=h59595ed_0
    - openssl=3.2.1=hd590300_1
    - pip=24.0=pyhd8ed1ab_0
    - python=3.12.2=hab00c5b_0_cpython
    - readline=8.2=h8228510_1
    - setuptools=69.2.0=pyhd8ed1ab_0
    - tk=8.6.13=noxft_h4845f30_101
    - tzdata=2024a=h0c530f3_0
    - wheel=0.43.0=pyhd8ed1ab_0
    - xz=5.2.6=h166bdaf_0
    we can see that the packages managed by Conda have not changed!
    
    Updating such a combined environment can be tricky. Start by updating all the Conda packages with
    
    micromamba update --name mkdocs-conda --all
    and then active the environment and update all the packages managed by pip one by one. Note that with the Conda environment active
    
    packages that appear in the output of micromamba env export and pip list --format=freeze are managed by Conda, and
    packages that appear in the output of pip list --format=freeze are managed by pip.
    Some Conda package managers integrate some function of the pip interface. For instance, recreating the same environment with official Conda package manager, conda, the packages exported are:
    
    $ conda env export --name mkdocs-python
    name: mkdocs-python
    channels:
      - conda-forge
      - nodefaults
    dependencies:
      - _libgcc_mutex=0.1=conda_forge
      - _openmp_mutex=4.5=2_gnu
      - bzip2=1.0.8=hd590300_5
      - ca-certificates=2024.2.2=hbcca054_0
      - ld_impl_linux-64=2.40=h41732ed_0
      - libexpat=2.6.2=h59595ed_0
      - libffi=3.4.2=h7f98852_5
      - libgcc-ng=13.2.0=h807b86a_5
      - libgomp=13.2.0=h807b86a_5
      - libnsl=2.0.1=hd590300_0
      - libsqlite=3.45.2=h2797004_0
      - libuuid=2.38.1=h0b41bf4_0
      - libxcrypt=4.4.36=hd590300_1
      - libzlib=1.2.13=hd590300_5
      - ncurses=6.4.20240210=h59595ed_0
      - openssl=3.2.1=hd590300_1
      - pip=24.0=pyhd8ed1ab_0
      - python=3.12.2=hab00c5b_0_cpython
      - readline=8.2=h8228510_1
      - setuptools=69.2.0=pyhd8ed1ab_0
      - tk=8.6.13=noxft_h4845f30_101
      - tzdata=2024a=h0c530f3_0
      - wheel=0.43.0=pyhd8ed1ab_0
      - xz=5.2.6=h166bdaf_0
      - pip:
          - click==8.1.7
          - csscompressor==0.9.5
          - ghp-import==2.1.0
          - htmlmin2==0.1.13
          - jinja2==3.1.3
          - jsmin==3.0.1
          - markdown==3.6
          - markupsafe==2.1.5
          - mergedeep==1.3.4
          - mkdocs==1.5.3
          - mkdocs-minify-plugin==0.8.0
          - packaging==24.0
          - pathspec==0.12.1
          - platformdirs==4.2.0
          - python-dateutil==2.9.0.post0
          - pyyaml==6.0.1
          - pyyaml-env-tag==0.1
          - six==1.16.0
          - watchdog==4.0.0
    prefix: /home/gkaf/micromamba/envs/conda/envs/mkdocs-python
    Note that in this output, conda package manager indicates correctly that the packages pip, wheel, and setuptools are managed by conda. Creating a clean environment with the resulting YAML file using the --file option will install the pip packages as well. Even though Micromamba does not support exporting the pip installed dependencies, it supports importing files with pip dependencies for compatibility.
    
    Despite any integration that conda offers for exporting pip installed packages, it is still the responsibility of the user to ensure that packages are managed with the correct package manager.
    
    Mixing packages from Conda channels and PyPI
    
    The mkdocs package is quite popular and as a result it is available through the conda-forge Conda channel. At the time of writing, the package mkdocs-minify-plugin is only available though PyPI. Given that the version on mkdocs in conda-forge is compatible with the version of mkdocs-minify-plugin in PyPI, you may consider installing the mkdocs from conda-forge, and mkdocs-minify-plugin from PyPI.
    
    Start by creating an empty Conda environment for Python, and install Python:
    
    $ micromamba create --name mkdocs-conda
    $ micromamba install --name mkdocs-conda conda-forge::python
    Then, install mkdocs from the conda-forge channel in the mkdocs-conda environment:
    
    $ micromamba install --name mkdocs-conda conda-forge::mkdocs
    You have now installed mkdocs through the conda-forge channel in the Conda environment. There are now 2 options for installing mkdocs-minify-plugin from PyPI, you can install it
    
    in the Conda environment from PyPI with pip, or
    in an isolated venv virtual Python environment.
    The option of installing packages from PyPI in a Conda environment has already been discussed in the previous section. The most interesting option is installing mkdocs-minify-plugin in a venv that has access to the Conda environment package mkdocs.
    
    Create a vanv that has access to system packages with the command:
    
    $ micromamba run --name mkdocs-conda python -m venv --system-site-packages ~/environments/mkdocs
    The resulting configuration configuration file
    
    $ cat ~/environments/mkdocs/pyvenv.cfg 
    home = /home/gkaf/micromamba/envs/mkdocs-conda/bin
    include-system-site-packages = true
    version = 3.12.2
    executable = /home/gkaf/micromamba/envs/mkdocs-conda/bin/python3.12
    command = /home/gkaf/micromamba/envs/mkdocs-conda/bin/python -m venv --system-site-packages /home/gkaf/environments/mkdocs
    has the include-system-site-packages option enabled. Activate the environment and export the environment setup:
    
    $ source ~/environments/mkdocs/bin/activate
    (mkdocs) $ pip freeze
    click @ file:///home/conda/feedstock_root/build_artifacts/click_1692311806742/work
    colorama @ file:///home/conda/feedstock_root/build_artifacts/colorama_1666700638685/work
    ghp-import @ file:///home/conda/feedstock_root/build_artifacts/ghp-import_1651585738538/work
    importlib_metadata @ file:///home/conda/feedstock_root/build_artifacts/importlib-metadata_1710971335535/work
    Jinja2 @ file:///home/conda/feedstock_root/build_artifacts/jinja2_1704966972576/work
    Markdown @ file:///home/conda/feedstock_root/build_artifacts/markdown_1710435156458/work
    MarkupSafe @ file:///home/conda/feedstock_root/build_artifacts/markupsafe_1706899920239/work
    mergedeep @ file:///home/conda/feedstock_root/build_artifacts/mergedeep_1612711302171/work
    mkdocs @ file:///home/conda/feedstock_root/build_artifacts/mkdocs_1695086541719/work
    packaging @ file:///home/conda/feedstock_root/build_artifacts/packaging_1710075952259/work
    pathspec @ file:///home/conda/feedstock_root/build_artifacts/pathspec_1702249949303/work
    platformdirs @ file:///home/conda/feedstock_root/build_artifacts/platformdirs_1706713388748/work
    python-dateutil @ file:///home/conda/feedstock_root/build_artifacts/python-dateutil_1709299778482/work
    PyYAML @ file:///home/conda/feedstock_root/build_artifacts/pyyaml_1695373450623/work
    pyyaml_env_tag @ file:///home/conda/feedstock_root/build_artifacts/pyyaml-env-tag_1624388951658/work
    setuptools==69.2.0
    six @ file:///home/conda/feedstock_root/build_artifacts/six_1620240208055/work
    typing_extensions @ file:///home/conda/feedstock_root/build_artifacts/typing_extensions_1708904622550/work
    watchdog @ file:///home/conda/feedstock_root/build_artifacts/watchdog_1707295114593/work
    wheel==0.43.0
    zipp @ file:///home/conda/feedstock_root/build_artifacts/zipp_1695255097490/work
    The freeze command now list the packages that are provided by the environment and the packages that are provided by the system which in our case is our Conda environment.
    
    To recreate the environment from a YAML file with the environment specifications,
    
    enable the include-system-site-packages option during the creation of the new environment, and
    when installing the packages from the YAML file, ensure that all the dependences are located in the path mentioned in the specification file.
    Despite the face that the package manager exports the dependencies correctly, the user is still responsible for using the correct tool to manage the packages. Listing all the packages in the environment,
    
    (mkdocs) $ pip list --format=freeze
    click==8.1.7
    colorama==0.4.6
    ghp-import==2.1.0
    importlib_metadata==7.1.0
    Jinja2==3.1.3
    Markdown==3.6
    MarkupSafe==2.1.5
    mergedeep==1.3.4
    mkdocs==1.5.3
    packaging==24.0
    pathspec==0.12.1
    pip==24.0
    platformdirs==4.2.0
    python-dateutil==2.9.0
    PyYAML==6.0.1
    pyyaml_env_tag==0.1
    setuptools==69.2.0
    six==1.16.0
    typing_extensions==4.10.0
    watchdog==4.0.0
    wheel==0.43.0
    zipp==3.17.0
    we can see that pip lists all the packages irrespective of whether they are installed in the environment of the system. The advantage of using a venv however, is that any change made with pip will be local to the venv and will simply override without altering the Conda installation.
    
    Combining Conda with pip to install a Python source package
    
    Let's consider the installation of the PySPQR module that wraps the SuiteSparseQR decomposition function for use with SciPy. Installing this software is challenging because
    
    the PySPQR package is not available through Conda channels, and
    installing PySPQR (sparseqr) directly in a venv fails.
    The PySPQR package is a source code package and its installation in a venv requires linking with the SuiteSparse library. We can install SparseSuite in our system (e.g. apt-get install libsuitesparse-dev in Debian), but we will avoid modifying system libraries and use a Conda environment instead.
    
    A Conda environment is a directory with packages where you can install anything you need with a compatible package manager like Micromamba. Create an environment with all the required dependencies:
    
    $ micromamba env create --name python-suitesparse
    $ micromamba install conda-forge::python conda-forge::suitesparse --name python-suitesparse
    The package conda-forge::suitesparse is effectively what apt-get install libsuitesparse-dev installs globally in a Deabian based system, but now it is only available in the Conda environment python-suitesparse.
    
    You are not going to use python-suitesparse directly though! You will use the Python installed in python-suitesparse to create a venv where you will install sparseqr.
    
    Create a venv with the Python of python-suitesparse like this:
    
    $ micromamba run --name python-suitesparse python -m venv ~/environments/PySPQR
    This will create a venv in ~/environments/PySPQR, and since it is created with the python of the python-suitesparse Conda environment, SuiteSparse is now available inside the environment.
    
    Activate your new environment, and install sparseqr with any method you prefer, I have chosen to use the PyPI repo:
    
    $ source ~/environments/PySPQR/bin/activate
    (PySPQR) $ pip install --upgrade setuptools sparseqr
    (PySPQR) $ python
    >>> import sparseqr
    Note that setuptools is also required and must be installed manually in a venv since PySPQR is a source package.
    
    N.B. When installing source packages from PyPI ensure that setuptools is available in your environment. The setuptools library is used to package Python projects, and it is required by source packages to build the source code.
    
    After the first Python import command, your package should compile successfully. After that, the package should be available for import without the need to compile again.
    
    For future use activate the venv environment with
    
    $ source ~/environments/PySPQR/bin/activate
    and deactivate with
    
    (PySPQR) $ deactivate
    Managing packages in R
    
    The R program has a built-in package manager. Assuming that you have access to an installation of R, the R system contains 2 utilities that allow the installation of packages in 3 different modes. First, there is the built-in package manager that can instal packages
    
    in system wide accessible locations for packages that should be available to all users (requires elevated privileges), or
    in user specific location for packages that are accessible to the current user only.
    There are default locations where the built-in package manager searches for packages. The user specific locations take precedence over system locations. The package manager search path can be extended by the user to include bespoke locations. There is also the Packrat package manager which installs packages
    
    in project directories, with the packages being available in an environment isolated within the project directory.
    The Packrat package manager is available as an R package. When creating an environment within a project directory, the environment is activated automatically when starting R in the project directory (but not in its subdirectories due to the implementation of Packrat).
    
    In your local system you can install packages in any mode. In the HPC systems, you can only install packages in the user accessible location, so you are limited to user and project wide installations. Nevertheless, the HPC installation of R includes a number of commonly used packages, such as dbplyr and tidyverse. You should check if the package you require is installed and that the installed version provides the functionality you need before installing any packages locally. Remember, local package installations consume space and inodes against personal or project quota.
    
    Installing R packages locally and globally
    
    Be default R installs packages system wide. When R detects that it does not have write access to the system directories it suggests installing packages for the current user only.
    
    Start and interactive session and then load the R module and start R:
    
    $ module load lang/R
    $ R
    You can list the directories where R is installing and looking for new packages using the function .libPaths()
    
    > .libPaths()
    [1] "/mnt/irisgpfs/apps/resif/aion/2020b/epyc/software/R/4.0.5-foss-2020b/lib64/R/library"
    If you haven't installed any libraries, only the system path appears in the path where R is looking for libraries. Now, try installing for instance the Packrat package globally with the install.packages command.
    
    > install.packages(c("packrat"))
    Warning in install.packages(c("packrat")) :
      'lib = "/mnt/irisgpfs/apps/resif/aion/2020b/epyc/software/R/4.0.5-foss-2020b/lib64/R/library"' is not writable
    Would you like to use a personal library instead? (yes/No/cancel) yes
    Would you like to create a personal library
    ‘~/R/x86_64-pc-linux-gnu-library/4.0’
    to install packages into? (yes/No/cancel) yes
    --- Please select a CRAN mirror for use in this session ---
    Secure CRAN mirrors
    Select any mirror apart from 1: 0-Cloud [https]; usually mirrors closer to your physical location will provide better bandwidth. After selecting a mirror the download and installation of the package proceeds automatically.
    
    Note that after failing to install the package in the system directory, R creates an installation directory for the user in their home directory ~/R/x86_64-pc-linux-gnu-library/4.0 and installs the package for the user only. After the installation, you can check the path where R is looking for packages again.
    
    > .libPaths()
    [1] "/mnt/irisgpfs/users/<user name>/R/x86_64-pc-linux-gnu-library/4.0"
    [2] "/mnt/irisgpfs/apps/resif/aion/2020b/epyc/software/R/4.0.5-foss-2020b/lib64/R/library"
    Now R will look for packages in the user directory first (/mnt/irisgpfs is another path for /home that appears in the ${HOME} variable). Note by the naming convention that R uses when it creates the directory for installing user packages, you can have multiple minor versions of R installed, and their packages will not interfere with each other. For instance,
    
    R version 4.0.5 installs packages in ~/R/x86_64-pc-linux-gnu-library/4.0, and
    R version 4.3.2 installs packages in ~/R/x86_64-pc-linux-gnu-library/4.3.
    Some useful commands for managing packages are,
    
    installed.packages() to list installed packages and various information regarding each package installation,
    old.packages() to list outdated packages,
    update.packages() to update installed packages, and
    remove.packages(c("packrat")) to remove packages.
    To list the loaded packages, use the command
    
    search()
    and to get a detailed description of the environment, use the command
    
    sessionInfo()
    which provides information about the version of R, the OS, and loaded packages.
    
    To load a library that has been installed use the command library. For instance,
    
    library(packrat)
    where you cam notice that the use of quotes is optional and only a single can be loaded at a time. The library function causes an error when the loading of a package fails, so R provides the function require which returns the status of the package loading operation in a return variable, and is design for use inside R functions.
    
    Useful resources
    
    R Packages: A Beginner's Tutorial
    Efficient R programming: Efficient set-up
    Configuring installation paths in R
    
    So far we have only used the default installation paths of R. However, in a local installation where the user has rights to install in the system directories (e.g. in a Conda environment with R) the user installation directory is not created automatically. Open an R session in an interactive session in the HPC cluster or in your personal machine. To get the location where user packages are installed call
    
    > Sys.getenv("R_LIBS_USER")
    [1] "/home/<user name>/R/x86_64-conda-linux-gnu-library/4.3"
    which will print an environment variable, R_LIBS_USER, which is set by R and stores the default location for storing user packages. If you create the directory with
    
    $ mkdir -p /home/<user name>/R/x86_64-conda-linux-gnu-library/4.3
    then you can print the locations where R is searching for packages (after reloading R), and the default location should appear first in the list. For instance for o Conda installation of R using the Micromamba package manager, the paths printed are
    
    > .libPaths()
    [1] "/home/<user name>/R/x86_64-conda-linux-gnu-library/4.3"
    [2] "/home/<user name>/micromamba/envs/R/lib/R/library"
    where R is installed in a Conda environment named R in the second entry of the search path.
    
    There are now multiple locations where packages are stored. The location used by default is the first in the list. Thus, after creating the default location for user installed packages, packages are installed by default in user wide mode. For instance, installing the Packrat package,
    
    > install.packages(c("packrat"))
    listing the user installation directory
    
    $ ls "/home/<user name>/R/x86_64-conda-linux-gnu-library/4.3"
    packrat
    will show the directory with the installed Packrat package files. To install the package in a system wide installation, use the libflag
    
    > install.packages(c("packrat"), lib="/home/<user name>/micromamba/envs/R/lib/R/library")
    to specify the installation location. During loading, all directories in the path are searched consecutively until the package is located.
    
    The package installation paths can also be used to maintain multiple independent environments in R. For instance, you can maintain a personal environment and project environment for your research group. Lets consider the case where you want the create an environment in a project directory. First, create a directory for the R environment
    
    $ mkdir -p "${PROJECTHOME}<project name>/R-environment"
    where the variable PROJECTHOME is defined in the UL HPC system environment to point to the home of the project directories (and includes a trailing slash '/'). To install a package in the project environment, call the installation function with the appropriate lib argument
    
    > install.packages( c("packrat"), lib=paste0( Sys.getenv("PROJECTHOME"), "<project name>/", "R-environment" ) )
    and follow the typical instructions. To load the package, you now must also specify the location of the library,
    
    > library( packrat, lib.loc=paste0( Sys.getenv("PROJECTHOME"), "<project name>/", "R-environment" ) )
    similar to the installation. Environment options can be used to extent the library paths and avoid having to specify the library path in each command.
    
    A startup file mechanism is provided by R to set up user and project wide environment options. There are 2 kinds of file,
    
    .Renviron files used to set-up environment variables for R, and
    .Rprofile files used to run any R code during initialization.
    Note that .Renviron files are simply a list of
    
    key=value
    assignment pairs which are read by R, not proper bash code (adding an export modifier is a syntax error). There are 2 locations where startup files appear,
    
    the home directory, ~/.Renviron and ~/.Rprofile, for user wide settings, and
    project directories for project wide settings.
    The definitions in project .Rprofile files override the user wide definitions in ~/.Rprofile. The definitions in .Renviron files supersede the definitions in ~/.Renviron, that is if the project has an environment file, the user wide definitions are ignored. Note that R is designed to source setup files at the directory where R starts, and any setup files in parent or descendent directories are ignored.
    
    Both the profile and environment startup files can setup a user wide environment. For instance, to use an environment setup in the project directories of the UL HPC systems add in the user wide environment setup file, ~/.Renviron, the entry
    
    R_LIBS=${PROJECTHOME}<project name>/R-environment
    and then reload R. The new library path is
    
    > .libPaths()
    [1] "/mnt/irisgpfs/projects/<project name>/R-environment"
    [2] "/mnt/irisgpfs/users/<user name>/R/x86_64-pc-linux-gnu-library/4.0"
    [3] "/mnt/irisgpfs/apps/resif/iris-rhel8/2020b/broadwell/software/R/4.0.5-foss-2020b/lib64/R/library"
    assuming that all directories appearing in the path exist. Note that the setup file options precede any default options.
    
    We can also use startup files to setup project wide libraries. For instance, assume that we are working on a project in a directory named project and the R packages are stored in a subdirectory R-environment. We use a project profile, to still be able to use any library paths defined in the user wide environment file. Add in a file project/.Rprofile the following definitions,
    
    project_path <- paste0( getwd(), "/R-environment" )
    newpaths <- c( project_path, .libPaths() )
    .libPaths( newpaths )
    and then start R in the project directory. The new library path is
    
    > .libPaths()
    [1] "/mnt/irisgpfs/users/<user name>/Documents/project/R-environment"
    [2] "/mnt/irisgpfs/projects/<project name>/R-environment"
    [3] "/mnt/irisgpfs/users/<user name>/R/x86_64-pc-linux-gnu-library/4.0"
    [4] "/mnt/irisgpfs/apps/resif/iris-rhel8/2020b/broadwell/software/R/4.0.5-foss-2020b/lib64/R/library"
    were the local project settings override the user and system wide settings. This is effectively a local project environment.
    
    Installing packages in R project directories with Packrat
    
    The Packrat library is used to automate the creation and management of project based environments. Packrat also automates operations such as tracking the version of the packages installed in the environment with snapshots, and saving the snapshot information in a text file that can be version controlled. The R distribution available through the UL HPC modules has a fairly old version of Packrat, which nevertheless supports all the basic features. Packrat is a light package, so you can install a more modern version in a user wide mode or in some environment accessible to all the users of a UL HPC project.
    
    To initialize the project, for instance in the directory ~/Documents/project, use the commands:
    
    library(packrat)
    packrat::init("~/Document/project")
    The initialization command creates, - a directory ~/Document/project/packrat to store the packages, and - a setup script ~/Document/project/.Rprofile to initialize the project. Therefore, start R within the project directory ~/Document/packrat, to activate the project environment. After initializing the project or whenever you start R in the project directory, the packrat directory and its subdirectories will be the only ones appearing in the library paths:
    
    > .libPaths()
    [1] "/mnt/irisgpfs/users/<user name>/Documents/project/packrat/lib/x86_64-pc-linux-gnu/4.0.5"    
    [2] "/mnt/irisgpfs/users/<user name>/Documents/project/packrat/lib-ext/x86_64-pc-linux-gnu/4.0.5"
    [3] "/mnt/irisgpfs/users/<user name>/Documents/project/packrat/lib-R/x86_64-pc-linux-gnu/4.0.5"
    Execute all package operations as usual. For instance, to install the plyr package, use the command:
    
    > install.packages(c("plyr"))
    All packages are stored in the packrat subdirectory of the project.
    
    Packrat stores the status of the project in the file packrat/packrat.lock. This file stores the precise package versions that were used to satisfy dependencies, including dependencies of dependencies, and should not be edited by hand. After any change in the installed packages run the command
    
    packrat::snapshot()
    to update the file. You can use the command
    
    packrat::status()
    to analyze the code in the project directory and get a report regarding the status of extraneous or missing packages. After running the status command, you can run
    
    packrat::clean()
    to remove any unused packages. Finally, after restoring the packrat/packrat.lock file from a version control system, or if status detects a missing package, use the command
    
    packrat::restore()
    to install any missing packages.
    
    Useful resources
    
    Official Packrat tutorial
    Issues with managing packages with the native R package managers
    
    The native package manager of R is quite potent, and there are packages such as Packrat that further extend its capabilities. However, there are some drawbacks in installing packages with the native tools. Consider for instance installing the hdf5r package, a package used to read and write binary files, that is quite popular in HPC engineering applications. The installation mode is not important for our demonstration purposes, but assume that you are performing a user wide installation.
    
    > install.packages(c("hdf5r"))
    During the installation, you can see that R is compiling the package components. This can be advantageous is the compilation process is tailored to optimize the build for the underlying system configuration. If you use the module available in the UL HPC systems, it is configured to use the main components of the FOSS tool chain (you can see that by calling module list after loading R), so the compiled packages are well optimized.
    
    N.B. If you encounter any issues with missing packages load the whole FOSS tool chain module with the command,
    
    module load toolchain/foss
    as there are a few popular packages missing in the dependencies of R.
    
    However, if you want to avoid compiling packages from source, which can be quite time consuming, you can use binary distributions of R. These include the distributions provided though native package managers in various Linux distributions, like APT and YUM, as well as Conda package managers like Mamba.


    Data Management on UL HPC Facility

    Copyright (c) 2020-2023 UL HPC Team <hpc-sysadmins@uni.lu>
   Author: Sarah Peter
   
   
   
   Note: To make it clear where you should execute a certain command, the prompt is prefixed with the location, i.e.
   
   (access)$> for commands on the cluster access/login nodes
   (node)$> for commands on a cluster node inside a job
   (laptop)$> for commands locally on your machine
   The actual command comes only after this prefix.
   
   Overview
   
   Requirements
   
   Access to the UL HPC clusters.
   Basic knowledge of the linux command-line.
   Questions
   
   How can I check the my quotas on file sizes and number of files?
   What do "soft quota", "hard quota" and "grace period" mean?
   How can I see how much space is used on a specific file system?
   How can I compute checksums?
   How can I verify checksums?
   How can I encrypt a file?
   How can I decrypt a file?
   How can I encrypt a directory of files?
   How can I read the files in an encrypted directory?
   Objectives
   
   Explain the df and df-ulhpc commands to check disk usage and quota status.
   Compute MD5 and SHA-256 checksums and understand the difference between them.
   Verify MD5 and SHA-256 checksums.
   Encrypt a single file with GPG.
   Decrypt a GPG-encrypted file.
   Encrypt a whole folder with gocryptfs.
   Mount a gocryptfs-encrypted folder to read and write files in it.
   Unmount a gocryptfs-encrypted folder to secure it from unauthorized access.
   Preliminaries
   
   Connect to the cluster.
   
   Quotas
   
   We provide the df-ulhpc command on the cluster login nodes, which displays current usage, soft quota, hard quota and grace period. Any directories that have exceeded the quota will be highlighted in red.
   
   Check your file size quota with:
   
   (access)$> df-ulhpc
   You will see a list of directories on which quotas are applied, how much space you are currently using, your soft quota, hard quota and the grace period.
   
   Once your usage reaches the soft quota you can still write data until the grace period expires (7 days) or you reach the hard quota. After you reach the end of the grace period or the hard quota, you have to reduce your usage to below the soft quota to be able to write data again.
   
   Check your inode quota with:
   
   (access)$> df-ulhpc -i
   Check the free space on all file systems with:
   
   (access)$> df -h
   Check the free space on the current file system with:
   
   (access)$> cd
   (access)$> df -h .
   (access)$> cd /mnt/isilon/projects
   (access)$> df -h .
   To see what directories are using your disk space and quota:
   
   (access)$> cd
   (access)$> ncdu
   Checksums
   
   Integrity of data files is critical for the verifiability of computational and lab-based analyses. The way to seal a data file's content at a point in time is to generate a checksum. Checksum is a small sized datum generated by running an algorithm, called a cryptographic hash function, on a file. As long as a data file does not change, the calculation of the checksum will always result in the same datum. If you recalculate the checksum and it is different from a past calculation, then you know the file has been altered or corrupted in some way.
   
   Below are typical situations that call for checksum generation:
   
   A data file has been newly downloaded or received from a collaborator.
   You have copied data files to a new storage location, for instance you moved data from local computer to HPC to start an analysis. You want to create a snapshot of your data, for instance when you’re creating a supplementary material folder for a paper/report.
   — LCSB How-to Cards
   
   SHA
   
   SHA is short for Secure Hash Algorithm. There are several versions of SHA that have been developed over time: SHA-1, SHA-2 and SHA-3. SHA-2 is a whole family of algorithms that create hash values of different lengths.
   
   The most commonly used version and the one recommended by the National Institute of Standards and Technology (NIST) is SHA-256, which creates a hash value of 256 bits.
   
   First, we need to start an interactive job and prepare some test data:
   
   (access)$> si -t 01:00:00
   (node)$> mkdir -p $SCRATCH/data_management
   (node)$> cd $SCRATCH/data_management
   (node)$> echo 'Happy secure computing!' > message.txt
   We can create the SHA-256 checksum with the following command:
   
   (node)$> sha256sum message.txt
   40d61ef3ba32cc17f2c90db65e6c4d884d220b1999cbded7e80988541c9db11b  message.txt
   Since we want to store the checksum, we should save it to a file:
   
   (node)$> sha256sum message.txt > message.sha256
   Given a data file and its checksum, you can verify the file against the checksum with the following command:
   
   (node)$> sha256sum -c message.sha256
   message.txt: OK
   Let us change the file and see what happens to the checksum:
   
   (node)$> echo "This file has been changed." >> message.txt
   (node)$> sha256sum message.txt
   01eed8eccae1728091ddc6b65fba5016a93757cab11f0c2d5c26b8e4d9321d11  message.txt
   (node)$> sha256sum -c message.sha256
   message.txt: FAILED
   sha256sum: WARNING: 1 computed checksum did NOT match
   MD5
   
   The MD5 message-digest algorithm is a widely used hash function producing a 128-bit hash value. Although MD5 was initially designed to be used as a cryptographic hash function, it has been found to suffer from extensive vulnerabilities. It can still be used as a checksum to verify data integrity, but only against unintentional corruption.
   
   — Wikipedia - MD5
   
   (...) it should not be relied on if there is a chance that files have been purposefully and maliciously tampered. In the latter case, the use of a newer hashing tool such as sha256sum is recommended.
   
   — Wikipedia - Md5sum
   
   We can create the MD5 checksum with the following command:
   
   (node)$> md5sum message.txt > message.md5
   We can verify the file against the checksum similar to above:
   
   (node)$> md5sum -c message.md5
   message.txt: OK
   Encryption
   
   Encryption is an effective measure to protect sensitive data.
   
   IMPORTANT NOTICE:
   
   Encryption keys and passphrases need to be kept safe and protected from unauthorised access.
   Loosing your encryption key means loosing your data.
   Ensure you have an off-site backup of critical data stored on the platform under encryption.
   (Disaster) recovery of encrypted data is not guaranteed to be viable, depending on internal consistency when the recovery snapshot is taken.
   Note that any use of user-level encryption remains under the responsability of the user, with her/him accepting any inherent risks, such as:
   
   loss of access to data due to loss of decryption password/keys
   data corruption due to encryption store corruption, or improper use of the encryption tools
   GPG
   
   We can encrypt our test file using GPG with the following command:
   
   (node)$> gpg -c message.txt
   Since you did not specify with what to encrypt your file, GPG will ask for a passphrase. Enter twice the same passphrase and make sure you remember it (in this case at least until the next step).
   
   This command will also prompt GPG to generate a keyring, if you do not have one yet. The passphrase will be cached for the current SSH session.
   
   This will create the encrypted file message.txt.gpg next to the unencrypted file, so let us delete the unencrypted file:
   
   (node)$> rm message.txt
   You can decrypt the file with the following command:
   
   (node)$> gpg -o message.txt -d message.txt.gpg
   gpg: AES encrypted data
   gpg: encrypted with 1 passphrase
   With older versions of GPG you might see some output telling you that the file was encrypted with CAST5 and a warning that it was not integrity protected.
   
   (node)$> gpg message.txt.gpg
   gpg: CAST5 encrypted data
   gpg: encrypted with 1 passphrase
   gpg: WARNING: message was not integrity protected
   To avoid that warning and since CAST5 is an older algorithm, you can encrypt the file with a newer and stronger algorithm:
   
   (node)$> rm message.txt.gpg
   (node)$> gpg --cipher-algo AES256 -c message.txt
   (node)$> rm message.txt
   (node)$> gpg -o message.txt -d message.txt.gpg
   gpg: AES256 encrypted data
   gpg: encrypted with 1 passphrase
   Optional
   
   Instead of using a passphrase, you can also encrypt files using an encryption key. You create an encryption key with GPG using the following command:
   
   (node)$> gpg --gen-key
   Keep in mind that DSA keys are deprecated, so you should generate a RSA key, ideally with 4096 bits. You will have to enter a couple of other details and then the key will be stored in the keyring.
   
   You can list the contents of your keyring with
   
   (node)$> gpg --list-keys
   If you are going to transfer the data to someone else, you want to encrypt the data with the public key of the recipient, though, and not your own key. You can upload the public keys to public repositories for easier sharing. Never share, upload or transfer private keys!
   
   You can export (the public part of) your key to a file with:
   
   (node)$> gpg --output jane-doe.key --armor --export jane.doe@uni.lu
   Make sure to replace jane-doe with your name and use the email address that you specified when generating the key.
   
   You can import other keys to your keyring using the following command:
   
   (node)$> gpg --import jane-doe.key
   To encrypt using a key, you must specify the email address associated to the key you want to use as recipient:
   
   (node)$> gpg --encrypt --sign --recipient jane.doe@uni.lu message.txt
   Gocryptfs
   
   Gocryptfs is a modern implementation of an encryption overlay filesystem.
   
   For more details on its inner workings, see the following:
   
   Security design documentation
   Threat model
   2017 security audit, Audit report as PDF
   Gocryptfs source code
   To use gocryptfs on the HPC platform you need to do the following steps:
   
   1. Load gocryptfs profile from the modules system
   
   (node)$> module load tools/gocryptfs
   2. Create two folders
   
   dir.crypt, which will act as the storage for the encrypted files (let’s call it crypt)
   dir, which will present (on demand) the unencrypted view (let’s call it view)
   (node)$> cd
   (node)$> mkdir data_management
   (node)$> cd data_management
   (node)$> mkdir dir.crypt dir
   NOTE: Currently gocryptfs does not work well on the Lustre filesystem ($SCRATCH). You need to specify the additional option -noprealloc to use it on Lustre. It works well on SpectrumScale/GPFS ($HOME and project directories), though.
   
   3. Initialize the crypt folder with a password
   
   (node)$> gocryptfs -init dir.crypt
   Choose a password for protecting your files.
   Password: 
   Repeat: 
   
   Your master key is:
   
   e1ecdcf1-6bcebaa0-cbe6cfb8-8e27d4ad-
   acefb9d4-bd98de59-311d1898-31d7e4e4
   
   If the gocryptfs.conf file becomes corrupted or you ever forget your password, there is only one hope for recovery: The master key. Print it to a piece of paper and store it in a drawer. This message is only printed once.
   
   The gocryptfs filesystem has been created successfully.
   You can now mount it using: gocryptfs dir.crypt MOUNTPOINT
   
   (node)$> ls dir.crypt/
   gocryptfs.conf  gocryptfs.diriv
   (node)$> ls dir
   On crypt store initialization, gocryptfs provides us with the master key that can be used to restore access to the data files, especially useful in case the password is lost.
   
   You should keep the master key safe, never store it unencrypted on the platform itself!
   
   After initialization, the crypt store contains two internal configuration files:
   
   gocryptfs.conf is the global configuration for the crypt store, while
   gocryptfs.diriv is created per-directory for encryption of file names
   Note that you should never modify (any) files within the crypt store.
   
   4. Mount the crypt folder into the view folder
   
   To be able to access/store data, the crypt store needs to be mounted in the view folder
   
   this can be done by supplying the initially set password, either on the command line or from a file with -passfile option
   … or with the generated master key, with the -masterkey option
   with the passfile option, it means that you have stored your password unencrypted on the filesystem - this is then a security risk!
   when using the master key mode, you should be in a full-node or exclusive job reservation such that there are no other users able to see the master key in the system
   (node)$> gocryptfs dir.crypt dir
   Password: 
   Decrypting master key
   Filesystem mounted and ready.
   (node)$> ls dir
   5. Add files to the view folder
   
   All your processing (new file/folder creation, modification and transfers) will happen in the view folder.
   
   Once the crypt store is mounted in the view directory we can create files in the latter:
   
   any folder/file created in the unencrypted view will have a 1:1 correspondent in the crypt store
   the plain text message.txt file is stored in encrypted format as 5o_WSYN-Tn59W3vrPiHXEA in the underlying crypt store (file name metadata is encrypted as well)
   the same permissions applied on message.txt are also set for its encrypted correspondent file
   (node)$> echo "Happy secure computing" > dir/message.txt
   (node)$> ls dir
   message.txt
   (node)$> ls dir.crypt/
   5o_WSYN-Tn59W3vrPiHXEA  gocryptfs.conf  gocryptfs.diriv
   6. Unmount the view folder
   
   At the end of our processing, we are using fusermount explicitly to unmount the encrypted overlay, such that the unencrypted view of your data is closed and data is flushed to the regular filesystem.
   
   Note that you should always ensure that this happens before your job reservation expires.
   
   (node)$> fusermount -u dir
   (node)$> ls dir
   (node)$> ls dir.crypt/
   5o_WSYN-Tn59W3vrPiHXEA  gocryptfs.conf  gocryptfs.diriv
   Other important details
   
   Data stored in a crypt store should not be used concurrently (e.g. by multiple users at the same time). The special option -sharedstorage exists for this use-case, but is not guaranteed to work for all applications.
   (Parallel) Applications ran through srun on the Iris cluster cannot "see" the unencrypted view folder as they are run in a different context.
   This is also the case if you use sjoin or srun --jobid to attach your terminal to a running job.
   
   The -init command has an option -plaintextnames to preserve file names.
   Gocryptfs store password management
   
   You can change the password of an existing crypt store with the -passwd option:
   
   (node)$> gocryptfs -passwd dir.crypt/
   Password: [your current password here]
   Decrypting master key
   Please enter your new password.
   Password: [your new password here]
   Repeat: [your new password here]
   Password changed.
   Note that the master key does not change.
   
   For running batch processing on a gocryptfs-based folder, you can provide the decryption password through an external application with the -extpass option:
   
   (node)$> gocryptfs -extpass "echo foobar" dir.crypt dir
   Reading password from extpass program
   Decrypting master key
   Filesystem mounted and ready.
   Note that this means that another application stores/has access to the password - this is then a security risk!
   
   References
   
   Wikipedia - MD5
   Wikipedia - Md5sum
   Wikipedia - SHA-2
   LCSB How-to Card on checksums
   LCSB How-to Card on encryption
   UL HPC blog post on sensitive data encryption
   Acknowledgements
   
   Many thanks to Valentin Plugaru for the initial version of the gocryptfs part and the LCSB data stewards and R3 teams.

   Advanced scheduling with SLURM

 Copyright (c) 2013-2019 UL HPC Team <hpc-sysadmins@uni.lu>


The objective of this tutorial is to practice using the SLURM cluster workload manager in use on the UL HPC iris cluster.

It's important that you read the slides first.

They review, for iris:

the way SLURM was configured, accounting and permissions
common and advanced SLURM tools and commands
SLURM job types
SLURM generic launchers you can use as a base for your own jobs
a comparison of SLURM (iris cluster) and OAR (gaia and chaos)
Part one

You will now get familiar, if not already, with the main tools part of SLURM (otherwise skip down to Part two).

You must first connect to the iris cluster frontend, e.g. with ssh yourlogin@access-iris.uni.lu -p 8022.

Notes:

All the commands used have detailed manuals (man $toolname) that you can always refer to.
To make interactive jobs easier to launch, a function si exists that starts an interactive job with your parameters and the debug QOS
You can override it in your own ~/.bashrc with aliases that customizes it for your particular needs, e.g.
alias si='salloc -p interactive --qos debug --time=0:30:0'
alias six='salloc -p interactive --qos debug --x11'
For the HPC Schools we generally create node reservations named as hpcschool
see information about the reservation with sinfo -T and scontrol show res
generally the reservation is made in the batch partition, thus to the (srun/sbatch) commands below that ask you to run in the batch partition you can prepend --reservation=hpcschool to use it
Partition (queue) and node status

Show queued jobs, show more details ('long' view that includes the job time limit):
squeue
squeue -l
Question: are all jobs visible by default? what does squeue -a do?

Show only the queued jobs of your user ($USER is an environment variable in your shell), then for another specific user:
squeue -u $USER
squeue -u vplugaru
Show queued jobs in a specific partition:
squeue -p $partition
Show queued jobs that are in a specific state (pending / running / failed / preempted, see man squeue for all available states):
squeue -t PD
squeue -t R
squeue -t F
squeue -t PR
Question: what other job states exist?

Show partition status, summarized status (without node state), and node-oriented partition status:
sinfo
sinfo -s
sinfo -N
Questions: What does the 'mix' state in the output of sinfo signify? What will happen to your jobs if the nodes are 'down' or 'drain'? What will you see when looking at a job with squeue or scontrol show job ?

Show node reservations that have been created by the administrators for specific users or accounts:
sinfo -T
Show node details (all nodes, specific node):
scontrol show nodes
scontrol show nodes $nodename
Check the default account your jobs will use:
sacctmgr show user $USER format=user%20s,defaultaccount%30s
See all account associations for your user and the QOS they grant access to:
sacctmgr list association where users=$USER format=account%30s,user%20s,qos%120s
Job submission and management

Starting interactive jobs

Start an interactive job with the default number of cores and walltime:
si
Question: now before exiting the job, what does env | grep SLURM give out? What is the walltime for this job?

Start an interactive job for 3 minutes, with 2 nodes and 4 tasks per node:
si --time=0:03:0 -N 2 --ntasks-per-node=4
Question: can you ssh between the nodes part of this job? what happens if you try to ssh to a different node (not from your job/jobs)? if you are still connected to the job after 3 minutes, what happens?

Start an interactive job with X11 forwarding such that GUI applications (running in the cluster) will be shown on your workstation:
note that your initial connection to the iris cluster needs to have X11 Forwarding enabled, e.g. ssh -Y iris-cluster
Note: as of 2017-11-09, direct X11 (--x11) support with srun (si --x11) is being patched, and the below workaround is needed. You can have a look at the FAQ about X11 forwarding on our website

Connect to iris using the X11 forwarding ssh -Y iris-cluster
Launch a job with X11 by doing an interactive reservation: salloc -p interactive --qos debug bash -c 'ssh -Y $(scontrol show hostnames | head -n 1)'
Here are some explanation of what the command do:

Request node allocation in interactive partition with qos debug
When the resource is allocated, spawn a bash process that will run a command
The command permits to connect to the first node of the reservation directly by using ssh with forwarding enable (-Y option)
You can give extra options at salloc (before the bash -c command) like the number of cores.
Question: what happens if you launch a graphical application (e.g. xterm)? did it appear on your own machine? if not, what went wrong?

Start an interactive job on nodes with Skylake CPUs
salloc -p interactive -C skylake
Question: what partition is this job running in? how many cores and walltime did it reserve?

Collecting job information

Now start a job with one of the previous commands, and you will check its details (runtime metrics, status, after execution statistics, etc.).

Show the details of a job:
scontrol show job $jobid
Question: what happens if you try to take a look at a job which is not in the queue (waiting/running) anymore (e.g. scontrol show job 2)?

Check waiting job priority (detailed view):
sprio -l
Check expected job start time:
squeue --start -u $USER
Show running job (and steps) system-level utilization (memory, I/O, energy):
note that sstat information is limited to your own jobs
sstat -j $jobid
Show specific statistics from a running job (and steps) or multiple jobs:
sstat -j $jobid --format=AveCPU,AveRSS,AveVMSize,MaxRSS,MaxVMSize
sstat -j $jobid1,$jobid2 --format=AveCPU,AveRSS,AveVMSize,MaxRSS,MaxVMSize
Output the statistics in a parseable format, delimited by | (with, then without trailing |):
sstat -p -j $jobid --format=AveCPU,AveRSS,AveVMSize,MaxRSS,MaxVMSize
sstat -P -j $jobid --format=AveCPU,AveRSS,AveVMSize,MaxRSS,MaxVMSize
Show running or completed job (and steps) system-level utilization from the accounting information, and with full details:
sacct -j $jobid
sacct -j $jobid -l
Question: remember that job id #2? can we see its information with sacct -j 2 --format=user and sacct -j 2 -l ?

Show statistics relevant to the job allocation itself not taking steps into consideration, and with more details:
sacct -X -j $jobid
sacct -X -j $jobid -l
Show a subset of interesting statistics from a completed job and its steps, including:
elapsed time in both human readable and total # of seconds
maximum resident set size of all tasks in job (you may want to add also maxrssnode and maxrsstask for a better understanding of which process consumed memory)
maximum virtual memory size (idem for maxvmsizenode and maxvmsizetask)
consumed energy (in Joules), be aware there are many caveats!
your job needs to be the only one running on the corresponding compute nodes
the RAPL mechanism will not take into account all possible hardware elements which consume power (CPUs, GPUs and DRAM are included)
sacct -j $jobid --format=account,user,jobid,jobname,partition,state,elapsed,elapsedraw,start,end,maxrss,maxvmsize,consumedenergy,consumedenergyraw,nnodes,ncpus,nodelist
Output the same statistics in the parseable |-delimited format, for a single and multiple jobs:
sacct -p -j $jobid --format=account,user,jobid,jobname,partition,state,elapsed,elapsedraw,start,end,maxrss,maxvmsize,consumedenergy,consumedenergyraw,nnodes,ncpus,nodelist
sacct -p -j $jobid1,$jobid2 --format=account,user,jobid,jobname,partition,state,elapsed,elapsedraw,start,end,maxrss,maxvmsize,consumedenergy,consumedenergyraw,nnodes,ncpus,nodelist
Show statistics for all personal jobs started since a particular date, then without job steps:
sacct --starttime 2019-06-19 -u $USER
sacct -X --starttime 2019-06-19 -u $USER
Pausing, resuming and cancelling jobs

To stop a waiting job from being scheduled and later to allow it to be scheduled:
scontrol hold $jobid
scontrol release $jobid
Question: what do you see as "Reason" in squeue output on a large (say, 10 nodes interactive) job that you submitted and then ran scontrol hold $jobid on?

To pause a running job and then resume it:
scontrol suspend $jobid
scontrol resume $jobid
Question: what happens when you try to suspend an interactive job?

To remove a job from the queue (stopping it if already started):
scancel $jobid
Question: what happens when you cancel a running job, does it get killed immediately?

To remove a job by name:
scancel --name=$jobname
scancel -n $jobname
Question: what happens if you start two jobs with the same name and cancel by name?

To remove all user jobs:
scancel --user=$USER
scancel -u $USER
To remove all waiting jobs (pending state) for a given user:
scancel --user=$USER --state=pending
scancel -u $USER -t pending
To remove all waiting jobs in a given partition (e.g. batch):
scancel -u $USER --partition=batch
scancel -u $USER -p batch
To stop and restart a given job:
scontrol requeue $jobid
Question: what message do you get when you try to requeue an interactive job?

Part two: the harder stuff

We have made available a set of template batch launcher scripts for SLURM.

You should now:

adapt the most appropriate one (sequential, parallel, etc.) for your most commonly used HPC application
launch your own (short execution time) test case, on a single core for sequential code or two distributed cores for parallel code
take note of your fair-share and usage values (e.g. with sshare -A $(sacctmgr -n show user $USER format=defaultaccount%30s))
alternative: compile and use HPCG from the ULHPC HPCG tutorial
monitor job progression:
with sprio / sprio -l to see its priority in the queue
using sstat once it starts, to get running metrics
by connecting to the job (sjoin $jobid or using the --jobid parameter of srun) and then using htop
finally, once the job finished or you have stopped it:
check the information visible with sacct
how did your fair-share and usage values change?
what efficiency values are shown by seff for the job?

Know Your Bugs: Weapons for Efficient Debugging

  Copyright (c) 2013-2018 X. Besseron, UL HPC Team <hpc-sysadmins@uni.lu>


The objective of this tutorial is to review the main tools that can be used to debug your [parallel] programs.

Hands/On 0 - Pre-requisites

Reserve 1 core (for 3h) over the UL HPC platform

  $> ssh gaia-cluster    # OR chaos-cluster
  $> oarsub -I -l core=1,walltime="03:00:00"
Hands/On 1 - GDB Tutorial

Tutorial from A GDB Tutorial with Examples

You'll need to load the latest GDB module:

  $> module spider gdb
  $> module load  debugger/GDB
Hands/On 2 - Valgrind Tutorial

Tutorial from Using Valgrind to Find Memory Leaks and Invalid Memory Use

You'll also need to load the appropriate module

  $> module spider valgrind
  $> module load debugger/Valgrind
Hands/On 3 - Bug Hunting

A list of programs demonstrating the different kind of bus are available in the exercises directory. Try the different debugging tools on every example to see how they behave and find the bugs.

Run the following command to download all the exercises:

$> git clone https://github.com/ULHPC/tutorials.git ulhpc-tutorials
$> cd ulhpc-tutorials/advanced/Debug/exercises/
Notes:

You can compile each program manually using gcc or icc (the latest coming from the toolchains/ictce module). You are encouraged to try both to see how differently they behave. Example: gcc program.c -o program. Add any additional parameter you might need.
Some program required additional options to be compiled. They are indicated in comment at the beginning of each source file.

UL HPC Tutorial: Advanced debugging on the UL HPC platform

 Copyright (c) 2013-2019 UL HPC Team <hpc-sysadmins@uni.lu>


The objective of this tutorial is to practice running and debugging applications on the UL HPC clusters.

It's important that you read the slides first.

They review:

basic tools you can use to view and understand how your application is running
HPC oriented debugging, profiling and analysis tools:
Allinea Forge (DDT+MAP) and Allinea Performance Reports
Intel Advisor, Inspector, ITAC and VTune Amplifier
Scalasca and Score-P
Your tasks

Compile and make a first timed run of unmodified HPCG v3.0 (MPI only) as per the ULHPC HPCG tutorial
full single node, use >= 80 80 80 for input params (edit hpcg.dat)
use /usr/bin/time -v to get details (especially as regards timing and memory usage)
Run HPCG (timed) through Allinea Performance Reports
use perf-report (bonus points if using iris to get energy metrics)
Instrument and measure HPCG execution with Scalasca
Re-compile HPCG with debugging symbols and run under Allinea DDT (GUI mode) and MAP (batch mode)
Helper notes:

Choose your toolchain (compilers, libraries, MPI implementations):

module load toolchain/intel or
module load toolchain/foss
Run HPCG in parallel:

Gaia, Chaos clusters: mpirun -hostfile $OAR_NODEFILE ./xhpcg
Iris cluster: srun ./xhpcg
Run HPCG with Allinea Performance Reports:

Gaia, Chaos clusters: perf-report mpirun -hostfile $OAR_NODEFILE ./xhpcg
Iris cluster: perf-report srun ./xhpcg
Compile and run HPCG with Scalasca/Score-P:

use toolchain/foss, perf/Scalasca and perf/Score-P
edit HPCG's setup file and prepend the compiler (e.g. mpicxx) on the the CXX = line with scorep (e.g. CXX = scorep mpicxx)
compile HPCG
run HPCG with Scalasca in summary mode:
Gaia, Chaos clusters: scan -s mpirun -hostfile $OAR_NODEFILE ./xhpcg
Iris cluster: scan -s srun ./xhpcg
generate text report square -s scorep_xhpcg_*_sum and see it with cat scorep_xhpcg_*_sum/scorep.score
Note: 2019 software environment (new compilers, debuggers, profilers) can be tested with module load swenv/default-env/devel. This is only necessary for the HPC School 2019.06, as this environment will become the new default production during summer 2019.

Many Tasks — Many Node Allocations using Slurm and GNU Parallel

 Copyright (c) 2022-2023 P. Pochelu, E. Kieffer and UL HPC Team <hpc-team@uni.lu>
GNU Parallel is the ideal tool for executing a custom program with varied parameters and utilizing the HPC.

The goal of GNU Parallel is to run the same program on a pool of different parameters. The pool of parameters can be larger than the number of core in the infrastructure.

For example, training and testing a neural network with different number of layers, running a simulator on different initial condition files, mathematical optimization ....

Useful Links

GNU Parallel: Learn about the powerful GNU Parallel tool.
Slurm Scheduler: Explore the Slurm scheduler documentation.
Job Arrays: Understand the concepts and usage of Slurm job arrays.
HPC Management of Sequential and Embarrassingly Parallel Jobs: Gain insights into managing sequential and embarrassingly parallel jobs on HPC.
Lmod: Discover Lmod, a tool for managing the software environment on HPC clusters.
NERSC Documentation: Read the NERSC documentation on the scalability of parallel tasks with SSH login.
Step 1: Multi-core code in one node

There command you can do from any machine in iris or aion.

Your experience code takes one input. The syntax is ${1} in bash. In programming language, you may access the input parameter with sys.argv[1] in python, argv[1] in C/C++, ... .

experiment.sh, this script emulates a task to run:

#!/bin/sh
echo "Running your experiment with parameter ${1} ..."
sleep 3 # emulate a long run time
echo "Experiment ${1} finished at $(date)"
The script below run the code on 5 inputs as input: "a", "b", "c", "d" and "e".

#/bin/sh
export experiment=${PWD}/experiment.sh
parallel -j 2 $experiment {} ::: a b c d e f
Breaking down the command: - parallel: The command itself, installed in your ULHPC. - experiment: Simulates your Bash script. Keeps aware you can run any software and is not reserved to bash scripts. - $experiment {}: The program to run, where {} is the placeholder for one parameter. - -j 2: Executes two parallel tasks using GNU Parallel. Each task corresponds to one parameter. - This key word ::: indicates parameters are inlined. Note the distinction between ::: and ::::. ::: a b c d e f provides different inputs to the experiment program. For sending files via meta characters such as ./input/*, use :::: instead.

The output:

Running your experiment with parameter a ...
Experiment a finished at Wed 15 Nov 17:07:08 CET 2023
Running your experiment with parameter b ...
Experiment b finished at Wed 15 Nov 17:07:08 CET 2023
Running your experiment with parameter c ...
Experiment c finished at Wed 15 Nov 17:07:11 CET 2023
Running your experiment with parameter d ...
Experiment d finished at Wed 15 Nov 17:07:11 CET 2023
Running your experiment with parameter e ...
Experiment e finished at Wed 15 Nov 17:07:14 CET 2023
Running your experiment with parameter f ...
Experiment f finished at Wed 15 Nov 17:07:14 CET 2023
Concluding remark: two jobs are effectively done simultaneously: "a" and "b" at 14:16:59, "c" and "d" finished at 14:17:02, due to the -j 2 option. It effectively uses the multi-core capability of the CPU.

Step 2 : Multi-node Multi-core code

First we need the code we need to distribute. There the example of a bash script, but again, it can be done with any programming language. experiment.sh:

#!/bin/sh
echo "Running your experiment with parameter ${1} ..."
sleep 3 # emulate a long run time
echo "Experiment ${1} finished at $(date)"
slurm_parallel_launcher.sh:

#!/bin/sh -l
#SBATCH -c 30 # How many cores to use in 1 single node ?
#SBATCH -N 3 # How many nodes ?
#SBATCH -t 1
#SBATCH --export=ALL


# get host name
hosts_file="hosts.txt"
scontrol show hostname $SLURM_JOB_NODELIST > $hosts_file


# Collect public key and accept them
while read -r node; do
    ssh-keyscan "$node" >> ~/.ssh/known_hosts
done < "$hosts_file"

experiment=${PWD}/experiment.sh

# Run. The -j option controls how many experiments run in each node (they will share the 30 cores).
# The number of experiments is given by N*j.
parallel  --sshloginfile  $hosts_file  -j 2 $experiment {} ::: a b c d e f g h i j k l m n o p q r s t u v w x y z
Finally, you can launch this script by doing:

[ppochelu@access1 ~]$ sbatch ./slurm_parallel_launcher.sh 
Finally, you can check the SLURM output the result of this:

Running your experiment with parameter e ...
Experiment e finished at Wed 15 Nov 12:21:20 CET 2023
Running your experiment with parameter b ...
Experiment b finished at Wed 15 Nov 12:21:20 CET 2023
Running your experiment with parameter c ...
Experiment c finished at Wed 15 Nov 12:21:20 CET 2023
Running your experiment with parameter f ...
Experiment f finished at Wed 15 Nov 12:21:20 CET 2023
Running your experiment with parameter a ...
Experiment a finished at Wed 15 Nov 12:21:20 CET 2023
Running your experiment with parameter d ...
Experiment d finished at Wed 15 Nov 12:21:20 CET 2023
Running your experiment with parameter g ...
Experiment g finished at Wed 15 Nov 12:21:24 CET 2023
Running your experiment with parameter h ...
Experiment h finished at Wed 15 Nov 12:21:24 CET 2023
Running your experiment with parameter i ...
Experiment i finished at Wed 15 Nov 12:21:24 CET 2023
Running your experiment with parameter j ...
[...]
Check the SLURM output to see the results. The order of finished tasks is not guaranteed, and six tasks are done in parallel (two tasks per node, utilizing three nodes).

Case study: Word Count on multiple files

Assuming we want to count the number of words in all files in the directory ./input. The words will be aggregated in the output.txt file.

You may download the input files here:

Download 1.txt, Download 2.txt, Download 3.txt, Download 4.txt, Download 5.txt, Download 6.txt. Download 7.txt, Download 8.txt, Download 9.txt, Download 10.txt, Download 11.txt, Download 12.txt.

Launch the program using the "::::" syntax to scan all files in the input directory:

#!/bin/sh
wc -w ${1} >> ${2}
slurm_parallel_launcher.sh:

#!/bin/sh -l
#SBATCH -c 30 # How many cores to use in 1 single node ?
#SBATCH -N 3 # How many nodes ?
#SBATCH -t 1
#SBATCH --export=ALL

# get host name
hosts_file="hosts.txt"
scontrol show hostname $SLURM_JOB_NODELIST > $hosts_file

# Collect public key and accept them
while read -r node; do
    ssh-keyscan "$node" >> ~/.ssh/known_hosts
done < "$hosts_file"

experiment=${PWD}/experiment.sh
outfile=${PWD}/output.txt

parallel  --sshloginfile  $hosts_file  -j 2 $experiment {} $outfile ::: ${PWD}/input/*.txt
Finally, you can launch this script by doing:

[ppochelu@access1 ~]$ sbatch ./slurm_parallel_launcher.sh 
After the process is scheduled and finished, the result looks like:

[ppochelu@access1 ~]$ cat output.txt 
78 ~/input/11.txt
75 ~/input/2.txt
158 ~/input/3.txt
Case study: Python (Hyper)-parameter Search

Code is organized here in 2 files python files.

parameters.py for generating (hyper)-parameters:

stepx=0.2
minx=-2.0
maxx=2.0
stepy=0.2
miny=-2.0
maxy=2.0

parameters=[]
x=minx
y=miny
while x<maxx:
    while y<maxy:
        param=(x,y)
        parameters.append( param )
        y+=stepy
    x+=stepx
params_str=[]
for x,y in parameters:
    param_str=str(round(x,3))+","+str(round(y,3))
    params_str.append(param_str)
output="\n".join(parameters)
print(output) # the output can be re-redirected to another program
The Grid Search samples 40 variations in the x-dimension and 40 in the y-dimension, it made 1600 experiments to perform.

experiment.py

import sys
import time
param_str=sys.argv[1]
x_str, y_str = param_str.split(",")
x=float(x_str)
y=float(y_str)
score=(x-1)**2 + b*(y-x**2)**2
time.sleep(3) # simulates longer execution time
return round(score, 3), x, y
slurm_parallel_launcher.sh:

#!/bin/sh -l
#SBATCH -c 100 # How many cores to use in 1 single node ?
#SBATCH -N 3 # How many nodes ?
#SBATCH -t 5
#SBATCH --export=ALL

hosts_file="hosts.txt"
scontrol show hostname $SLURM_JOB_NODELIST > $hosts_file

while read -r node; do  # collect and accept ssh keys
    ssh-keyscan "$node" >> ~/.ssh/known_hosts
done < "$hosts_file"

PARAMS=$(python3 parameters.py) # Store in PARAMS the experiments pool
experiment_py=${PWD}/experiment.py
output=${PWD}/output.txt
parallel  --sshloginfile  $hosts_file  -j 100 python3 $experiment_py >> $output  {}  ::: $PARAMS
Each node is responsible for 100 tasks at the same time. There are 3 nodes. The HPC will perform batch of 300 at the same.

Finally, you can launch this script by doing:

[ppochelu@access1 ~]$ sbatch ./slurm_parallel_launcher.sh 
The running time will take approximatively (1600 tasks // 300 tasks in parallel) * 3 seconds per task = 18 seconds

Distributing embarrassingly parallel tasks GNU Parallel

 Copyright (c) 2021 UL HPC Team <hpc-team@uni.lu>


GNU Parallel is a tool for executing tasks in parallel, typically on a single machine. When coupled with the Slurm command srun, parallel becomes a powerful way of distributing a set of tasks amongst a number of workers. This is particularly useful when the number of tasks is significantly larger than the number of available workers (i.e. $SLURM_NTASKS), and each tasks is independent of the others.

Prerequisites

The parallel command is available at the system level across the ULHPC clusters, yet under a relatively old version:

(access)$ which parallel
/usr/bin/parallel
(access)$ parallel --version
GNU parallel 20190922
[...]
If you want to build a more recent version. The process is quite straight-forward and we will illustrate this process Easybuild (see the Easybuild tutorial and Spack tutorial).

With Easybuild

We are going to extend the current software set to avoid recompiling a toolchain
The parallel version will be built using GCC-10.2.0
(node)$ resif-load-home-swset-prod
# Check where it will be installed
(node)$ echo $MODULEPATH
# Loading Easybuild
(node)$ module load tools/EasyBuild
(node)$ eb -S parallel
eb -S parallel
== found valid index for /opt/apps/resif/aion/2020b/epyc/software/EasyBuild/4.5.4/easybuild/easyconfigs, so using it...
CFGS1=/opt/apps/resif/aion/2020b/epyc/software/EasyBuild/4.5.4/easybuild/easyconfigs
 * $CFGS1/a/Amber/Amber-16-AT-17-fix_missing_do_parallel_in_checkrismunsupported.patch
 * $CFGS1/a/Amber/Amber-18-AT-18_fix_missing_do_parallel_in_checkrismunsupported.patch
 * $CFGS1/h/HPL/HPL_parallel-make.patch
 * $CFGS1/i/ipyparallel/ipyparallel-6.2.2-foss-2018a-Python-3.6.4.eb
 * $CFGS1/j/Judy/Judy-1.0.5_parallel-make.patch
 * $CFGS1/n/NWChem/NWChem-6.3.revision2-parallelbuild.patch
 * $CFGS1/n/NWChem/NWChem-6.5.revision26243-parallelbuild.patch
 * $CFGS1/n/NWChem/NWChem-6.6.revision27746-parallelbuild.patch
 * $CFGS1/n/netCDF/netCDF-4.3.2-parallel-HDF.patch
 * $CFGS1/o/OpenSSL/OpenSSL-1.0.1i-fix_parallel_build-1.patch
 * $CFGS1/o/OpenSSL/OpenSSL-1.0.1m_fix-parallel.patch
 * $CFGS1/o/OpenSees/OpenSees-3.2.0-add_Makefile_def_parallel.patch
 * $CFGS1/o/OpenSees/OpenSees-3.2.0-intel-2020a-Python-3.8.2-parallel.eb
 * $CFGS1/p/ParallelIO/ParallelIO-2.2.2a-intel-2017a.eb
 * $CFGS1/p/PyTorch/PyTorch-1.7.0_fix_test_DistributedDataParallel.patch
 * $CFGS1/p/parallel-fastq-dump/parallel-fastq-dump-0.6.5-GCCcore-8.2.0-Python-3.7.2.eb
 * $CFGS1/p/parallel-fastq-dump/parallel-fastq-dump-0.6.6-GCCcore-9.3.0-Python-3.8.2.eb
 * $CFGS1/p/parallel/parallel-20141122-GCC-4.9.2.eb
 * $CFGS1/p/parallel/parallel-20150322-GCC-4.9.2.eb
 * $CFGS1/p/parallel/parallel-20150822-GCC-4.9.2.eb
 * $CFGS1/p/parallel/parallel-20160622-foss-2016a.eb
 * $CFGS1/p/parallel/parallel-20170822-intel-2017a.eb
 * $CFGS1/p/parallel/parallel-20171022-intel-2017b.eb
 * $CFGS1/p/parallel/parallel-20171122-foss-2017b.eb
 * $CFGS1/p/parallel/parallel-20171122-intel-2017b.eb
 * $CFGS1/p/parallel/parallel-20180422-intel-2018a.eb
 * $CFGS1/p/parallel/parallel-20180822-foss-2018b.eb
 * $CFGS1/p/parallel/parallel-20181222-intel-2018b.eb
 * $CFGS1/p/parallel/parallel-20190222-GCCcore-7.3.0.eb
 * $CFGS1/p/parallel/parallel-20190622-GCCcore-8.2.0.eb
 * $CFGS1/p/parallel/parallel-20190922-GCCcore-8.3.0.eb
 * $CFGS1/p/parallel/parallel-20200422-GCCcore-9.3.0.eb
 * $CFGS1/p/parallel/parallel-20200522-GCCcore-9.3.0.eb
 * $CFGS1/p/parallel/parallel-20210322-GCCcore-10.2.0.eb
 * $CFGS1/p/parallel/parallel-20210622-GCCcore-10.3.0.eb
 * $CFGS1/p/parallel/parallel-20210722-GCCcore-11.2.0.eb
 * $CFGS1/r/R/DMCfun-1.3.0_fix-parallel-detect.patch
 * $CFGS1/w/WRF/WRF_parallel_build_fix.patch
 * $CFGS1/x/Xmipp/Xmipp-3.19.04-Apollo_add_missing_pthread_to_XmippParallel.patch

Note: 7 matching archived easyconfig(s) found, use --consider-archived-easyconfigs to see them
Let's install $CFGS1/p/parallel/parallel-20210322-GCCcore-10.2.0.eb
(node)$ eb parallel-20210322-GCCcore-10.2.0.eb -r 
(node)$ module av parallel

-------------------------------------------------------------------------------------------------------------------------- /home/users/ekieffer/.local/easybuild/aion/2020b/epyc/modules/all ---------------------------------------------------------------------------------------------------------------------------
   tools/parallel/20210322-GCCcore-10.2.0

If the avail list is too long consider trying:

"module --default avail" or "ml -d av" to just list the default modules.
"module overview" or "ml ov" to display the number of modules for each name.

Use "module spider" to find all possible modules and extensions.
Use "module keyword key1 key2 ..." to search for all possible modules matching any of the "keys".
With Spack

If we don't have it installed, please follow first the Spack tutorial
(node)$ spack list parallel
aws-parallelcluster  intel-parallel-studio  parallel  parallel-netcdf  parallelio  parallelmergetree  perl-parallel-forkmanager  py-ipyparallel  py-pytest-parallel  r-biocparallel  r-doparallel  r-optimparallel  r-parallelly  r-parallelmap  r-rcppparallel
==> 15 packages
(node)$ spack versions parallel
==> Safe versions (already checksummed):
  20220522  20220422  20220322  20220222  20220122  20210922  20200822  20190222  20170322  20170122  20160422  20160322
==> Remote versions (not yet checksummed):
  20230422  20221222  20220822  20211122  20210622  20210222  20201022  20200522  20200122  20190922  20190522  20181222  20180822  20180422  20171222  20170822  20170422  20161022  20160622  20151222  20150822  20150422  20141122  20140722  20140322  20131122  20130722  20130222  20121022  20120522  20120122
  20230322  20221122  20220722  20211022  20210522  20210122  20200922  20200422  20191222  20190822  20190422  20181122  20180722  20180322  20171122  20170722  20170222  20160922  20160522  20151122  20150722  20150322  20141022  20140622  20140222  20131022  20130622  20130122  20120822  20120422
  20230222  20221022  20220622  20210822  20210422  20201222  20200722  20200322  20191122  20190722  20190322  20181022  20180622  20180222  20171022  20170622  20161222  20160822  20160222  20151022  20150622  20150222  20140922  20140522  20140122  20130922  20130522  20121222  20120722  20120322
  20230122  20220922  20211222  20210722  20210322  20201122  20200622  20200222  20191022  20190622  20190122  20180922  20180522  20180122  20170922  20170522  20161122  20160722  20160122  20150922  20150522  20150122  20140822  20140422  20131222  20130822  20130422  20121122  20120622  20120222
The most recent and safe version is 20220522
(node)$ spack install parallel@20220522
[+] /mnt/irisgpfs/users/ekieffer/.spack/opt/spack/linux-rhel8-zen/gcc-8.5.0/berkeley-db-18.1.40-uw5w4yhzzi2fatjzb72ipgdf3w657tle
[+] /mnt/irisgpfs/users/ekieffer/.spack/opt/spack/linux-rhel8-zen/gcc-8.5.0/bzip2-1.0.8-ymcs7cevgovcd3bc5iphzo5ztzv62jue
[+] /mnt/irisgpfs/users/ekieffer/.spack/opt/spack/linux-rhel8-zen/gcc-8.5.0/ncurses-6.4-3qm6oylywjcvizw7xyqbkxg33vqtgppp
[+] /mnt/irisgpfs/users/ekieffer/.spack/opt/spack/linux-rhel8-zen/gcc-8.5.0/zlib-1.2.13-426hs7tsxcfpebed5uqlogma32dbuvj5
[+] /mnt/irisgpfs/users/ekieffer/.spack/opt/spack/linux-rhel8-zen/gcc-8.5.0/readline-8.2-zgvmdyizb6g4ee2ozansvqkxvgbq6a6r
[+] /mnt/irisgpfs/users/ekieffer/.spack/opt/spack/linux-rhel8-zen/gcc-8.5.0/gdbm-1.23-seklrqiazmh54ts3tdx6fpnlobviw3ia
[+] /mnt/irisgpfs/users/ekieffer/.spack/opt/spack/linux-rhel8-zen/gcc-8.5.0/perl-5.36.0-yvgdvqozt46iimytfldxpjhxxao2gtdy
==> Installing parallel-20220522-ediq6fzty3il5qc3frzlwn45szfvbhek
==> No binary for parallel-20220522-ediq6fzty3il5qc3frzlwn45szfvbhek found: installing from source
==> Fetching https://mirror.spack.io/_source-cache/archive/bb/bb6395f8d964e68f3bdb26a764d3c48b69bc5b759a92ac3ab2bd1895c7fa8c1f.tar.bz2
==> No patches needed for parallel
==> parallel: Executing phase: 'autoreconf'
==> parallel: Executing phase: 'configure'
==> parallel: Executing phase: 'build'
==> parallel: Executing phase: 'install'
==> parallel: Successfully installed parallel-20220522-ediq6fzty3il5qc3frzlwn45szfvbhek
  Stage: 2.99s.  Autoreconf: 0.00s.  Configure: 1.20s.  Build: 0.02s.  Install: 0.60s.  Total: 4.90s
[+] /mnt/irisgpfs/users/ekieffer/.spack/opt/spack/linux-rhel8-zen/gcc-8.5.0/parallel-20220522-ediq6fzty3il5qc3frzlwn45szfvbhek
(node)$ spack find -vpl parallel
-- linux-rhel8-zen / gcc@=8.5.0 ---------------------------------
ediq6fz parallel@20220522 build_system=autotools  /mnt/irisgpfs/users/ekieffer/.spack/opt/spack/linux-rhel8-zen/gcc-8.5.0/parallel-20220522-ediq6fzty3il5qc3frzlwn45szfvbhek
==> 1 installed package
Discovering the parallel command

The GNU Parallel syntax can be a little distributing, but basically it supports two modes:

Reading command arguments on the command line:

parallel    [-j N] [OPTIONS]    COMMAND {} ::: TASKLIST
Reading command arguments from an input file:

parallel    –a  TASKLIST.LST    [-j N] [OPTIONS]    COMMAND {}
parallel    [-j N] [OPTIONS]    COMMAND {} :::: TASKLIST.LST
If your COMMAND embed a pipe stage, you have to escape the pipe symbol as follows \|. Let's make some tests. The -j <N> option permits to define the jobs per machine - in particular you may want to use -j 1 to enable a sequential resolution of the parallel command.

In all cases, the parallel command is available at the system across the ULHPC clusters. Run it once.

(access)$> parallel --version
GNU parallel 20160222
Copyright (C) 2007,2008,2009,2010,2011,2012,2013,2014,2015,2016
Ole Tange and Free Software Foundation, Inc.
License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>
This is free software: you are free to change and redistribute it.
GNU parallel comes with no warranty.

Web site: http://www.gnu.org/software/parallel

When using programs that use GNU Parallel to process data for publication
please cite as described in 'parallel --bibtex'.
If you want to avoid the disclaimer requesting to cite the paper describing GNU parallel, you have to run as indicated parallel --bibtex, type: 'will cite' and press enter.

Let's play with a TASKLIST from the command line:

(access)$> parallel echo {} ::: A B C
A
B
C
(access)$> parallel echo {} ::: {1..3}
1
2
3
# As above
(access)$> parallel echo {} ::: $(seq 1 3)
1
2
3
# Use index to refer to a given TASKLIST
(access)$> parallel echo {1} {2} ::: A B C ::: {1..3}
A 1
A 2
A 3
B 1
B 2
B 3
C 1
C 2
C 3
# Combine (link) TASKLIST with same size - Use '--link' on more recent parallel version
(access)$> parallel --xapply "echo {1} {2}" ::: A B C ::: {1..3}
A 1
B 2
C 3
# This can be useful to output command text with arguments
(access)$> parallel --xapply echo myapp_need_argument {1} {2} ::: A B C ::: {1..3}
myapp_need_argument A 1
myapp_need_argument B 2
myapp_need_argument C 3
# /!\ IMPORTANT: you can then **execute** these commands as above  by removing 'echo'
#     DON'T do that unless you know what you're doing
# You can filter out some elements:
(access)$> parallel --xapply echo myapp_need_argument {1} {2} \| grep -v 2 ::: A B C ::: {1..3}
myapp_need_argument A 1
myapp_need_argument C 3
Let's play now with a TASKLIST from an input file.

Let's assume you wish to process some images from the OpenImages V4 data set. A copy of this data set is available on the ULHPC facility, under /work/projects/bigdata_sets/OpenImages_V4/. Let's create a CSV file which contains a random selection of 10 training files within this dataset (prefixed by a line number). You may want to do it as follows (copy the full command):

#                                                       training set     select first 10K  random sort  take only top 10   prefix by line number      print to stdout AND in file
#                                                         ^^^^^^           ^^^^^^^^^^^^^   ^^^^^^^^     ^^^^^^^^^^^^^      ^^^^^^^^^^^^^^^^^^^^^^^^   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(access)$> find /work/projects/bigdata_sets/OpenImages_V4/train/ -print | head -n 10000 | sort -R   |  head -n 10       | awk '{ print ++i","$0 }' | tee openimages_v4_filelist.csv
1,/work/projects/bigdata_sets/OpenImages_V4/train/6196380ea79283e0.jpg
2,/work/projects/bigdata_sets/OpenImages_V4/train/7f23f40740731c03.jpg
3,/work/projects/bigdata_sets/OpenImages_V4/train/dbfc1b37f45b3957.jpg
4,/work/projects/bigdata_sets/OpenImages_V4/train/f66087cdf8e172cd.jpg
5,/work/projects/bigdata_sets/OpenImages_V4/train/5efed414dd8b23d0.jpg
6,/work/projects/bigdata_sets/OpenImages_V4/train/1be054cb3021f6aa.jpg
7,/work/projects/bigdata_sets/OpenImages_V4/train/61446dee2ee9eb27.jpg
8,/work/projects/bigdata_sets/OpenImages_V4/train/dba2da75d899c3e7.jpg
9,/work/projects/bigdata_sets/OpenImages_V4/train/7ea06f092abc005e.jpg
10,/work/projects/bigdata_sets/OpenImages_V4/train/2db694eba4d4bb04.jpg
Let's manipulate the file content with parallel (prefer the -a <filename> syntax):

# simple echo of the file
(access)$> parallel -a openimages_v4_filelist.csv echo {}
1,/work/projects/bigdata_sets/OpenImages_V4/train/6196380ea79283e0.jpg
2,/work/projects/bigdata_sets/OpenImages_V4/train/7f23f40740731c03.jpg
3,/work/projects/bigdata_sets/OpenImages_V4/train/dbfc1b37f45b3957.jpg
[...]
# print specific column of the CSV file
(access)$> parallel --colsep '\,' -a openimages_v4_filelist.csv echo {1}
1
2
3
4
5
6
7
8
9
10
(access)$> parallel --colsep '\,' -a openimages_v4_filelist.csv echo {2}
/work/projects/bigdata_sets/OpenImages_V4/train/6196380ea79283e0.jpg
/work/projects/bigdata_sets/OpenImages_V4/train/7f23f40740731c03.jpg
/work/projects/bigdata_sets/OpenImages_V4/train/dbfc1b37f45b3957.jpg
/work/projects/bigdata_sets/OpenImages_V4/train/f66087cdf8e172cd.jpg
/work/projects/bigdata_sets/OpenImages_V4/train/5efed414dd8b23d0.jpg
/work/projects/bigdata_sets/OpenImages_V4/train/1be054cb3021f6aa.jpg
/work/projects/bigdata_sets/OpenImages_V4/train/61446dee2ee9eb27.jpg
/work/projects/bigdata_sets/OpenImages_V4/train/dba2da75d899c3e7.jpg
/work/projects/bigdata_sets/OpenImages_V4/train/7ea06f092abc005e.jpg
/work/projects/bigdata_sets/OpenImages_V4/train/2db694eba4d4bb04.jpg

# reformat and change order
(access)$> parallel --colsep '\,' -a openimages_v4_filelist.csv echo {2} {1}
/work/projects/bigdata_sets/OpenImages_V4/train/6196380ea79283e0.jpg 1
/work/projects/bigdata_sets/OpenImages_V4/train/7f23f40740731c03.jpg 2
/work/projects/bigdata_sets/OpenImages_V4/train/dbfc1b37f45b3957.jpg 3
/work/projects/bigdata_sets/OpenImages_V4/train/f66087cdf8e172cd.jpg 4
/work/projects/bigdata_sets/OpenImages_V4/train/5efed414dd8b23d0.jpg 5
/work/projects/bigdata_sets/OpenImages_V4/train/1be054cb3021f6aa.jpg 6
/work/projects/bigdata_sets/OpenImages_V4/train/61446dee2ee9eb27.jpg 7
/work/projects/bigdata_sets/OpenImages_V4/train/dba2da75d899c3e7.jpg 8
/work/projects/bigdata_sets/OpenImages_V4/train/7ea06f092abc005e.jpg 9
/work/projects/bigdata_sets/OpenImages_V4/train/2db694eba4d4bb04.jpg 10
The ULHPC team has designed a generic launcher for single node GNU parallel: see ../basics/scripts/launcher.parallel.sh.

Many Tasks — Many Node Allocations using Slurm and GNU Parallel

 Copyright (c) 2022-2023 E. Kieffer and UL HPC Team <hpc-team@uni.lu>
Going beyond a single node

GNU Parallel is definitely a powerful system when combined with the Slurm scheduler and avoid all the disavantages of Job Arrays. An extensive use of job arrays is NOT recommanded as described in HPC Management of Sequential and Embarrassingly Parallel Jobs while using a script to submit jobs inside a loop is even worse as all your submitted jobs compete to be scheduled.

Although using Parallel on a single node allows to schedule your tasks inside your own allocation, it may not be sufficient. You may want to use a multiple node allocation to run more than 28 independent tasks on the Iris cluster and 128 on the Aion cluster. In mutiple research area, repeatbility is necessary and this is absolutely NOT embarrassing. In fact, task independence is a perfect case with no communication overhead.

In the HPC Management of Sequential and Embarrassingly Parallel Jobs tutorial, you have seen how to use efficiently the GNU Parallel for executing multiple independent tasks in parallel on a single machine/node. In this tutorial, we will consider a multi-node allocation with GNU Parallel.

Prerequisites

It's very important to make sure that parallel is installed on the computing nodes and not only on the access. On the ULHPC clusters, this should the case. If you would like to test it on a different platform, please refer to GNU Parallel to learn how to install it locally. Some other well-know HPC centers provides GNU Parallel as a Lmod module.

Methodology

Spawning parallel workers

By default, GNU Parallel can distribute tasks to multiples nodes using the --sshlogin feature. Nonetheless, the approach is not really effective as suggested in the NERSC documentation. In fact, we will use a simple but efficient approach to distribute tasks to nodes in a round-robin manner.

Using srun, we will spawn a single parallel worker on each node. Each parallel worker will manage its own queue of tasks which implies that if a node fail only its queue will be impacted. To do so, have a look on the slurm_parallel_launcher.sh script:

#!/bin/bash -l
#SBATCH --time=02:00:00
#SBATCH --partition=batch
#SBATCH --nodes=2
#SBATCH --exclusive
#SBATCH --mem=0
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=128

# Increase the user process limit.
ulimit -u 10000

echo "Spawning ${SLURM_NTASKS_PER_NODE} parallel worker on ${SLURM_NNODES} nodes"
echo "Nodes: ${SLURM_NODELIST}"
echo "Each parallel worker can execute ${SLURM_CPUS_PER_TASK} independant tasks"
export SRUN_CPUS_PER_TASK=${SLURM_CPUS_PER_TASK}
export TIMESTAMP=$(date +"%Y%m%dT%H%M%S")
srun --no-kill --wait=0 $*
The option --exclusive has been added to be sure that we do not share the node with another job.
The option --mem=0 ensures that all memory will be available
Note: Even if the previous options were missing, the --ntasks-per-node and --cpus-per-task options would ensure that all memory and cores are allocated to our job. In some situation, you may not need all cores but all the memory. Therefore I kept --mem=0 and --exclusive.
You probably notice the srun options --no-kill and --wait=0 which respectively ensure that the job does NOT automatically terminate if one of the nodes it has been allocated fails and does wait on all tasks, i.e., does not terminate some time after the first task has been completed.

The parallel worker

We now need to define the script starting the parallel workers parallel_worker.sh. Please keep in mind that this script will be executing exactly ${SLURM_NTASKS} times. Since we want a single parallel worker on each node, we have to specify in the previous launcher --ntasks-per-node=1.

#!/usr/bin/bash -l

print_error_and_exit() { echo "***ERROR*** $*"; exit 1; }


hash parallel 2>/dev/null && test $? -eq 0 || print_error_and_exit "Parallel is not installed on the system"

if [[ -z "${SLURM_NODEID}" ]]; then
    print_error_and_exit "The variable \${SLURM_NODEID} is required (but missing)... exiting"
fi
if [[ -z "${SLURM_NNODES}" ]]; then
    print_error_and_exit "The variable \${SLURM_NNODES} is required (but missing)... exiting"
    exit
fi


NBCORES_TASK=1
SCRIPT_DIR=$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)
SCRIPT_NAME="$(basename "${BASH_SOURCE[0]}")"
HOSTNAME=$(hostname)
LOGS="logs.${TIMESTAMP}"
RESUME=""
TASKFILE=""
NTASKS=""


#=======================
# Get Optional arguments
#=======================
while [ $# -ge 1 ]; do
    case $1 in
        -r | --resume)           shift; LOGS=$1; RESUME=" --resume ";;
        -n | --ntasks)           shift; NTASKS="$1"                            ;;
        -* | --*)                echo "[Warning] Invalid option $1"          ;;
        *)                       break                                       ;;
    esac
    shift
done

#=======================
# Get Mandatory  Options
#=======================

if [[ "$#" < 1 ]]; then
    print_error_and_exit "No tasks defined"
else
    TASKFILE="$1"
fi

echo "Starting worker initialisation on $(hostname)"

#=======================
# Set logs for resuming
#=======================

LOGS_DIR="${SCRIPT_DIR}/${LOGS}/Worker${SLURM_NODEID}"
SCRIPT_MAINLOG="${LOGS_DIR}/${SCRIPT_NAME//sh/log}"
PARALLEL="parallel --delay 0.2 -j $((SLURM_CPUS_PER_TASK / NBCORES_TASK)) --joblog ${SCRIPT_MAINLOG} ${RESUME}"




echo "Create logs directory if not existing"
mkdir -p ${LOGS_DIR}

if [[ -z ${NTASKS} ]];then
    cat ${TASKFILE} |                                      \
    awk -v NNODE="$SLURM_NNODES" -v NODEID="$SLURM_NODEID" \
    'NR % NNODE == NODEID' |                               \
    ${PARALLEL} "{1} > ${LOGS_DIR}/$(basename ${TASKFILE}).log.{%}"
else
    echo  "$(seq 1 ${NTASKS})" |                             \
    awk -v NNODE="$SLURM_NNODES" -v NODEID="$SLURM_NODEID" \
    'NR % NNODE == NODEID' |                               \
    ${PARALLEL} "${TASKFILE} > ${LOGS_DIR}/$(basename ${TASKFILE}).log.{1}"
fi
It is essential to ensure that we have a valid allocation of computing nodes, i.e, ${SLURM_NODEID} and  ${SLURM_NNODES} are defined.
Parallel is really a fantastic tool that offers the possibility to record all performed tasks into a joblog
This is extremely useful in the case you underestimate the walltime associated to your allocation because you can resume from the last unfinished job using this joblog
In this parallel_worker.sh script, we propose to the user:
Either to repeat a task --tasks time
Or to provide a taskfile in which each line defines a command/app and its parameters

Example of tasks
The script contains also a mechanism to backup and clean a log directory in which each task output is stored
Each parallel worker has its own joblog
logs/
├── Worker0
│   ├── parallel_worker.log
│   ├── task.sh.log.10
│   ├── task.sh.log.12
│   ├── task.sh.log.14
│   ├── task.sh.log.16
│   ├── task.sh.log.18
│   ├── task.sh.log.2
│   ├── task.sh.log.20
│   ├── task.sh.log.4
│   ├── task.sh.log.6
│   └── task.sh.log.8
└── Worker1
    ├── parallel_worker.log
    ├── task.sh.log.1
    ├── task.sh.log.11
    ├── task.sh.log.13
    ├── task.sh.log.15
    ├── task.sh.log.17
    ├── task.sh.log.19
    ├── task.sh.log.3
    ├── task.sh.log.5
    ├── task.sh.log.7
    └── task.sh.log.9

Applications

Stress test on the Aion cluster

Among the tools to stress CPU and Memory, we have:

stress: a simple workload generator for POSIX systems
stress-ng: an updated version of stress tool with more features (CPU compute, cache thrastring, Drive stress, etc...)
To install stress-ng, please clone the following repository
Follow the code chunk below to install it in the ${HOME}/.local folder
$user@aion-XXXX> git clone https://github.com/ColinIanKing/stress-ng
$user@aion-XXXX> cd stress-ng
$user@aion-XXXX> make 
$user@aion-XXXX> ln -s $(pwd)/stress-ng ~/.local/bin/
Let's now create a task script task.sh which stresses a single core
#!/bin/bash -l

# $PARALLEL_SEQ is a special variable from GNU parallel. It gives the
# number of the job in the sequence.
#
# Here we print the sleep time, host name, and the date and time.
echo "Task ${PARALLEL_SEQ}  started on host:$(hostname) date:$(date)"
echo "Stress test on 1 core"
${HOME}/.local/bin/stress-ng --cpu 1 --timeout 600s --metrics-brief
Copy-paste the task.sh script to the current directory
Now, submit a job with `sbatch slurm_parallel_launcher.sh parallel_worker.sh -n 300 "./task.sh"

Scheduled job
You can join your allocation using srun --jobid <jobid> --overlap -w <node> --pty bash -i and monitor the cpu usage
Here, as the ekieffer user, I can join
the first allocated node with srun --jobid 375186 --overlap -w aion-0192 --pty bash -i
the second allocated node with srun --jobid 375186 --overlap -w aion-0238 --pty bash -i

htop on aion-0192

htop on aion-0238
Exercice

Provide a multi-threaded task and adapt the parallel_worker.sh consequently
hint: replace the variable NBCORES_TASK=1

HPC Management of Sequential and Embarrassingly Parallel Jobs

 Copyright (c) 2020-2023 E. Kieffer, S. Varrette and UL HPC Team <hpc-team@uni.lu>
Embarrassingly parallel jobs, also known as "perfectly parallel" or "pleasingly parallel" jobs, refer to computational tasks that can be easily divided into smaller, independent sub-tasks that can be executed simultaneously without any need for communication or interaction between them. This makes them ideal for parallel computing, where multiple processors or computing units can work on separate sub-tasks at the same time, leading to significant performance improvements.

The term "embarrassingly parallel" is used because these problems are considered relatively simple to parallelize, compared to other computational problems that require complex coordination or communication between processors.

Examples of embarrassingly parallel jobs include:

Monte Carlo simulations: Many independent runs can be executed concurrently, with each simulation using different random numbers or initial conditions.
Image or video processing: Each frame or section of an image can be processed independently, such as applying filters or transformations.
Parameter sweeps: Running a computational model with varying parameter values, where each combination of parameters is independent of the others.
Data mining: Analyzing large datasets by breaking them into smaller chunks and processing them concurrently.
Brute-force search: Trying out all possible solutions to a problem, where each possible solution can be tested independently.
Embarrassingly parallel jobs are well-suited for distributed computing environments, such as clusters, grids, or cloud computing, where multiple computing resources can be leveraged to process the sub-tasks in parallel, significantly reducing the overall time to completion.

Pre-requisites

Ensure you are able to connect to the UL HPC clusters. In particular, recall that the module command is not available on the access frontends.

Bad approaches

Job Arrays

One legitimate naive way would be to perform N tasks (N=30) which can be obtained by submitting N=30 jobs, typically through a for [...] sbatch [...] loop or job Arrays, a mechanism for submitting and managing collections of similar jobs quickly and easily.

This works of course but this is generally against best-practices (and basically every time you consider this approach, and except in a very few case, a for [...] sbatch [...] loop is probably a bad idea). Indeed: - to complete N (serial) tasks, you need to submit N jobs that could be spread on up to N different nodes. * this induces an non-necessary overload of the scheduler for (generally) very short tasks * you will likely wait (sometimes very long time) to have all your jobs completed * All these jobs will compete for a limited resource - Node coverage is sub-optimal * your serial jobs can be spread on up to N different nodes Imagine expanding the job campaign to 1000 or 10000 test cases if not more, you risk to degrade significantly the HPC environment (the scheduler will likely have trouble to manage so many short-live jobs).

To prevent this behaviour, We have thus limit the number of jobs allowed per user (see sqos). The objective is to invite you gently to regroup your tasks per node in order to better exploit their many-core configuration (28 cores on iris, 128 on aion).

So you should favor a single job running on a full node and use it to schedule inside your independent tasks. It will be both more efficient and faster to schedule as your tasks will not compete with other users

Note on Job Arrays support in Slurm: All jobs must have the same initial options (e.g. size, time limit, etc.) and you can limit the number of simultaneous jobs by specifying the array index values using the --array or -a option of the sbatch command. You SHOULD set the maximum number of simultaneously running tasks from the job array using a "%" separator -- typically match the number of cores per node (28 on iris, 128 on aion). For example `--array=0-100%28" will limit the number of simultaneously running tasks from this job array to 28. Job arrays will have several additional environment variable set:

$SLURM_ARRAY_JOB_ID will be set to the first job ID of the array
$SLURM_ARRAY_TASK_ID will be set to the job array index value.
$SLURM_ARRAY_TASK_COUNT will be set to the number of tasks in the job array.
We have configured the ULHPC facility to prevent job arrays. Indeed, massive job arrays campaign were run in the past that used to overwhelm the slurm controllers. To avoid this behaviour to repeat, we drastically reduce the capabilities of job arrays:

(access)$> scontrol show config | grep -i maxarray
In short, Don't use job arrays!!!: you can do better with GNU Parallel

For loops inside the job allocation

So you are encouraged to aggregate several serial tasks within a single job to better exploit the many-core configuration of each nodes (28 or 128 cores per node) - this would clearly be beneficial compared to you laptop that probably have "only" 4 cores. One natural way of doing so would be to aggregate these tasks within a single slurm launcher, start them in the background (i.e. as child processes) by using the ampersand & after a Bash command, and the wait command to restrict the number of simultaneous tasks:

# For illustration purpose only, DON'T DO THAT, prefer GNU Parallel
TASK=run_stressme
ncores=${SLURM_NTASKS_PER_NODE:-$(nproc --all)}
For i in {1..30}; do
    srun -n1 --exclusive -c 1 --cpu-bind=cores ${TASK} $i &
    [[ $((i%ncores)) -eq 0 ]] && wait    # Safeguard to limit simultaneously executed forsked sub-processes
done
wait
The ampersand & spawns the command srun -n1 --exclusive -c 1 --cpu-bind=cores ${TASK} $i in the background. The propose check to run the wait command allows to execute your bag of tasks by groups of 28 (or 128) to match the hardware configuration of ULHPC nodes. In generic terms, you wish to target ${SLURM_NTASKS_PER_NODE} (if set) or the output of nproc --all (28 on iris, 128 on aion) process to be forked assuming you use a full node. This approach is nevertheless sub-optimal: each bag of tasks is waiting for the longest one to complete before the next one is executed. That why you should favor the GNU Parallel approach proposed on the sequel.

Using GNU Parallel



GNU Parallel is a tool for executing tasks in parallel, typically on a single machine. When coupled with the Slurm command srun, parallel becomes a powerful way of distributing a set of embarrassingly parallel tasks amongst a number of workers. This is particularly useful when the number of tasks is significantly larger than the number of available workers (i.e. $SLURM_NTASKS), and each tasks is independent of the others.

Follow now our GNU Parallel tutorial to become more accustomed with the special (complex) syntax of this tool.

#!/bin/bash -l
#SBATCH --time=02:00:00
#SBATCH --partition=batch
#SBATCH --nodes=1
#SBATCH --exclusive
#SBATCH --mem=0
#SBATCH --ntasks-per-node=128
#SBATCH --cpus-per-task=1


# Just useful check
print_error_and_exit() { echo "***ERROR*** $*"; exit 1; }
hash parallel 2>/dev/null && test $? -eq 0 || print_error_and_exit "Parallel is not installed on the system"

# Increase the user process limit.
ulimit -u 10000

echo "Node: ${SLURM_NODELIST}"
echo "Executing ${SLURM_NTASKS} independant tasks at the same time"
export TIMESTAMP=$(date +"%Y%m%dT%H%M%S")


# the --exclusive to srun makes srun use distinct CPUs for each job step
# -N1 -n1 single task with ${SLURM_CPUS_PER_TASK} cores
SRUN="srun  --exclusive -n1 -c ${SLURM_CPUS_PER_TASK:=1} --cpu-bind=cores"

HOSTNAME=$(hostname)
LOGS="logs.${TIMESTAMP}"
RESUME=""
TASKFILE=""
NTASKS=""


#=======================
# Get Optional arguments
#=======================
while [ $# -ge 1 ]; do
    case $1 in
        -r | --resume)           shift; LOGS=$1; RESUME=" --resume ";;
        -n | --ntasks)           shift; NTASKS="$1"                            ;;
        -* | --*)                echo "[Warning] Invalid option $1"          ;;
        *)                       break                                       ;;
    esac
    shift
done

#=======================
# Get Mandatory  Options
#=======================

if [[ "$#" < 1 ]]; then
    print_error_and_exit "No tasks defined"
else
    TASKFILE="$1"
    TASKFILE_DIR=$(cd "$(dirname "${TASKFILE}")" && pwd)
    TASKFILE_NAME="$(basename "${TASKFILE}")"
fi

echo "Starting parallel worker initialisation on $(hostname)"

#=======================
# Set logs for resuming
#=======================

LOGS_DIR="${TASKFILE_DIR}/${LOGS}"
TASKFILE_MAINLOG="${LOGS_DIR}/${TASKFILE_NAME//sh/log}"
PARALLEL="parallel --delay 0.2 -j ${SLURM_NTASKS} --joblog ${TASKFILE_MAINLOG} ${RESUME}"


echo "Create logs directory if not existing"
mkdir -p ${LOGS_DIR}

if [[ -z ${NTASKS} ]];then
    cat ${TASKFILE} |                                      \
    awk -v NNODE="$SLURM_NNODES" -v NODEID="$SLURM_NODEID" \
    'NR % NNODE == NODEID' |                               \
    ${PARALLEL} "${SRUN} {1} > ${LOGS_DIR}/$(basename ${TASKFILE}).log.{%}"
else
    echo  "$(seq 1 ${NTASKS})" |                             \
    awk -v NNODE="$SLURM_NNODES" -v NODEID="$SLURM_NODEID" \
    'NR % NNODE == NODEID' |                               \
    ${PARALLEL} "${SRUN} ${TASKFILE} > ${LOGS_DIR}/$(basename ${TASKFILE}).log.{1}"
fi
The option --exclusive has been added to be sure that we do not share the node with another job.
The option --mem=0 ensures that all memory will be available
Note: Even if the previous options were missing, the --ntasks-per-node and --cpus-per-task options would ensure that all memory and cores are allocated to our job. In some situation, you may not need all cores but all the memory. Therefore I kept --mem=0 and --exclusive.
Parallel is really a fantastic tool that offers the possibility to record all performed tasks into a joblog
This is extremely useful in the case you underestimate the walltime associated to your allocation because you can resume from the last unfinished job using this joblog
In this slurm_parallel_launcher_single_node.sh script, we propose to the user:
Either to repeat a task --tasks time
Or to provide a taskfile in which each line defines a command/app and its parameters
Applications

Stress test on the Aion cluster

Among the tools to stress CPU and Memory, we have:

stress: a simple workload generator for POSIX systems
stress-ng: an updated version of stress tool with more features (CPU compute, cache thrastring, Drive stress, etc...)
To install stress-ng, please clone the following repository
Follow the code chunk below to install it in the ${HOME}/.local folder
$user@aion-XXXX> git clone https://github.com/ColinIanKing/stress-ng
$user@aion-XXXX> cd stress-ng
$user@aion-XXXX> make 
$user@aion-XXXX> ln -s $(pwd)/stress-ng ~/.local/bin/
Let's now create a task script task.sh which stresses a single core
#!/bin/bash -l

# $PARALLEL_SEQ is a special variable from GNU parallel. It gives the
# number of the job in the sequence.
#
# Here we print the sleep time, host name, and the date and time.
echo "Task ${PARALLEL_SEQ}  started on host:$(hostname) date:$(date)"
echo "Stress test on 1 core"
${HOME}/.local/bin/stress-ng --cpu 1 --timeout 600s --metrics-brief
Copy-paste the task.sh script to the current directory
Now, submit a job with sbatch slurm_parallel_launcher_single_node.sh -n 300 "./task.sh"
You can join your allocation using srun --jobid <jobid> --overlap -w <node> --pty bash -i and monitor the cpu usage

Parallel is submitting tasks to be executed in parallel on different cores

Almost done

Now all cores are occupied
The script contains also a mechanism to backup and clean a log.timestamp directory in which each task output is stored
logs.20230511T112250
├── task.log
├── task.sh.log.1
├── task.sh.log.10
├── task.sh.log.100
├── task.sh.log.101
├── task.sh.log.102
├── task.sh.log.103
├── task.sh.log.104
├── task.sh.log.105
[...]
├── task.sh.log.93
├── task.sh.log.94
├── task.sh.log.95
├── task.sh.log.96
├── task.sh.log.97
├── task.sh.log.98
└── task.sh.log.99
cat logs.20230511T112250/task.log
Seq     Host    Starttime       JobRuntime      Send    Receive Exitval Signal  Command
1       :       1683796970.000     601.000      0       0       0       0       srun  --exclusive -n1 -c 1 --cpu-bind=cores ./task.sh > /home/users/ekieffer/parallel-srun/logs.20230511T112250/task.sh.log.1
2       :       1683796971.000     600.000      0       0       0       0       srun  --exclusive -n1 -c 1 --cpu-bind=cores ./task.sh > /home/users/ekieffer/parallel-srun/logs.20230511T112250/task.sh.log.2
3       :       1683796972.000     600.000      0       0       0       0       srun  --exclusive -n1 -c 1 --cpu-bind=cores ./task.sh > /home/users/ekieffer/parallel-srun/logs.20230511T112250/task.sh.log.3
4       :       1683796973.000     600.000      0       0       0       0       srun  --exclusive -n1 -c 1 --cpu-bind=cores ./task.sh > /home/users/ekieffer/parallel-srun/logs.20230511T112250/task.sh.log.4
5       :       1683796974.000     600.000      0       0       0       0       srun  --exclusive -n1 -c 1 --cpu-bind=cores ./task.sh > /home/users/ekieffer/parallel-srun/logs.20230511T112250/task.sh.log.5
6       :       1683796975.000     600.000      0       0       0       0       srun  --exclusive -n1 -c 1 --cpu-bind=cores ./task.sh > /home/users/ekieffer/parallel-srun/logs.20230511T112250/task.sh.log.6
7       :       1683796976.000     600.000      0       0       0       0       srun  --exclusive -n1 -c 1 --cpu-bind=cores ./task.sh > /home/users/ekieffer/parallel-srun/logs.20230511T112250/task.sh.log.7
[...]
297     :       1683798213.000     601.000      0       0       0       0       srun  --exclusive -n1 -c 1 --cpu-bind=cores ./task.sh > /home/users/ekieffer/parallel-srun/logs.20230511T112250/task.sh.log.297
298     :       1683798214.000     601.000      0       0       0       0       srun  --exclusive -n1 -c 1 --cpu-bind=cores ./task.sh > /home/users/ekieffer/parallel-srun/logs.20230511T112250/task.sh.log.298
299     :       1683798215.000     601.000      0       0       0       0       srun  --exclusive -n1 -c 1 --cpu-bind=cores ./task.sh > /home/users/ekieffer/parallel-srun/logs.20230511T112250/task.sh.log.299
300     :       1683798216.000     601.000      0       0       0       0       srun  --exclusive -n1 -c 1 --cpu-bind=cores ./task.sh > /home/users/ekieffer/parallel-srun/logs.20230511T112250/task.sh.log.300
Suppose that some job tasks failed. You can resume the job using the following command: sbatch slurm_parallel_launcher_single_node.sh -n 300 -r "logs.20230511T112250" "./task.sh
Embarrassingly [GNU] parallel tasks across multiples nodes

GNU Parallel supports the distribution of tasks to multiple compute nodes using ssh connections, i.e. via the the --sshloginfile <filename> or --sshlogin options.

However, Scaling Parallel with --sshlogin[file] is Not Recommended Though this allows work to be balance between multiple nodes, past experience suggests that scaling is much less effective. Additionnaly, GNU Parallel can cause some issues regarding the way it spawns tasks across nodes. Indeed, GNU Parallel uses internally the ssh command and don't use slurm. The notion of job steps is therefore lost which can lead to a bad job cleanup. To overcome this issue, you may follow the Many Tasks — Many Node Allocations using Slurm and GNU Parallel.

Past HPC School Exercises: HPC workflow with sequential jobs

Copyright (c) 2013-2019 UL HPC Team <hpc-sysadmins@uni.lu>


Prerequisites

Make sure you have followed the tutorial "Getting started" and the HPC Management of Sequential and Embarrassingly Parallel Jobs tutorials.

For many users, the typical usage of the HPC facilities is to execute 1 program with many parameters. On your local machine, you can just start your program 100 times sequentially. However, you will obtain better results if you parallelize the executions on a HPC Cluster.

You will see below 3 use cases mentioned in past training events:

Exercise 1: Use the serial launcher (1 node, in sequential and parallel mode);
application to Object recognition with Tensorflow and Python Imageai
Exercise 2: Use the generic launcher, distribute your executions on several nodes (python script);
Illustration on an Image Watermarking process in Python
Exercise 3: Advanced use case, using a Java program: "JCell".

Serial tasks in action: Object recognition with Tensorflow and Python Imageai

Copyright (c) 2013-2019 UL HPC Team <hpc-team@uni.lu>


The following github repositories will be used:

ULHPC/launcher-scripts
UPDATE (Dec 2020) This repository is deprecated and kept for archiving purposes only. Consider the up-to-date launchers listed at the root of the ULHPC/tutorials repository, under launchers/
ULHPC/tutorials
In this exercise, we will process some images from the OpenImages V4 data set with an object recognition tools.

Create a file which contains the list of parameters (random list of images):

(access)$>  find /work/projects/bigdata_sets/OpenImages_V4/train/ -print | head -n 10000 | sort -R | head -n 50 | tail -n +2 > $SCRATCH/PS2/param_file
Step 0: Prepare the environment

(access)$> si -N 1
Load the default Python module

(node) module load lang/Python

(node) module list
Create a new python virtual env

(node) cd $SCRATCH/PS2
(node) virtualenv venv
Enable your newly created virtual env, and install the required modules inside

(node) source venv/bin/activate

(node) pip install tensorflow scipy opencv-python pillow matplotlib keras
(node) pip install https://github.com/OlafenwaMoses/ImageAI/releases/download/2.0.2/imageai-2.0.2-py3-none-any.whl

(node) exit
Step 1: Naive workflow

We will use the launcher NAIVE_AKA_BAD_launcher_serial.sh (full path: $SCRATCH/PS2/launcher-scripts/bash/serial/NAIVE_AKA_BAD_launcher_serial.sh).

Edit the following variables:

MODULE_TO_LOAD must contain the list of modules to load before executing $TASK,
TASK must contain the path of the executable,
ARG_TASK_FILE must contain the path of your parameter file.
(node)$> nano $SCRATCH/PS2/launcher-scripts/bash/serial/NAIVE_AKA_BAD_launcher_serial.sh

    MODULE_TO_LOAD=(lang/Python)
    TASK="$SCRATCH/PS2/tutorials/sequential/examples/scripts/run_object_recognition.sh"
    ARG_TASK_FILE=$SCRATCH/PS2/param_file
Using Slurm on Iris

Launch the job, in interactive mode and execute the launcher:

(access)$> si -N 1

(node)$> source venv/bin/activate
(node)$> $SCRATCH/PS2/launcher-scripts/bash/serial/NAIVE_AKA_BAD_launcher_serial.sh
Or in passive mode (the output will be written in a file named BADSerial-<JOBID>.out)

(access)$> sbatch $SCRATCH/PS2/launcher-scripts/bash/serial/NAIVE_AKA_BAD_launcher_serial.sh
You can use the command scontrol show job <JOBID> to read all the details about your job:

(access)$> scontrol show job 207001
JobId=207001 JobName=BADSerial
   UserId=hcartiaux(5079) GroupId=clusterusers(666) MCS_label=N/A
   Priority=8791 Nice=0 Account=ulhpc QOS=normal
   JobState=RUNNING Reason=None Dependency=(null)
   Requeue=0 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0
   RunTime=00:00:23 TimeLimit=01:00:00 TimeMin=N/A
   SubmitTime=2018-11-23T10:01:04 EligibleTime=2018-11-23T10:01:04
   StartTime=2018-11-23T10:01:05 EndTime=2018-11-23T11:01:05 Deadline=N/A
And the command sacct to see the start and end date

(access)$> sacct --format=start,end --j 207004
              Start                 End
------------------- -------------------
2018-11-23T10:01:20 2018-11-23T10:02:31
2018-11-23T10:01:20 2018-11-23T10:02:31
In all cases, you can connect to a reserved node using the command srun and check the status of the system using standard linux command (free, top, htop, etc)

(access)$> sjoin <JOBID>
During the execution, you can see the job in the queue with the command squeue:

(access)$> squeue
         JOBID PARTITION     NAME             USER ST       TIME  NODES NODELIST(REASON)
        207001     batch BADSeria        hcartiaux  R       2:27      1 iris-110
Using the system monitoring tool ganglia, check the activity on your node.

Step 2: Optimal method using GNU parallel (GNU Parallel)

We will use the launcher launcher_serial.sh (full path: $SCRATCH/PS2/launcher-scripts/bash/serial/launcher_serial.sh).

Edit the following variables:

(access)$> nano $SCRATCH/PS2/launcher-scripts/bash/serial/launcher_serial.sh

MODULE_TO_LOAD=(lang/Python)
TASK="$SCRATCH/PS2/tutorials/sequential/examples/scripts/run_object_recognition.sh"
ARG_TASK_FILE=$SCRATCH/PS2/param_file
Submit the (passive) job with sbatch

(access)$> sbatch $SCRATCH/PS2/launcher-scripts/bash/serial/launcher_serial.sh
Question: compare and explain the execution time with both launchers:

Naive workflow: time = ?  CPU usage for the sequential workflow
Parallel workflow: time = ?  CPU usage for the parallel workflow
Conclusion

At the end, please clean up your home and scratch directories :)

Please do not store unnecessary files on the cluster's storage servers:

(access)$> rm -rf $SCRATCH/PS2

Serial tasks in action: Watermarking images in Python

Copyright (c) 2013-2019 UL HPC Team <hpc-team@uni.lu>


The following github repositories will be used:

ULHPC/launcher-scripts
UPDATE (Dec 2020) This repository is deprecated and kept for archiving purposes only. Consider the up-to-date launchers listed at the root of the ULHPC/tutorials repository, under launchers/
ULHPC/tutorials
In this exercise, we will use another program, watermark.py (full path: $SCRATCH/PS2/tutorials/sequential/examples/scripts/watermark.py), and we will distribute the computation on 2 nodes with the launcher parallel_launcher.sh (full path: $SCRATCH/PS2/launcher-scripts/bash/generic/parallel_launcher.sh).

This python script will apply a watermark to the images (using the Python Imaging library).

The command works like this:

python watermark.py <path/to/watermark_image> <source_image>
We will work with 2 files:

copyright.png: a transparent images, which can be applied as a watermark
images.tgz: a compressed file, containing 30 JPG pictures (of the Gaia Cluster :) ).
Step 0: python image manipulation module installation

In an interactive job, install pillow in your home directory using this command:

(access IRIS)>$ si -N 1


(node)>$ pip install --user pillow
Step 1: Prepare the input files

Copy the source files in your $SCRATCH directory.

(access)>$ tar xvf /mnt/isilon/projects/ulhpc-tutorials/sequential/images2.tgz -C $SCRATCH/PS2/
(access)>$ cp /mnt/isilon/projects/ulhpc-tutorials/sequential/ulhpc_logo.png $SCRATCH/PS2

(access)>$ cd $SCRATCH/PS2
Step 2: Create a list of parameters

We must create a file containing a list of parameters, each line will be passed to watermark.py.

ls -d -1 $SCRATCH/PS2/images/*.JPG | awk -v watermark=$SCRATCH/PS2/ulhpc_logo.png '{print watermark " " $1}' > $SCRATCH/PS2/generic_launcher_param
\_____________________________/   \_________________________________________________________________/ \_________________________________/
               1                                                    2                                                3
ls -d -1: list the images
awk ...: prefix each line with the first parameter (watermark file)
>: redirect the output to the file $SCRATCH/generic_launcher_param
Step 3: Configure the launcher

We will use the launcher parallel_launcher.sh (full path: $SCRATCH/PS2/launcher-scripts/bash/generic/parallel_launcher.sh).

Edit the following variables:

(access)$> nano $SCRATCH/PS2/launcher-scripts/bash/generic/parallel_launcher.sh

TASK="$SCRATCH/PS2/tutorials/sequential/examples/scripts/watermark.py"
ARG_TASK_FILE="$SCRATCH/PS2/generic_launcher_param"
# number of cores needed for 1 task
NB_CORE_PER_TASK=2
Step 4: Submit the job

We will spawn 1 process per 2 cores on 2 nodes

On Iris, the Slurm job submission command is sbatch

(access IRIS)>$ sbatch $SCRATCH/PS2/launcher-scripts/bash/generic/parallel_launcher.sh
Step 5: Download the files

On your laptop, transfer the files in the current directory and look at them with your favorite viewer. Use one of these commands according to the cluster you have used:

(yourmachine)$> rsync -avz iris-cluster:/scratch/users/<LOGIN>/PS2/images .
Question: which nodes are you using, identify your nodes with the command sacct

Conclusion

At the end, please clean up your home and scratch directories :)

Please do not store unnecessary files on the cluster's storage servers:

(access)$> rm -rf $SCRATCH/PS2

Serial tasks in action: Genetic Algorithms Evolution with JCell

Copyright (c) 2013-2019 UL HPC Team <hpc-team@uni.lu>


The following github repositories will be used:

ULHPC/launcher-scripts
UPDATE (Dec 2020) This repository is deprecated and kept for archiving purposes only. Consider the up-to-date launchers listed at the root of the ULHPC/tutorials repository, under launchers/
ULHPC/tutorials
In this exercise, we will use JCell, a framework for working with genetic algorithms, programmed in Java.

We will use 3 scripts:

jcell_config_gen.sh (full path: $SCRATCH/PS2/tutorials/sequential/examples/scripts/jcell_config_gen.sh)
We want to execute Jcell, and change the parameters MutationProb and CrossoverProb. This script will install JCell, generate a tarball containing all the configuration files, and the list of parameters to be given to the launcher.

jcell_wrapper.sh (full path: $SCRATCH/PS2/tutorials/sequential/examples/scripts/jcell_wrapper.sh)
This script is a wrapper, and will start one execution of jcell with the configuration file given in parameter. If a result already exists, then the execution will be skipped. Thanks to this simple test, our workflow is fault tolerant, if the job is interrupted and restarted, only the missing results will be computed.

parallel_launcher.sh (full path: $SCRATCH/PS2/launcher-scripts/bash/generic/parallel_launcher.sh)
This script will drive the experiment, start and balance the java processes on all the reserved resources.

Step 1: Generate the configuration files:

Execute this script:

    (access)$> $SCRATCH/PS2/tutorials/sequential/examples/scripts/jcell_config_gen.sh
This script will generate the following files in $SCRATCH/PS2/jcell:

config.tgz
jcell_param
Step 2: Edit the launcher configuration, in the file $SCRATCH/PS2/launcher-scripts/bash/generic/parallel_launcher.sh.

This application is cpu-bound and not memory-bound, so we can set the value of NB_CORE_PER_TASK to 1. Using these parameters, the launcher will spawn one java process per core on all the reserved nodes.

    (access)$> nano $SCRATCH/PS2/launcher-scripts/bash/generic/parallel_launcher.sh

    TASK="$SCRATCH/PS2/tutorials/sequential/examples/scripts/jcell_wrapper.sh"
    ARG_TASK_FILE="$SCRATCH/PS2/jcell/jcell_param"
    # number of cores needed for 1 task
    NB_CORE_PER_TASK=1
Step 3: Submit the job

On Iris, the Slurm job submission command is sbatch

(access IRIS)>$ sbatch $SCRATCH/PS2/launcher-scripts/bash/generic/parallel_launcher.sh
Step 4. Retrieve the results on your laptop:

Use one of these commands according to the cluster you have used:

    (yourmachine)$> rsync -avz iris-cluster:/scratch/users/<LOGIN>/PS2/jcell/results .
Question: check the system load and memory usage with Ganglia

Conclusion

At the end, please clean up your home and scratch directories :)

Please do not store unnecessary files on the cluster's storage servers:

(access)$> rm -rf $SCRATCH/PS2

Scalable Science and Parallel computations with OpenMP/MPI

 Copyright (c) 2013-2021 UL HPC Team  <hpc-sysadmins@uni.lu>


When granted access to the UL HPC platform you will have at your disposal parallel computing resources.

Thus you will be able to run:

ideally parallel (OpenMP, MPI, CUDA, OpenCL...) jobs
however if your workflow involves serial tasks/jobs, you must run them efficiently
The objective of this tutorial is to show you how to run your OpenMP and/or [hybrid] MPI applications on top of the UL HPC platform.

For all the executions we are going to perform in this tutorial, you probably want to monitor the parallel execution on one of the allocated nodes. To do that, and assuming you have reserved computing resources or have a passive job running (see below):

open another terminal (or another tmux/screen window -- see ) as you'll want to monitor the execution.
Connect to the allocated node with sjoin <jobid> <nodename)
For this new terminal/window
run htop
press 'u' to filter by process owner, select your login
press 'F5' to enable the tree view
Note that there are other more advanced ways and tools to monitor parallel executions over OpenMP/MPI that are covered in another tutorial.

Finally, and this is especially true for hydrid OpenMP/MPI code, remember that you should always align resource specs with physical NUMA characteristics

Ex (AION): 16 cores per socket, 8 sockets ("physical" CPUs) per node (128c/node)
[-N <N>] --ntasks-per-node <8n> --ntasks-per-socket <n> -c <thread>
Total: <N>×8×<n> tasks, each on <thread> threads, Ensure <n>×<thread>= 16 on aion if you target a full node utilisation
Ex: -N 2 --ntasks-per-node 32 --ntasks-per-socket 4 -c 4 (Total: 64 tasks)
Ex (IRIS): 14 cores per socket, 2 sockets ("physical" CPUs) per node (28c/node)
[-N <N>] --ntasks-per-node <2n> --ntasks-per-socket <n> -c <thread>
Total: <N>×2×<n> tasks, each on <thread> threads, Ensure <n>×<thread>= 14 on iris if you target a full node utilisation
Ex: -N 2 --ntasks-per-node 4 --ntasks-per-socket 2  -c 7 (Total: 8 tasks)
Pre-requisites

Ensure you are able to connect to the UL HPC clusters. In particular, recall that the module command is not available on the access frontends. For all tests and compilation, you MUST work on a computing node

Now you'll need to pull the latest changes in your working copy of the ULHPC/tutorials you should have cloned in ~/git/github.com/ULHPC/tutorials (see "preliminaries" tutorial)

(access)$> cd ~/git/github.com/ULHPC/tutorials
(access)$> git pull
Now configure a dedicated directory ~/tutorials/OpenMP-MPI for this session

# return to your home
(access)$> mkdir -p ~/tutorials/OpenMP-MPI/bin
(access)$> cd ~/tutorials/OpenMP-MPI
# create a symbolic link to the top reference material
(access)$> ln -s ~/git/github.com/ULHPC/tutorials/parallel ref.d
$> ln -s ~/git/github.com/ULHPC/tutorials/parallel ref.d  # Symlink to the **root** reference parallel tutorial
$> ln -s ref.d/basics .   # Basics instructions
$> cd basics
Advanced users (eventually yet strongly recommended), create a Tmux session (see Tmux cheat sheet and tutorial) or GNU Screen session you can recover later. See also "Getting Started" tutorial .

# /!\ Advanced (but recommended) best-practice:
#     Always work within a TMux or GNU Screen session named '<topic>' (Adapt accordingly)
(access-aion)$> tmux new -s HPC-school   # Tmux
(access-iris)$> screen -S HPC-school     # GNU Screen
#  TMux     | GNU Screen | Action
# ----------|------------|----------------------------------------------
#  CTRL+b c | CTRL+a c   | (create) creates a new Screen window. The default Screen number is zero.
#  CTRL+b n | CTRL+a n   | (next) switches to the next window.
#  CTRL+b p | CTRL+a p   | (prev) switches to the previous window.
#  CTRL+b , | CTRL+a A   | (title) rename the current window
#  CTRL+b d | CTRL+a d   | (detach) detaches from a Screen -
# Once detached:
#   tmux ls  | screen -ls : list available screen
#   tmux att | screen -x  : reattach to a past screen
Parallel OpenMP Jobs

OpenMP (Open Multi-Processing) is a popular parallel programming model for multi-threaded applications. More precisely, it is an Application Programming Interface (API) that supports multi-platform shared memory multiprocessing programming in C, C++, and Fortran on most platforms, instruction set architectures and operating systems.

Reference website: https://www.openmp.org/
Latest version: 5.2 (Nov 2021) -- specifications
Below notes are adapted from LLNL OpenMP tutorial
OpenMP is designed for multi-processor/core, shared memory machine (nowadays NUMA). OpenMP programs accomplish parallelism exclusively through the use of threads.

A thread of execution is the smallest unit of processing that can be scheduled by an operating system.
Threads exist within the resources of a single process. Without the process, they cease to exist.
Typically, the number of threads match the number of machine processors/cores -- see Resource Allocation
Reminder: aion compute nodes MUST be seen as 8 (virtual) processors of 16 cores each, even if physically the nodes are hosting 2 physical sockets of AMD Epyc ROME 7H12 processors having 64 cores each (total: 128 cores per node). iris: compute nodes typically hosts 2 physical processors of 14 cores each (total: 28 cores per nodes). The exception are the bigmem nodes (4 physical processors of 28 cores each, total 112 cores per nodes)
However, the actual use of threads is up to the application.
OMP_NUM_THREADS (if present) specifies initially the max number of threads;
you can use omp_set_num_threads() to override the value of OMP_NUM_THREADS;
the presence of the num_threads clause overrides both other values.
OpenMP is an explicit (not automatic) programming model, offering the programmer full control over parallelization.

parallelization can be as simple as taking a serial program and inserting compiler directives....
in general, this is way more complex
OpenMP uses the fork-join model of parallel execution

FORK: the master thread then creates a team of parallel threads.
The statements in the program that are enclosed by the parallel region construct are then executed in parallel among the various team threads.
JOIN: When the team threads complete the statements in the parallel region construct, they synchronize and terminate, leaving only the master thread.


Slurm reservations for OpenMP programs

(eventually as this is the default but recommended to always specify to ensure you know what you're doing) set a single task per node with --ntasks-per-node=1
Use -c <N> (or --cpus-per-task <N>) to set the number of OpenMP threads you wish to use.
(again) The number of threads should not exceed the number of cores on a compute node.
Thus a minimal Slurm launcher for OpenMP would typically look like that -- see also our default Slurm launchers.

#!/bin/bash -l
#SBATCH --ntasks-per-node=1 # Run a single task per node, more explicit than '-n 1'
#SBATCH -c 28               # on iris: number of CPU cores i.e. OpenMP threads per task
###SBATCH -c 128            # on aion (remove first '##' and top line)
#SBATCH --time=0-01:00:00
#SBATCH -p batch

print_error_and_exit() { echo "***ERROR*** $*"; exit 1; }
module purge || print_error_and_exit "No 'module' command"
module load toolchain/foss    # or toolchain/intel

export SRUN_CPUS_PER_TASK=${SLURM_CPUS_PER_TASK}
export OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK:-1}
OPTS=$*

srun /path/to/your/threaded.app ${OPTS}
OpenMP Compilation

Toolchain	Compilation command (C)	Compilation command (C++)
toolchain/intel	icc -qopenmp [...]	icpc -qopenmp [...]
toolchain/foss	gcc -fopenmp [...]	g++  -fopenmp [...]
Hands-on: OpenMP Helloworld and matrix multiplication

You can find in src/hello_openmp.c the traditional OpenMP "Helloworld" example.

Reserve an interactive job to launch 4 OpenMP threads (for 30 minutes)
(access)$> si -c 4 -t 0:30:00
$> export OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK:-1}
Check the set variable $OMP_NUM_THREADS. Which value do you expect?

$> echo $OMP_NUM_THREADS
Check and compile the source src/hello_openmp.c to generate:

bin/${ULHPC_CLUSTER}_hello_openmp (compiled over the foss toolchain)
bin/${ULHPC_CLUSTER}_intel_hello_openmp (compiled over the intel toolchain)
indeed it's always a good practice to specify the supercomputer that was used to generate the binary. Use the global variable ${ULHPC_CLUSTER} for that.
$> cat src/hello_openmp.c
######### foss toolchain
$> module purge                # Safeguard
$> module load toolchain/foss
$> gcc -fopenmp -Wall -O2 src/hello_openmp.c -o bin/${ULHPC_CLUSTER}_hello_openmp

######### intel toolchain
$> module purge                # Safeguard
$> module load toolchain/intel
$> icc -qopenmp -xhost -Wall -O2 src/hello_openmp.c -o bin/intel_${ULHPC_CLUSTER}_hello_openmp
(only if you have trouble to compile): make omp
Execute the generated binaries multiple times. What do you notice?
Exit your interactive session (exit or CTRL-D)
Prepare a launcher script (use your favorite editor) to execute this application in batch mode -- adapt pthreads/OpenMP template Launcher
$> sbatch ./launcher.OpenMP.sh
Repeat the above procedure on a more serious computation: a naive matrix multiplication using OpenMP, those source code is located in src/matrix_mult_openmp.c

Adapt the launcher script to sustain both executions (OpenMP helloworld and matrix multiplication)

Note: if you are lazy (or late), you can use the provided launcher script scripts/launcher.OpenMP.sh.

$> ./scripts/launcher.OpenMP.sh -h
NAME
  launcher.OpenMP.sh: Generic OpenMP launcher
    Default APPDIR: /home/users/svarrette/tutorials/OpenMP-MPI/basics/bin
    Default APP: aion_hello_openmp
  Take the good habit to prefix the intel binaries (as foss toolchain is assumed by default)
  with 'intel_'

USAGE
  [sbatch] ./scripts/launcher.OpenMP.sh [-n] {intel | foss } [app]
  EXE=/path/to/multithreadedapp.exe [sbatch] ./scripts/launcher.OpenMP.sh [-n] {intel | foss }

OPTIONS:
  -n --dry-run: Dry run mode

Example:
  [sbatch] ./scripts/launcher.OpenMP.sh                          # run FOSS  build   <cluster>_hello_openmp
  [sbatch] ./scripts/launcher.OpenMP.sh intel                    # run intel build   intel_<cluster>_hello_openmp
  [sbatch] ./scripts/launcher.OpenMP.sh foss matrix_mult_openmp  # run FOSS  build   matrix_mult_openmp
  EXE=/home/users/svarrette/bin/datarace [sbatch] ./scripts/launcher.OpenMP.sh intel # run intel build  ~/bin/datarace
Now you can execute it:

$ ./scripts/launcher.OpenMP.sh
$ ./scripts/launcher.OpenMP.sh intel
$ ./scripts/launcher.OpenMP.sh foss ${ULHPC_CLUSTER}_matrix_mult_openmp
Passive jobs examples on aion:

$> sbatch -c 128 ./scripts/launcher.OpenMP.sh foss  ${ULHPC_CLUSTER}_matrix_mult_openmp
$> sbatch -c 128 ./scripts/launcher.OpenMP.sh intel ${ULHPC_CLUSTER}_matrix_mult_openmp
Check the elapsed time: what do you notice ?

(optional) Hands-on: OpenMP data race benchmark suite

One way to test most of OpenMP feature is to evaluate its execution against a benchmark. For instance, we are going to test OpenMP installation against DataRaceBench, a benchmark suite designed to systematically and quantitatively evaluate the effectiveness of data race detection tools. It includes a set of microbenchmarks with and without data races. Parallelism is represented by OpenMP directives.

$> cd ~/git/github.com/ULHPC/tutorials/parallel/basics
$> make fetch      # clone src/dataracebench
$> cd src/dataracebench
Now you can reserve the nodes and set OMP_NUM_THREADS:

Reserve an interactive job to launch a maximum of OpenMP threads on 1 node (for 1 hour)
# Example on Aion, 128 cores per node
(access)$> si --ntasks-per-node=1 -c 128 -t 1:00:00
$> export OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK:-1}
Open another terminal (or another tmux/screen window) to monitor the execution (see intructions on top).
Execute the benchmark, for instance using the intel toolchain:
$> module load toolchain/intel
$> ./check-data-races.sh --help

Usage: ./check-data-races.sh [--run] [--help] language

--help      : this option
--small     : compile and test all benchmarks using small parameters with Helgrind, ThreadSanitizer, Archer, Intel inspector.
--run       : compile and run all benchmarks with gcc (no evaluation)
--run-intel : compile and run all benchmarks with Intel compilers (no evaluation)
--helgrind  : compile and test all benchmarks with Helgrind
--tsan-clang: compile and test all benchmarks with clang ThreadSanitizer
--tsan-gcc  : compile and test all benchmarks with gcc ThreadSanitizer
--archer    : compile and test all benchmarks with Archer
--coderrect : compile and test all benchmarks with Coderrect Scanner
--inspector : compile and test all benchmarks with Intel Inspector
--romp      : compile and test all benchmarks with Romp
--llov    : compile and test all benchmarks with LLVM OpenMP Verifier (LLOVE)
--customize : compile and test customized test list and tools

$> ./check-data-races.sh --run-intel C
Useful OpenMP links:

https://www.openmp.org/
OpenMP Tutorial LLNL
Data race benchmark suite
Parallel/Distributed MPI Jobs

The Message Passing Interface (MPI) Standard is a message passing library standard based on the consensus of the MPI Forum. The goal of the Message Passing Interface is to establish a portable, efficient, and flexible standard for message passing that will be widely used for writing message passing programs. MPI is not an IEEE or ISO standard, but has in fact, become the "industry standard" for writing message passing programs on HPC platforms.

Reference website: https://www.mpi-forum.org/
Latest version: 4.0 (June 2021) -- specifications
Below notes are adapted from LLNL MPI tutorial
In the MPI programming model, a computation comprises one or more processes that communicate by calling library routines to send and receive messages to other processes. In most MPI implementations, a fixed set of processes is created at program initialization, and one process is created per processor.

MPI implementations

The UL HPC platform offers to you different MPI implementations:

MPI Suite	module load...	Compiler (C)	Compiler (C++)
Intel MPI	toolchain/intel	mpiicc [...]	mpiicpc [...]
OpenMPI	mpi/OpenMPI	mpicc  [...]	mpic++  [...]
MPI compilation

MPI Suite	Example Compilation command
Intel MPI	{mpiicc/mpiicpc} -Wall [-qopenmp] [-xhost] -O2 [...]
OpenMPI	{mpicc/mpic++}   -Wall [-fopenmp] -O2 [...]
Of course, it is possible to have hybrid code, mixing MPI and OpenMP primitives.

Slurm reservations and usage for MPI programs

set the number of distributed nodes you want to reserver with -N <N>
set the number of MPI processes per node (that's more explicit) with --ntasks-per-node=<N>
you can also use -n <N> to specify the total number of MPI processes you want, but the above approach is advised.
(eventually as this is the default) set a single thread per MPI process with -c 1
except when running an hybrid code...
Important:

To run your MPI program, be aware that Slurm is able to directly launch MPI tasks and initialize of MPI communications via Process Management Interface (PMI) and PMIx
permits to resolve the task affinity by the scheduler (avoiding to use mpirun --map-by [...])
Simply use (whatever MPI flavor you use):
    srun -n $SLURM_NTASKS /path/to/mpiprog [...]
Thus a minimal launcher for OpenMPI would typically look like that -- see MPI template Launcher

#!/bin/bash -l
#SBATCH -N 2
#SBATCH --ntasks-per-node 128    # MPI processes per node - use 28 on iris
#SBATCH -c 1
#SBATCH --time=0-01:00:00
#SBATCH -p batch

print_error_and_exit() { echo "***ERROR*** $*"; exit 1; }
module purge || print_error_and_exit "No 'module' command"
module load toolchain/foss
module load mpi/OpenMPI
OPTS=$*

srun -n $SLURM_NTASKS /path/to/your/openmpi.app ${OPTS}
In the above example, 2x128 = 256 MPI processes will be launched (matches Aion configuration). You will have to adapt it for running on Iris.

Hands-on: MPI Helloworld and matrix multiplication

You can find in src/hello_mpi.c the traditional MPI "Helloworld" example.

Reserve an interactive job to launch 6 MPI processes across two nodes 2x3 (for 30 minutes)
(access)$> si -N 2 --ntasks-per-node=3 -t 0:30:00
Check and compile the source src/hello_mpi.c to generate:
bin/openmpi_hello_mpi (compiled with the mpi/OpenMPI module)
bin/intel_hello_mpi (compiled over the intel toolchain and Intel MPI)
$> cat src/hello_mpi.c
######### OpenMPI
$> module purge                # Safeguard
$> module load mpi/OpenMPI
$> mpicc -Wall -O2 src/hello_mpi.c -o bin/openmpi_${ULHPC_CLUSTER}_hello_mpi

######### Intel MPI
$> module purge                # Safeguard
$> module load toolchain/intel
$> mpiicc -Wall -xhost -O2 src/hello_mpi.c -o bin/intel_${ULHPC_CLUSTER}_hello_mpi
(only if you have trouble to compile): make mpi
Execute the generated binaries multiple times. What do you notice?
Exit your interactive session (exit or CTRL-D)
Prepare a launcher script (use your favorite editor) to execute this application in batch mode -- -- adapt MPI template Launcher
$> sbatch ./launcher.MPI.sh
Repeat the above procedure on a more serious computation: a naive matrix multiplication using MPI, those source code is located in src/matrix_mult_mpi.c

Adapt the launcher script to sustain both executions (MPI helloworld and matrix multiplication)

Note: if you are lazy (or late), you can use the provided launcher script scripts/launcher.MPI.sh.

$ ./scripts/launcher.MPI.sh -h
NAME
  launcher.MPI.sh: Generic MPI launcher
    Default APPDIR: /Users/svarrette/tutorials/OpenMP-MPI/basics/bin
    Default APP: _hello_mpi
  Take the good habit to prefix the binary to execute with MPI suit used for
  the build. Here the default MPI application run would be
        EXE=/Users/svarrette/tutorials/OpenMP-MPI/basics/bin/openmpi__hello_mpi
  which will be run as     srun -n $SLURM_NTASKS [...]

USAGE
  [sbatch] ./scripts/launcher.MPI.sh [-n] {intel | openmpi | mvapich2} [app]
  EXE=/path/to/mpiapp.exe [sbatch] ./scripts/launcher.MPI.sh [-n] {intel | openmpi | mvapich2}

OPTIONS:
  -n --dry-run: Dry run mode

Example:
  [sbatch] ./scripts/launcher.MPI.sh                          # run OpenMPI build    openmpi_<cluster>_hello_mpi
  [sbatch] ./scripts/launcher.MPI.sh intel                    # run Intel MPI build  intel_<cluster>_hello_mpi
  [sbatch] ./scripts/launcher.MPI.sh openmpi matrix_mult_mpi  # run OpenMPI build    openmpi_matrix_mult_mpi
  EXE=/Users/svarrette/bin/xhpl [sbatch] ./scripts/launcher.MPI.sh intel # run intel build  ~/bin/xhpl
Now you can execute it:

$ ./scripts/launcher.MPI.sh
$ ./scripts/launcher.MPI.sh intel
$ ./scripts/launcher.MPI.sh ${ULHPC_CLUSTER}_matrix_mult_mpi
$ ./scripts/launcher.MPI.sh intel ${ULHPC_CLUSTER}_matrix_mult_mpi
Passive jobs examples:

$ sbatch --ntasks-per-node 128 ./scripts/launcher.MPI.sh openmpi
$ sbatch --ntasks-per-node 128 ./scripts/launcher.MPI.sh intel   ${ULHPC_CLUSTER}_matrix_mult_mpi
Check the elapsed time: what do you notice ?

Useful MPI links:

http://www.mpi-forum.org/docs/
MPI Tutorial LLNL
Intel MPI:
Step by Step Performance Optimization with Intel® C++ Compiler
Intel(c) C++ Compiler Developer Guide and Reference
Hybrid OpenMP+MPI Programs

Of course, you can have hybrid code mixing MPI and OpenMP primitives.

You need to compile the code with the -qopenmp (with Intel MPI) or -fopenmp (for the other MPI suits) flags
You need to adapt the OMP_NUM_THREADS environment variable accordingly
you need to adapt the value -c <N> (or --cpus-per-task <N>) to set the number of OpenMP threads you wish to use per MPI process
try to inherit from the Slurm allocation (and provide a meaningfull default value): export OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK:-1}
For best performances, you MUST align resource specs with physical NUMA characteristics -- see ULHPC Technical documentation on Slurm Resource Allocation
aion compute nodes MUST be seen as 8 (virtual) processors of 16 cores each, even if physically the nodes are hosting 2 physical sockets of AMD Epyc ROME 7H12 processors having 64 cores each (total: 128 cores per node).
iris compute nodes typically hosts 2 physical processors of 14 cores each (total: 28 cores per nodes). The exception are the bigmem nodes (4 physical processors of 28 cores each, total 112 cores per nodes).
Other misc considerations:

You need to ensure the environment variable OMP_NUM_THREADS is shared across the nodes
(Intel MPI only) you probably want to set I_MPI_PIN_DOMAIN=omp
Like any MPI execution, simply use for whatever MPI flavor you use:
 srun -n $SLURM_NTASKS /path/to/hybrid [...]
Thus a minimal launcher for hybrid OpenMP/MPI would typically look like that -- see Hybrid OpenMP+MPI template Launcher

#!/bin/bash -l     # Multi-node hybrid application IntelMPI+OpenMP launcher
#SBATCH -N 2
#SBATCH --ntasks-per-node   8    # MPI processes per node - use 2 on iris
#SBATCH --ntasks-per-socket 1    # MPI processes per [virtual] processor
#SBATCH -c 16                    # OpenMP threads per MPI process - use 14 on iris
#SBATCH --time=0-01:00:00
#SBATCH -p batch

print_error_and_exit() { echo "***ERROR*** $*"; exit 1; }
module purge || print_error_and_exit "No 'module' command"
module load mpi/OpenMPI   # or toolchain/intel
export OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK:-1}
OPTS=$*

srun -n $SLURM_NTASKS /path/to/your/parallel-hybrid-app ${OPTS}
In the above example, 2x8 = 16 MPI processes will be launched, each with 16 OpenMP threads (to match Aion configuration). You will have to adapt it for running on Iris.

Hands-on: Hybrid OpenMP+MPI Helloworld

You can find in src/hello_hybrid.c the traditional OpenMP+MPI "Helloworld" example.

Reserve an interactive job to launch 2 MPI processes (1 per node), each composed of 4 OpenMP threads (for 30 minutes)
(access)$ si -N 2 --ntasks-per-node=1 -c 4 -t 0:30:00
$ export OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK:-1}
Check the set variable $OMP_NUM_THREADS. Which value do you expect?

$> echo $OMP_NUM_THREADS
Check and compile the source src/hello_hybrid.c to generate:

bin/openmpi_hello_hybrid (compiled with the mpi/OpenMPI module)
bin/intel_hello_hybrid (compiled over the intel toolchain and Intel MPI)
$> cat src/hello_hybrid.c
######### OpenMPI
$> module purge                # Safeguard
$> module load mpi/OpenMPI
$> mpicc -fopenmp -Wall -O2 src/hello_hybrid.c -o bin/openmpi_${ULHPC_CLUSTER}_hello_hybrid

######### Intel MPI
$> module purge                # Safeguard
$> module load toolchain/intel
$> mpiicc -qopenmp -Wall -xhost -O2 src/hello_hybrid.c -o bin/intel_${ULHPC_CLUSTER}_hello_hybrid

(only if you have trouble to compile): make hybrid
Execute the generated binaries (see above tips)
Exit your interactive session (exit or CTRL-D)
Adapt the MPI launcher to allow for batch jobs submissions over hybrid programs
$> sbatch ./launcher.hybrid.sh
Note: if you are lazy (or late), you can use the provided launcher script runs/launcher.hybrid.sh.

$ ./scripts/launcher.hybrid.sh -h
NAME
  launcher.hybrid.sh: Generic Hybrid OpenMP+MPI launcher
    Default APPDIR: /Users/svarrette/tutorials/OpenMP-MPI/basics/bin
    Default APP: _hello_hybrid
  Take the good habit to prefix the binary to execute with MPI suit used for
  the build. Here the default Hybrid OpenMP+MPI application run would be
        EXE=/Users/svarrette/tutorials/OpenMP-MPI/basics/bin/openmpi__hello_hybrid
  which will be run as     srun -n $SLURM_NTASKS [...]

USAGE
  [sbatch] ./scripts/launcher.hybrid.sh [-n] {intel | openmpi | mvapich2} [app]
  EXE=/path/to/hydridapp.exe [sbatch] ./scripts/launcher.hybrid.sh [-n] {intel | openmpi | mvapich2}

OPTIONS:
  -n --dry-run: Dry run mode

Example:
  [sbatch] ./scripts/launcher.hybrid.sh                          # run hybrid OpenMPI build    openmpi_<cluster>_hello_hybrid
  [sbatch] ./scripts/launcher.hybrid.sh intel                    # run hybrid Intel MPI build  intel_<cluster>_hello_hybrid
  [sbatch] ./scripts/launcher.hybrid.sh openmpi matrix_mult      # run hybrid OpenMPI build    openmpi_matrix_mult
  EXE=/Users/svarrette/bin/hpcg [sbatch] ./scripts/launcher.hybrid.sh intel # run hybrid intel build ~/bin/hpcg
Now you can execute it:

$ ./scripts/launcher.hybrid.sh
$ ./scripts/launcher.hybrid.sh intel
Passive jobs examples:

# On Aion, you need to adapt the default settings
$> sbatch --ntasks-per-node 8 -c 16 ./scripts/launcher.hybrid.sh
$> sbatch --ntasks-per-node 8 -c 16 ./scripts/launcher.hybrid.sh intel
Code optimization tips for your OpenMP and/or MPI programs

Consider changing your memory allocation functions to avoid fragmentation and enable scalable concurrency support (this applies for OpenMP and/or MPI programs)

Facebook's jemalloc
Google's tcmalloc
When using the intel toolchain:

see the Step by Step Performance Optimization with Intel(c) C++ Compiler
the -xhost option permits to enable processor-specific optimization.
you might wish to consider Interprocedural Optimization (IPO) approach, an automatic, multi-step process that allows the compiler to analyze your code to determine where you can benefit from specific optimizations.
Troubleshooting

srun: error: PMK_KVS_Barrier duplicate request from task ...
you are trying to use mpirun (instead of srun) from Intel MPI within a SLURM session and receive such error on mpirun: make sure $I_MPI_PMI_LIBRARY is not set (`unset I_MPI_PMI_LIBRARY``).

UL HPC MPI Tutorial: Building and Runnning OSU Micro-Benchmarks

 Copyright (c) 2013-2021 UL HPC Team  <hpc-sysadmins@uni.lu>
The objective of this tutorial is to compile and run on of the OSU micro-benchmarks which permit to measure the performance of an MPI implementation. Kindly ensure your followed the "Scalable Science and Parallel computations with OpenMP/MPI" tutorial

The latest version of this tutorial is available on Github. Finally, advanced MPI users might be interested to take a look at the Intel Math Kernel Library Link Line Advisor.

Objectives

The OSU micro-benchmarks feature a series of MPI benchmarks that measure the performances of various MPI operations:

Point-to-Point MPI Benchmarks: Latency, multi-threaded latency, multi-pair latency, multiple bandwidth / message rate test bandwidth, bidirectional bandwidth
Collective MPI Benchmarks: Collective latency tests for various MPI collective operations such as MPI_Allgather, MPI_Alltoall, MPI_Allreduce, MPI_Barrier, MPI_Bcast, MPI_Gather, MPI_Reduce, MPI_Reduce_Scatter, MPI_Scatter and vector collectives.
One-sided MPI Benchmarks: one-sided put latency (active/passive), one-sided put bandwidth (active/passive), one-sided put bidirectional bandwidth, one-sided get latency (active/passive), one-sided get bandwidth (active/passive), one-sided accumulate latency (active/passive), compare and swap latency (passive), and fetch and operate (passive) for MVAPICH2 (MPI-2 and MPI-3).
Since the 4.3 version, the OSU micro-benchmarks also features OpenSHMEM benchmarks, a 1-sided communications library.
In this tutorial, we will build version 5.8 of the OSU micro-benchmarks (the latest at the time of writing), and focus on two of the available tests:

osu_get_latency - Latency Test
osu_get_bw - Bandwidth Test
The latency tests are carried out in a ping-pong fashion. The sender sends a message with a certain data size to the receiver and waits for a reply from the receiver. The receiver receives the message from the sender and sends back a reply with the same data size. Many iterations of this ping-pong test are carried out and average one-way latency numbers are obtained. Blocking version of MPI functions (MPI_Send and MPI_Recv) are used in the tests.

The bandwidth tests were carried out by having the sender sending out a fixed number (equal to the window size) of back-to-back messages to the receiver and then waiting for a reply from the receiver. The receiver sends the reply only after receiving all these messages. This process is repeated for several iterations and the bandwidth is calculated based on the elapsed time (from the time sender sends the first message until the time it receives the reply back from the receiver) and the number of bytes sent by the sender. The objective of this bandwidth test is to determine the maximum sustained date rate that can be achieved at the network level. Thus, non-blocking version of MPI functions (MPI_Isend and MPI_Irecv) were used in the test.

The idea is to compare the different MPI implementations available on the UL HPC platform.:

Intel MPI
OpenMPI
For the sake of time and simplicity, we will focus on the first two suits. Eventually, the benchmarking campain will typically involves for each MPI suit two nodes

Pre-requisites

If not yet done, you'll need to pull the latest changes in your working copy of the ULHPC/tutorials you should have cloned in ~/git/github.com/ULHPC/tutorials (see "preliminaries" tutorial)

(access)$ cd ~/git/github.com/ULHPC/tutorials
(access)$ git pull
Now configure a dedicated directory ~/tutorials/OSU-MicroBenchmarks for this session

# return to your home
(access)$ mkdir -p ~/tutorials/OSU-MicroBenchmarks
(access)$ cd ~/tutorials/OSU-MicroBenchmarks
# create a symbolic link to the top reference material
(access)$ ln -s ~/git/github.com/ULHPC/tutorials/parallel/mpi/OSU_MicroBenchmarks ref.d  # Symlink to the reference tutorial
# create other convenient symlinks
(access)$ ln -s ref.d/Makefile .     # symlink to the root Makefile - DO NOT forget the trailing '.'
(access)$ ln -s ref.d/scripts  .     # symlinkls to launcher/build scripts - DO NOT forget the trailing '.'
Advanced users (eventually yet strongly recommended), create a Tmux session (see Tmux cheat sheet and tutorial) or GNU Screen session you can recover later. See also "Getting Started" tutorial .

Now you can reserve an interactive job on 2 nodes and 1 task per node (for 30 minutes)

(access)$> si -N 2 --ntasks-per-node=1 -t 0:30:00
$ echo $SLURM_NTASKS
Fetch and uncompress OSU Micro-benchmarks Sources

Fetch and uncompress the latest version of the OSU micro-benchmarks

$ cd ~/tutorials/OSU-MicroBenchmarks
$ mkdir src
$ cd src
# Download the latest version
$ export OSU_VERSION=5.8     # Just to abstract from the version to download
$ wget --no-check-certificate http://mvapich.cse.ohio-state.edu/download/mvapich/osu-micro-benchmarks-${OSU_VERSION}.tgz
$ tar xf osu-micro-benchmarks-${OSU_VERSION}.tar.gz
$ cd osu-micro-benchmarks-${OSU_VERSION}
(only if you have trouble to fetch and uncompress): make uncompress
Building the OSU Micro-benchmarks

We will build the OSU micro-benchmarks for each considered MPI suit, thus in a separate directory build.<suit> -- that's a good habit you're encouraged to follow (as with CMake based projects) In all cases, you should now operate the compilation within an interactive job to be able to use the module command.

# If not yet done
(access)$> si -N 2 --ntasks-per-node=1
Compilation based on the Intel MPI suit

We are first going to use the Intel Cluster Toolkit Compiler Edition, which provides Intel C/C++ and Fortran compilers, Intel MPI. We will compile the OSU micro-benchmarks in a specific directory (that a good habbit)

$ cd ~/tutorials/OSU-MicroBenchmarks/
$ mkdir build.intel    # Prepare the specific building directory
$ cd  build.intel
$ echo $OSU_VERSION    # Check that the variable is defined and with teh appropriate value
# Load the appropriate module
$ module load toolchain/intel
# Configure the Intel MPI-based build for installation in the current directory
$ ../src/osu-micro-benchmarks-${OSU_VERSION}/configure CC=mpiicc CXX=mpiicpc CFLAGS=-I$(pwd)/../src/osu-micro-benchmarks-${OSU_VERSION}/util --prefix=$(pwd)
$ make && make install
Questions:

Q1: Why did we request the use of the --prefix at the configure step ?
Q2: Why did we enforce the environment variables CC and CXX?
Q3: Why did we enforce the environment variables CFLAGS?
If everything goes fine, you shall have the OSU micro-benchmarks installed in the directory libexec/osu-micro-benchmarks/mpi/.

(only if you have trouble to compile): ./scripts/build.OSU intel
Once compiled, ensure you are able to run it:

$ cd libexec/osu-micro-benchmarks/mpi/one-sided/
$ srun -n $SLURM_NTASKS ./osu_get_latency
$ srun -n $SLURM_NTASKS ./osu_get_bw
Compilation based on the OpenMPI suit

Repeat the procedure for the OpenMPI suit:

$ cd ~/tutorials/OSU-MicroBenchmarks/
$ mkdir build.openmpi    # Prepare the specific building directory
$ cd  build.openmpi
$ echo $OSU_VERSION    # Check that the variable is defined and with teh appropriate value
# Clean the previously loaded module and load the appropriate OpenMPI one
$ module purge
$ module load mpi/OpenMPI
$ module list
# Configure the OpenMPI-based build for installation in the current directory
$> ../src/osu-micro-benchmarks-${OSU_VERSION}/configure CC=mpicc CFLAGS=-I$(pwd)/../src/osu-micro-benchmarks-${OSU_VERSION}/util --prefix=$(pwd)
$> make && make install
Questions:

Q4: Why do we need to precise the CC variable at the configure step?
Q5: How to get rid of CFLAGS at the configure step ?
(only if you have trouble to compile): ./scripts/build.OSU
Once compiled, ensure you are able to run it:

$ cd libexec/osu-micro-benchmarks/mpi/one-sided/
$ srun -n $SLURM_NTASKS ./osu_get_latency
$ srun -n $SLURM_NTASKS ./osu_get_bw
OSU Launcher script

We are now going to prepare launcher scripts to permit passive runs (typically in the {default | batch} queue). We will place them in a separate directory (runs/) as it will host the outcomes of the executions on the UL HPC platform .

$ cd ~/tutorials/OSU-MicroBenchmarks/
$ mkdir runs    # Prepare the specific run directory
$ cd runs
$ cp ../ref.d/runs/launcher.OSU.sh .
You can test the script in an interactive job as follows:

# go into the runs directory
$ cd ~/tutorials/OSU-MicroBenchmarks/runs
$ si -N 2 --ntasks-per-node=1     # create an interactive job, 1 core on 2 nodes
$ ./launcher.OSU.sh -h
$ ./launcher.OSU.sh
$ ./launcher.OSU.sh intel
Exit your job and test it in batch mode:

$ cd ~/tutorials/OSU-MicroBenchmarks/runs
$ sbatch ./launcher.OSU.sh
$ sbatch ./launcher.OSU.sh intel
Now for Lazy / frustrated persons

You will find in the UL HPC tutorial repository, under the parallel/mpi/OSU_MicroBenchmarks directory, a set of tools / script that facilitate the running and analysis of this tutorial that you can use/adapt to suit your needs. In particular,

running make fetch will automatically download the archives for the OSU micro-benchmarks in the src/ directory
The different launcher files in runs/
Some sample output data in runs/data/
run make build to build the different versions of the OSU Micro-benchmarks
run make plot to invoke the Gnuplot script  plots/benchmark_OSU.gnuplot and generate various plots from the sample runs.
You can find the obtained results (long time ago ;)) on the iris cluster:

 
 High-Performance Linpack (HPL) benchmarking on UL HPC platform

 Copyright (c) 2013-2021 UL HPC Team  <hpc-sysadmins@uni.lu>
The objective of this tutorial is to compile and run on of the reference HPC benchmarks, HPL, on top of the UL HPC platform. Kindly ensure your followed the "Scalable Science and Parallel computations with OpenMP/MPI" tutorial

The latest version of this tutorial is available on Github and on http://ulhpc-tutorials.readthedocs.io/en/latest/parallel/mpi/HPL/

Resources

Tweak HPL parameters
HPL Calculator to find good parameters and expected performances
Intel Math Kernel Library Link Line Advisor
Pre-requisites

If not yet done, you'll need to pull the latest changes in your working copy of the ULHPC/tutorials you should have cloned in ~/git/github.com/ULHPC/tutorials (see "preliminaries" tutorial)

(access)$ cd ~/git/github.com/ULHPC/tutorials
(access)$ git pull
Now configure a dedicated directory ~/tutorials/HPL for this session

(access)$ mkdir -p ~/tutorials/HPL
(access)$ cd ~/tutorials/HPL
# create a symbolic link to the top reference material
(access)$ ln -s ~/git/github.com/ULHPC/tutorials/parallel/mpi/HPL ref.d
# create other convenient symlinks
(access)$ ln -s ref.d/Makefile .     # symlink to the root Makefile
Advanced users (eventually yet strongly recommended), create a Tmux session (see Tmux cheat sheet and tutorial) or GNU Screen session you can recover later. See also "Getting Started" tutorial .

Theoretical Peak Performance Rpeak

The ULHPC computing nodes on aion or iris feature the following types of processors (see also /etc/motd on the access node):

Cluster	Vendor	Model	#cores	TDP	Freq.	AVX512 Freq	Nodes	Rpeak	Rpeak
Aion	AMD	AMD Epyc ROME 7H12	64	280W	2.6 GHz	n/a	aion-[0001-0318]	2.66 TF	2.13 TF
Iris	Intel	Xeon E5-2680v4 (broadwell)	14	120W	2.4Ghz	n/a	iris-[001-108]	0.538 TF	0.46 TF
Iris	Intel	Xeon Gold 6132 (skylake)	14	140W	2.6GHz	2.3 GHz	iris-[109-186,191-196]	1.03 TF	0.88 TF
Iris	Intel	Xeon Platinum 8180M (skylake)	28	205W	2.5GHz	2.3 GHz	iris-[187-190]	2.06 TF	1.75 TF
Computing the theoretical peak performance of these processors is done using the following formulae:

Rpeak = #Cores x [AVX512 All cores Turbo] Frequency x #DP_ops_per_cycle

Knowing that:

Broadwell processors (iris-[001-108] nodes) carry on 16 DP ops/cycle and supports AVX2/FMA3.
Skylake processors (iris-[109-196] nodes) belongs to the Gold or Platinum family and thus have two AVX512 units, thus they are capable of performing 32 Double Precision (DP) Flops/cycle. From the reference Intel documentation, it is possible to extract for the featured model the AVX-512 Turbo Frequency (i.e., the maximum core frequency in turbo mode) in place of the base non-AVX core frequency that can be used to compute the peak performance (see Fig. 3 p.14).
AMD Epyc processors carry on 16 Double Precision (DP) ops/cycle.
HPL permits to measure the effective Rmax performance (as opposed to the above peak performance Rpeak). The ratio Rmax/Rpeak corresponds to the HPL efficiency.

Objectives

HPL is a portable implementation of the High-Performance Linpack (HPL) Benchmark for Distributed-Memory Computers. It is used as reference benchmark to provide data for the Top500 list and thus rank to supercomputers worldwide. HPL rely on an efficient implementation of the Basic Linear Algebra Subprograms (BLAS). You have several choices at this level:

Intel MKL
ATLAS
GotoBlas
The idea is to compare the different MPI and BLAS implementations:

Intel MPI and the Intel MKL
OpenMPI
ATLAS
GotoBlas
For the sake of time and simplicity, we will focus on the combination expected to lead to the best performant runs, i.e. Intel MKL and Intel MPI suite, either in full MPI or in hybrid run (on 1 or 2 nodes). As a bonus, comparison with the reference HPL binary compiled as part of the toolchain/intel will be considered.

Fetching the HPL sources

In the working directory ~/tutorials/HPL, fetch and uncompress the latest version of the HPL benchmark (i.e. version 2.3 at the time of writing).

$ cd ~/tutorials/HPL
$ mkdir src
# Download the sources
$ cd src
# Download the latest version
$ export HPL_VERSION=2.3
$ wget --no-check-certificate http://www.netlib.org/benchmark/hpl/hpl-${HPL_VERSION}.tar.gz
$ tar xvzf hpl-${HPL_VERSION}.tar.gz
$ cd  hpl-${HPL_VERSION}
Alternatively, you can use the following command to fetch and uncompress the HPL sources:

$ cd ~/tutorials/HPL
$ make fetch
$ make uncompress
Building the HPL benchmark

We are first going to use the Intel Cluster Toolkit Compiler Edition, which provides Intel C/C++ and Fortran compilers, Intel MPI.

$ cd ~/tutorials/HPL
# Copy the provided Make.intel64
$ cp ref.d/src/Make.intel64 src/
Now you can reserve an interactive job for the compilation from the access server:

# Quickly get one interactive job for 1h
$ si -N 2 --ntasks-per-node 2
# OR get one interactive (totalling 2*2 MPI processes) on broadwell-based nodes
$ si -C broadwell -N2 --ntasks-per-node 2
# OR get one interactive (totalling 2*2 MPI processes) on skylake-based nodes
$ si -C skylake -N2 --ntasks-per-node 2
Now that you are on a computing node, you can load the appropriate module for Intel MKL and Intel MPI suite, i.e. toolchain/intel:

# Load the appropriate module
$ module load toolchain/intel
$ module list
Intel MKL is now loaded.

Read the INSTALL file under src/hpl-2.3. In particular, you'll have to edit and adapt a new makefile Make.intel64 (inspired from setup/Make.Linux_Intel64 typically) and provided to you provided to you on Github for that purpose.

cd src/hpl-2.3
cp ../Make.intel64 .
# OR (if the above command fails)
# cp ~/git/github.com/ULHPC/tutorials/parallel/mpi/HPL/src/Make.intel64  Make.intel64
# Automatically adapt at least the TOPdir variable to the current directory $(pwd),
# thus it SHOULD be run from 'src/hpl-2.3'
sed -i \
  -e "s#^[[:space:]]*TOPdir[[:space:]]*=[[:space:]]*.*#TOPdir = $(pwd)#" \
  Make.intel64
# Check the difference:
$ diff -ru ../Make.intel64 Make.intel64
--- ../Make.intel64     2019-11-19 23:43:26.668794000 +0100
+++ Make.intel64        2019-11-20 00:33:21.077914972 +0100
@@ -68,7 +68,7 @@
 # - HPL Directory Structure / HPL library ------------------------------
 # ----------------------------------------------------------------------
 #
-TOPdir       = $(HOME)/benchmarks/HPL/src/hpl-2.3
+TOPdir = /home/users/svarrette/tutorials/HPL/src/hpl-2.3
 INCdir       = $(TOPdir)/include
 BINdir       = $(TOPdir)/bin/$(ARCH)
 LIBdir       = $(TOPdir)/lib/$(ARCH)
In general, to build HPL, you first need to configure correctly the file Make.intel64. Take your favorite editor (vim, nano, etc.) to modify it. In particular, you should adapt:

TOPdir to point to the directory holding the HPL sources (i.e. where you uncompress them: $(HOME)/tutorials/HPL/src/hpl-2.3)
this was done using the above sed command
Adapt the MP* variables to point to the appropriate MPI libraries path.
Correct the OpenMP definitions OMP_DEFS
(eventually) adapt the CCFLAGS
in particular, with the Intel compiling suite, you SHOULD at least add -xHost to ensure the compilation that will auto-magically use the appropriate compilation flags -- see (again) the Intel Math Kernel Library Link Line Advisor
(eventually) adapt the ARCH variable
Here is for instance a suggested difference for intel MPI:

--- setup/Make.Linux_Intel64    1970-01-01 06:00:00.000000000 +0100
+++ Make.intel64        2019-11-20 00:15:11.938815000 +0100
@@ -61,13 +61,13 @@
 # - Platform identifier ------------------------------------------------
 # ----------------------------------------------------------------------
 #
-ARCH         = Linux_Intel64
+ARCH         = $(arch)
 #
 # ----------------------------------------------------------------------
 # - HPL Directory Structure / HPL library ------------------------------
 # ----------------------------------------------------------------------
 #
-TOPdir       = $(HOME)/hpl
+TOPdir       = $(HOME)/tutorials/HPL/src/hpl-2.3
 INCdir       = $(TOPdir)/include
 BINdir       = $(TOPdir)/bin/$(ARCH)
 LIBdir       = $(TOPdir)/lib/$(ARCH)
@@ -81,9 +81,9 @@
 # header files,  MPlib  is defined  to be the name of  the library to be
 # used. The variable MPdir is only used for defining MPinc and MPlib.
 #
-# MPdir        = /opt/intel/mpi/4.1.0
-# MPinc        = -I$(MPdir)/include64
-# MPlib        = $(MPdir)/lib64/libmpi.a
+MPdir        = $(I_MPI_ROOT)/intel64
+MPinc        = -I$(MPdir)/include
+MPlib        = $(MPdir)/lib/libmpi.a
 #
 # ----------------------------------------------------------------------
 # - Linear Algebra library (BLAS or VSIPL) -----------------------------
@@ -177,9 +178,9 @@
 #
 CC       = mpiicc
 CCNOOPT  = $(HPL_DEFS)
-OMP_DEFS = -openmp
-CCFLAGS  = $(HPL_DEFS) -O3 -w -ansi-alias -i-static -z noexecstack -z relro -z now -nocompchk -Wall
-#
+OMP_DEFS = -qopenmp
+CCFLAGS  = $(HPL_DEFS) -O3 -w -ansi-alias -i-static -z noexecstack -z relro -z now -nocompchk -Wall -xHost
+
 #
 # On some platforms,  it is necessary  to use the Fortran linker to find
 # the Fortran internals used in the BLAS library.
Once tweaked, run the compilation by:

$> make arch=intel64 clean_arch_all
$> make arch=intel64
If you don't succeed by yourself, use the following Make.intel64.

Once compiled, ensure you are able to run it (you will need at least 4 MPI processes -- for instance with -N 2 --ntasks-per-node 2):

$> cd ~/tutorials/HPL/src/hpl-2.3/bin/intel64
$> cat HPL.dat      # Default (dummy) HPL.dat  input file

# On Slurm cluster, store the output logs into a text file -- see tee
$> srun -n $SLURM_NTASKS ./xhpl | tee test_run.logs
Check the output results with less test_run.logs. You can also quickly see the 10 best results obtained by using:

# ================================================================================
# T/V             N      NB    P     Q               Time             Gflops
# --------------------------------------------------------------------------------
$> grep WR test_run.logs | sort -k 7 -n -r | head -n 10
WR00L2L2          29     3     4     1               0.00             9.9834e-03
WR00L2R2          35     2     4     1               0.00             9.9808e-03
WR00R2R4          35     2     4     1               0.00             9.9512e-03
WR00L2C2          30     2     1     4               0.00             9.9436e-03
WR00R2C2          35     2     4     1               0.00             9.9411e-03
WR00R2R2          35     2     4     1               0.00             9.9349e-03
WR00R2R2          30     2     1     4               0.00             9.8879e-03
WR00R2C4          30     2     1     4               0.00             9.8771e-03
WR00C2R2          35     2     4     1               0.00             9.8323e-03
WR00L2C4          29     3     4     1               0.00             9.8049e-03
Alternatively, you can use the building script scripts/build.HPL to build the HPL sources on both broadwell and skylake nodes (with the corresponding architectures):

# (eventually) release you past interactive job to return on access
$> exit

$> cd ~/tutorials/HPL
# Create symlink to the scripts directory
$> ln -s ref.d/scripts .
# Create a logs/ directory to store the Slurm logs
$> mkdir logs

# Now submit two building jobs targeting both CPU architecture
$> sbatch -C broadwell ./scripts/build.HPL -n broadwell  # Will produce bin/xhpl_broadwell
$> sbatch -C skylake   ./scripts/build.HPL -n skylake    # Will produce bin/xhpl_skylake
Preparing batch runs

We are now going to prepare launcher scripts to permit passive runs (typically in the {default | batch} queue). We will place them in a separate directory (runs/) as it will host the outcomes of the executions on the UL HPC platform .

$> cd ~/tutorials/HPL
$> mkdir -p runs/{broadwell,skylake}/{1N,2N}/{MPI,Hybrid}/    # Prepare the specific run directory
$> cp ref.d/
We are indeed going to run HPL in two different contexts:

Full MPI, with 1 MPI process per (physical) core reserved.
As mentioned in the basics Parallel computations with OpenMP/MPI tutorial, it means that you'll typically reserve the nodes using the -N <#nodes> --ntasks-per-node 28 options for Slurm as there are in general 28 cores per nodes on iris.
Hybrid OpenMP+MPI, with 1 MPI process per CPU socket, and as many OpenMP threads as per (physical) core reserved.
As mentioned in the basics Parallel computations with OpenMP/MPI tutorial, it means that you'll typically reserve the nodes using the -N <#nodes> --ntasks-per-node 2 --ntasks-per-socket 1 -c 14 options for Slurm there are in general 2 processors (each with 14 cores) per nodes on iris
These two contexts will directly affect the values for the HPL parameters P and Q since their product should match the total number of MPI processes.

HPL main parameters

Running HPL depends on a configuration file HPL.dat -- an example is provided in the building directory i.e. src/hpl-2.3/bin/intel64/HPL.dat.

$> cat src/hpl-2.3/bin/intel64/HPL.dat
HPLinpack benchmark input file
Innovative Computing Laboratory, University of Tennessee
HPL.out      output file name (if any)
6            device out (6=stdout,7=stderr,file)
4            # of problems sizes (N)
29 30 34 35  Ns
4            # of NBs
1 2 3 4      NBs
0            PMAP process mapping (0=Row-,1=Column-major)
3            # of process grids (P x Q)
2 1 4        Ps
2 4 1        Qs
16.0         threshold
3            # of panel fact
0 1 2        PFACTs (0=left, 1=Crout, 2=Right)
2            # of recursive stopping criterium
2 4          NBMINs (>= 1)
1            # of panels in recursion
2            NDIVs
3            # of recursive panel fact.
0 1 2        RFACTs (0=left, 1=Crout, 2=Right)
1            # of broadcast
0            BCASTs (0=1rg,1=1rM,2=2rg,3=2rM,4=Lng,5=LnM)
1            # of lookahead depth
0            DEPTHs (>=0)
2            SWAP (0=bin-exch,1=long,2=mix)
64           swapping threshold
0            L1 in (0=transposed,1=no-transposed) form
0            U  in (0=transposed,1=no-transposed) form
1            Equilibration (0=no,1=yes)
8            memory alignment in double (> 0)
See http://www.netlib.org/benchmark/hpl/tuning.html for a description of this file and its parameters (see also the authors tips).

You can use the following sites for finding the appropriate values:

Tweak HPL parameters
HPL Calculator to find good parameters and expected performances
The main parameters to play with for optimizing the HPL runs are:

NB: depends on the CPU architecture, use the recommended blocking sizes (NB in HPL.dat) listed after loading the toolchain/intel module under $EBROOTIMKL/compilers_and_libraries/linux/mkl/benchmarks/mp_linpack/readme.txt, i.e
NB=192 for the broadwell processors available on iris
NB=384 on the skylake processors available on iris
P and Q, knowing that the product  P x Q SHOULD typically be equal to the number of MPI processes.
Of course N the problem size.
An example of P by Q partitioning of a HPL matrix in 6 processes (2x3 decomposition) (Source )



In order to find out the best performance of your system, the largest problem size fitting in memory is what you should aim for. Since HPL performs computation on an N x N array of Double Precision (DP) elements, and that each double precision element requires sizeof(double) = 8 bytes, the memory consumed for a problem size of N is 8N2.

It follows that N can be derived from a simple dimensional analysis based on the involved volatile memory to compute the number of Double Precision :

N≃αTotal Memory Size in bytes𝚜𝚒𝚣𝚎𝚘𝚏(𝚍𝚘𝚞𝚋𝚕𝚎)‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾√=α#Nodes×RAMsize(GiB)×10243𝚜𝚒𝚣𝚎𝚘𝚏(𝚍𝚘𝚞𝚋𝚕𝚎)‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾√
where α is a global ratio normally set to (at least) 80% (best results are typically obtained with α>92%).

Alternatively, one can target a ratio β of the total memory used (for instance 85%), i.e.

N≃β×Total Memory Size in bytes𝚜𝚒𝚣𝚎𝚘𝚏(𝚍𝚘𝚞𝚋𝚕𝚎)‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾√
Note that the two ratios you might consider are of course linked, i.e. β=α2

Finally, the problem size should be ideally set to a multiple of the block size NB.

Example of HPL parameters we are going to try (when using regular nodes on the batch partition) are proposed on the below table. Note that we will use on purpose a relatively low value for the ratio α (or β), and thus N, to ensure relative fast runs within the time of this tutorial.

Architecture	#Node	Mode	MPI proc.	NB	PxQ	α	N
broadwell	1	MPI	28	192	1x28, 2x14, 4x7	0.3	39360
broadwell	2	MPI	56	192	1x56, 2x28, 4x14, 7x8	0.3	55680
broadwell	1	Hybrid	2	192	1x2	0.3	39360
broadwell	2	Hybrid	4	192	1x2	0.3	55680
skylake	1	MPI	28	384	1x28, 2x14, 4x7	0.3	39168
skylake	2	MPI	56	384	1x56, 2x28, 4x14, 7x8	0.3	55680
skylake	1	Hybrid	2	384	1x2	0.3	39168
skylake	2	Hybrid	4	384	1x2	0.3	55680
You can use the script scripts/compute_N to compute the value of N depending on the global ratio α (using -r <alpha>) or β (using -p <beta*100>).

./scripts/compute_N -h
# 1 Broadwell node, alpha = 0.3
./scripts/compute_N -m 128 -NB 192 -r 0.3 -N 1
# 2 Skylake (regular) nodes, alpha = 0.3
./scripts/compute_N -m 128 -NB 384 -r 0.3 -N 2
# 4 bigmem (skylake) nodes, beta = 0.85
./scripts/compute_N -m 3072 -NB 384 -p 85 -N 4
Using the above values, create the appropriate HPL.dat files for each case, under the appropriate directory, i.e. runs/<arch>/<N>N/

Slurm launcher (Intel MPI)

Copy and adapt the default MPI SLURM launcher you should have a copy in ~/git/ULHPC/launcher-scripts/slurm/launcher.default.sh

Copy and adapt the default SLURM launcher you should have a copy in ~/git/ULHPC/launcher-scripts/slurm/launcher.default.sh

$> cd ~/tutorials/HPL/runs
# Prepare a laucnher for intel suit
$> cp ~/git/github.com/ULHPC/launcher-scripts/slurm/launcher.default.sh launcher-HPL.intel.sh
Take your favorite editor (vim, nano, etc.) to modify it according to your needs.

Here is for instance a suggested difference for intel MPI (adapt accordingly):

--- ~/git/ULHPC/launcher-scripts/slurm/launcher.default.sh  2017-06-11 23:40:34.007152000 +0200
+++ launcher-HPL.intel.sh       2017-06-11 23:41:57.597055000 +0200
@@ -10,8 +10,8 @@
 #
 #          Set number of resources
 #
-#SBATCH -N 1
+#SBATCH -N 2
 #SBATCH --ntasks-per-node=28
 ### -c, --cpus-per-task=<ncpus>
 ###     (multithreading) Request that ncpus be allocated per process
 #SBATCH -c 1
@@ -64,15 +64,15 @@
 module load toolchain/intel

 # Directory holding your built applications
-APPDIR="$HOME"
+APPDIR="$HOME/tutorials/HPL/src/hpl-2.3/bin/intel64"
 # The task to be executed i.E. your favorite Java/C/C++/Ruby/Perl/Python/R/whatever program
 # to be invoked in parallel
-TASK="${APPDIR}/app.exe"
+TASK="${APPDIR}/xhpl"

 # The command to run
-CMD="${TASK}"
+# CMD="${TASK}"
 ### General MPI Case:
-# CMD="srun -n $SLURM_NTASKS ${TASK}"
+CMD="srun -n $SLURM_NTASKS ${TASK}"
 ### OpenMPI case if you wish to specialize the MCA parameters
 #CMD="mpirun -np $SLURM_NTASKS --mca btl openib,self,sm ${TASK}"
Now you should create an input HPL.dat file within the runs/<arch>/<N>N/<mode>.

$> cd ~/tutorials/HPL/runs
$> cp ../ref.d/HPL.dat .
$> ll
total 0
-rw-r--r--. 1 svarrette clusterusers 1.5K Jun 12 15:38 HPL.dat
-rwxr-xr-x. 1 svarrette clusterusers 2.7K Jun 12 15:25 launcher-HPL.intel.sh
You are ready for testing a batch job:

$> cd ~/tutorials/HPL/runs
$> sbatch ./launcher-HPL.intel.sh
$> sq     # OR (long version) squeue -u $USER
(bonus) Connect to one of the allocated nodes and run htop (followed by u to select process run under your username, and F5 to enable the tree-view.

Now you can check the output of the HPL runs:

$> grep WR slurm-<jobid>.out    # /!\ ADAPT <jobid> appropriately.
Of course, we made here a small test and optimizing the HPL parameters to get the best performances and efficiency out of a given HPC platform is not easy.

Below are some plots obtained when benchmarking the iris cluster and seeking the best set of parameters across increasing number of nodes (see this blog post)

-- mode: markdown; mode: visual-line; fill-column: 80 --

Author: Valentin Plugaru Valentin.Plugaru@uni.lu Copyright (c) 2015-2017 UL HPC Team hpc-sysadmins@uni.lu

UL HPC MPI Tutorial: High Performance Conjugate Gradients (HPCG) benchmarking on UL HPC platform

By ULHPC Licence GitHub issues Github Documentation Status GitHub forks

The objective of this tutorial is to compile and run one of the newest HPC benchmarks, High Performance Conjugate Gradients (HPCG), on top of the UL HPC platform.

You can work in groups for this training, yet individual work is encouraged to ensure you understand and practice the usage of MPI programs on an HPC platform. If not yet done, you should consider completing the OSU Micro-benchmark and HPL tutorials.

In all cases, ensure you are able to [connect to the UL HPC clusters]((https://hpc-docs.uni.lu/connect/access/).

# /!\ FOR ALL YOUR COMPILING BUSINESS, ENSURE YOU WORK ON A (at least half) COMPUTING NODE
# Have an interactive job
(access)$> si -n 14                                      # iris
(access)$> salloc -p interactive --qos debug -n 14       # iris (long version)
(access)$> oarsub -I -l enclosure=1/nodes=1,walltime=4   # chaos / gaia
Advanced users only: rely on screen (see tutorial or the UL HPC tutorial on the frontend prior to running any oarsub or srun/sbatch command to be more resilient to disconnection.

The latest version of this tutorial is available on Github. Finally, advanced MPI users might be interested to take a look at the Intel Math Kernel Library Link Line Advisor.

Objectives

The High Performance Conjugate Gradient HPCG project is an effort to create a more relevant metric for ranking HPC systems than the High Performance LINPACK (HPL) benchmark, which is currently used in the Top500 ranking.

HPCG exhibits the following patterns: * Dense and sparse computations * Dense and sparse collective operations * Data-driven parallelism (unstructured sparse triangular solves)

For more details, check out: * Toward a New Metric for Ranking High Performance Computing Systems * Technical specification

HPCG is written in C++, with OpenMP and MPI parallelization capabilities, thus requires a C++ compiler with OpenMP support, and/or a MPI library.

The objective of this practical session is to compare the performance obtained by running HPCG compiled with different compilers and options:

HPCG + Intel C++ + Intel MPI
architecture native build, using the most recent supported instruction set (AVX2/FMA3)
SSE4.1 instruction set build
HPCG + GNU C++ + Open MPI
architecture native build, using the most recent supported instruction set (AVX2/FMA3)
SSE4.1 instruction set build
The benchmarking tests should be performed on:

a single node
two nodes, ideally belonging to the same enclosure
two nodes, belonging to different enclosures
Executions on a single node

High Performance Conjugate Gradient (HPCG) with the Intel Suite

We are first going to use the Intel Cluster Toolkit Compiler Edition, which provides Intel C/C++ and Fortran compilers, Intel MPI & Intel MKL.

Resources:

HPCG project
Get the latest release:

$> mkdir ~/TP && cd ~/TP
$> wget http://www.hpcg-benchmark.org/downloads/hpcg-3.0.tar.gz
$> tar xvzf hpcg-3.0.tar.gz
$> cd hpcg-3.0
$> module avail MPI
$> module load toolchain/intel
$> module list
Currently Loaded Modules:
  1) compiler/GCCcore/6.3.0                   4) compiler/ifort/2017.1.132-GCC-6.3.0-2.27                 7) toolchain/iimpi/2017a
  2) tools/binutils/2.27-GCCcore-6.3.0        5) toolchain/iccifort/2017.1.132-GCC-6.3.0-2.27             8) numlib/imkl/2017.1.132-iimpi-2017a
  3) compiler/icc/2017.1.132-GCC-6.3.0-2.27   6) mpi/impi/2017.1.132-iccifort-2017.1.132-GCC-6.3.0-2.27   9) toolchain/intel/2017a
$> module show mpi/impi/2017.1.132-iccifort-2017.1.132-GCC-6.3.0-2.27
Read the INSTALL file.

In particular, you'll have to edit a new makefile Make.intel64 (inspired from setup/Make.MPI_ICPC typically), adapting:

the CXX variable specifying the C++ compiler (use mpiicpc for the MPI Intel C++ wrapper)
the CXXFLAGS variable with architecture-specific compilation flags (see this Intel article)
Once the configuration file is prepared, run the compilation with: $> make arch=intel64

Once compiled, ensure that you are able to run it:

$> cd bin
$> cat hpcg.dat
$> mkdir intel64-optimized
$> mv xhpcg intel64-optimized
$> cd intel64-optimized
$> ln -s ../hpcg.dat .
$> mpirun -hostfile $OAR_NODEFILE ./xhpcg
As configured in the default hpcg.dat, HPCG generates a synthetic discretized three-dimensional partial differential equation model problem with Nx=Ny=Nz=104 local subgrid dimensions. NPx, NPy, NPz are a factoring of the MPI process space, giving a global domain dimension of (Nx * NPx ) * (Ny * NPy ) * (Nz * NPz).

You can tune Nx, Ny, Nz to increase/decrease the problem size, yet take care not to generate a problem whose local node grid representation exceeds computing node memory.

The result of your experiments will be stored in the directory HPCG was started in, in a HPCG-Benchmark-2.4_$(date).yaml file. Check out the benchmark result (GFLOP/s) in the final summary section:

$> grep "HPCG result is" $file.yaml
In addition to the architecture optimized build, re-generate xhpcg to with the compiler options to support only the SSE4.1 instruction set (common across all UL HPC computing nodes) and perform the same experiment, in a new intel64-generic directory.

HPCG with GNU C++ and Open MPI

Re-compile HPCG with GNU C++, adapting the setup file  Make.gcc from Make.Linux_MPI to use the mpicxx wrapper and the GCC specific architecture options.

$> cd ~/TP
$> make clean
$> module purge
$> module load mpi/OpenMPI
$> make arch=gcc
Once compiled, ensure you are able to run it:

$> cd bin
$> cat hpcg.dat
$> mkdir gnu-optimized
$> mv xhpcg gnu-optimized
$> cd gnu-optimized
$> ln -s ../hpcg.dat .
$> mpirun -x PATH -x LD_LIBRARY_PATH -hostfile $OAR_NODEFILE ./xhpcg
Benchmarking on two nodes

Restart the benchmarking campaign (for both the Intel and GCC) in the following context:

2 nodes belonging to the same enclosure. Use for that:

$> oarsub -l enclosure=1/nodes=2,walltime=1 […]
2 nodes belonging to the different enclosures:

$> oarsub -l enclosure=2/core=1,walltime=1 […]
Benchmarking with OpenMP active

Finally, activate OpenMP support when building HPCG, adapting for the Intel and GCC suites Make.MPI_ICPC_OMP and Make.MPI_GCC_OMP respectively. As before, perform single and multiple node benchmarks.

How is the performance result for the OpenMP+MPI vs MPI-only executions?

UL HPC Tutorial: Python basics

  Copyright (c) 2018-2021 UL HPC Team <hpc-sysadmins@uni.lu>


Python is a high-level interpreted language widely used in research. It lets you work quickly and comes with a lot of available packages which give more useful functionalities.

In this tutorial, we are going to explain the steps to run a Python script on the cluster and install a Python package as a user. We will also create a virtual environment and switch from one to the other. We will show how to use different versions of Python on a node. We will speed up the code using packages and by compiling it in C. Finally, we will install an independent Python version using the conda package manager.

Overview

Requirements

Access to the UL HPC clusters.
Basic knowledge of the linux command-line.
Basic programming knowledge.
Basic Python knowledge.
Running passive SLURM jobs using launcher scripts.
Questions

How can I run Python scripts on the cluster?
What Python versions are available on the cluster and how can I use them?
How can I speed up my python code?
How can I install python packages?
How can I manage different versions of Python or packages?
Objectives

Run Python scripts on the cluster.
See the difference between Python versions.
Speed up code using packages.
Speed up code by compiling it in C.
Install Python packages.
Switch between different Python and package versions using a virtual environment.
Create an independent Python installation with conda.
Example: compute standard deviation

The first example used in this tutorial is fully inspired from PythonCExtensions. This code computes the standard deviation of an array of random numbers. The naïve code used to compute the standard deviation of an array (lst) is:

def mean(lst):
    return sum(lst) / len(lst)


def standard_deviation(lst):
    m = mean(lst)
    variance = sum([(value - m) ** 2 for value in lst])
    return math.sqrt(variance / len(lst))
The variable will be the size of the array on which we want to compute the standard deviation. The idea is to reduce the time used to compute this value by using libraries (numpy) or compile the code in C.

Python usage

In this part we will simply run our Python script on the UL HPC platform, on a single node.

Get all the scripts

Clone the UL HPC python tutorial under your home directory on the Iris or Aion cluster. If you have cloned it before, simply run git pull to update it to the latest version.

(laptop)$> ssh aion-cluster
(access)$> git clone https://github.com/ULHPC/tutorials.git
(access)$> cd tutorials/
(access)$> git stash && git pull -r && git stash pop
All the scripts used in this tutorial can be found under tutorials/python/basics.

Execute your first python script on the cluster (Example 1)

First, connect to aion-cluster and go to example 1:

(laptop)$> ssh aion-cluster
(access)$> cd tutorials/python/basics/example1/
To run your script interactively on the cluster, you should do:

(access)>$ si
(node)$> python example1.py
You should see the output of your script directly written in your terminal. It prints the length of the array and the number of seconds it took to compute the standard deviation 10,000 times.

To run your script in a passive way, you should create a batch script to run your python script.

Create a example1.sh file under tutorials/advanced/Python/example1/.
Edit it by using your favorite editor (vim, nano, emacs...)
Add a shebang at the beginning (#!/bin/bash -l)
Add #SBATCH parameters (see Slurm documentation)
1 core
example1 name
maximum 10m walltime
logfile under example1.out
Now run the script using

(access)$> sbatch example1.sh
Now, check that the content of example1.out corresponds to the expected output (in interactive mode).

HINT: You can find the answer under tutorials/python/basics/example1/answer/example1.sh.answer.

Different versions of Python

There are multiple versions of Python installed on the UL HPC clusters.

First, you have the Python provided with the operating system, e.g. the default python. Since you cannot be sure which version this is, you should check it with:

(node)$> python --version
Usually, you have both versions 2.7 and 3 available this way:

(node)$> python2 --version
(node)$> python3 --version
Additionally, we have newer versions of Python available through the modules. To list these versions, you should use this command on a compute node:

(node)$> module avail lang/Python
QUESTIONS:

What are the versions of Python available on Iris cluster? On Aion cluster? To update Iris to the same versions as Aion, you can run resif-load-swset-devel.
Which toolchains have been used to build them?
You can load a specific Python version provided through the modules with module load:

(node)$> module load lang/Python/3.8.6-GCCcore-10.2.0
You can pick any of these Python versions and try to rerun example1.py.

For the rest of the tutorial we will use the Python 3 version from the modules.

IMPORTANT:

Python code is not necessarily compatible between versions 2 and 3.
For many packages recent versions are only available for Python 3.
Make sure to always use the same Python version (and package versions) when running your code or workflow.
Use a package to optimize your code

In this part we will try to use Numpy, a Python package, to optimize our code.

In tutorials/python/basics/example3/example3.py you should see a version of the previous script using Numpy.

Try to execute the script on the Iris or Aion cluster in interactive mode.

(node)$> module purge
(node)$> module load lang/Python/3.8.6-GCCcore-10.2.0
(node)$> python example3.py
QUESTIONS

Why did the execution fail ? What is the problem ?
We need to install the numpy library. We can install it ourselves in our home directory. For that we will use the pip tool.

pip is a package manager for Python. With this tool you can manage Python packages easily: install, uninstall, list, search packages or upgrade them. If you specify the --user parameter, the package will be installed under your home directory and will be available on all the compute nodes. You should also use --no-cache to prevent pip from searching in the cache directory which can be wrongly populated if you deal with several version of Python. Let's install numpy using pip.

(node)$> python -m pip install --no-cache --user numpy==1.18
(node)$> python -m pip show numpy
(node)$> python -m pip install --no-cache --user numpy==1.21
(node)$> python -m pip show numpy
Notice that with pip you can only have one version of numpy installed at a time. In the next section, we will see how to easily switch between several versions of numpy by using vitualenv.

You can now run example3.py code and check its execution time.

QUESTIONS

Which execution is faster between numpy code (example3.py) and naïve code (example1.py)?
Why do you think that numpy is not as powerful as intended? Which parameter can we change to compare the performances?
NOTES

Numpy is also available from the lang/SciPy-bundle modules, tied to different Python versions. Check module list to see which Python version was loaded along the SciPy bundle.
Create virtual environment to switch between several versions of a package

Here comes a very specific case. Sometimes you have to use tools which depends on a specific version of a package. You probably don't want to uninstall and reinstall the package with pip each time you want to use one tool or the other.

Virtualenv allows you to create several environments which will contain their own list of Python packages. The basic usage is to create one virtual environment per project.

In this tutorial we will create a new virtual environment for the previous code in order to install a different version of numpy and check the performances of our code with it.

Create two virtual environments for your project. They will contain two different versions of numpy (1.21 and 1.18). Name thennumpy21 and numpy18, respectively.

(node)$> cd ~/tutorials/python/basics/example3/
(node)$> python3 -m venv numpy21
(node)$> python3 -m venv numpy18
So now you should be able to active any of these environments with this source command. Please notice the (numpy21) present in your prompt that indicates that the numpy21 environment is active. You can use deactivate command to exit the virtual environment.

(node)$> source numpy21/bin/activate
(numpy21)(node)$> # You are now inside numpy21 virtual environment
(numpy21)(node)$> deactivate
(node)$> source numpy18/bin/activate
(numpy18)(node)$> # You are now inside numpy18 virtual environment
QUESTIONS

Using python -m pip freeze, what are the modules available before the activation of your virtual environment?
What are the module available after?
What version of python is used inside the virtual environment ? Where is it located ? (You can use which command.)
To exit a virtual environment run the deactivate command.

So now, we can install a different numpy version inside each of your virtual environments. Check that the version installed corresponds to numpy 1.21 for numpy21 and numpy 1.18 in numpy18.

# Go inside numpy21 environment and install numpy 1.21
(node)$> source numpy21/bin/activate
(numpy21)(node)$> python -m pip install numpy==1.21
(numpy21)(node)$> python -m pip show numpy
(numpy21)(node)$> deactivate

# Go inside numpy18 environment and install numpy 1.18
(node)$> source numpy18/bin/activate
(numpy18)(node)$> python -m pip install numpy==1.18
(numpy18)(node)$> python -m pip show numpy
(numpy18)(node)$> deactivate
Now you can write a batch script to load the right virtualenv and compare the performance of different versions of numpy.

Here are the steps to compare the two versions:

Go to tutorials/python/basics/example3
Create a batch script named numpy_compare.sh
Edit it with your favorite editor (vim, nano, emacs...)
Add a shebang at the beginning (#!/bin/bash -l)
Add #SBATCH parameters
1 core
numpy_compare name
maximum 10m walltime
logfile under numpy_compare.out
Activate numpy18 environment.
Execute numpy_compare.py a first time with this version of numpy.
Deactivate environment
Activate numpy21 environment..
Execute the script a second time with this numpy version.
Check the content of the file numpy_compare.out and identify the two executions.
QUESTIONS

Check the size of numpy21 folder. Why is it so big ? What does it contain ?
Compile your code in C language

C language is known to be very powerful and to execute faster. It has to be compiled (typically using GCC compiler) to be executed. There exist many tools that can convert your Python code to C code to benefit from its performances (Cython, Pythran, ...).

The goal of this part is to adapt our naïve code and use the Pythran tool to convert it to C code. This code will then be imported as a standard Python module and executed.

The code can be found under tutorials/python/basics/example4/example4.py.

Open the example4.py file
Referring to Pythran documentation, add a comment before the standard_deviation function to help pythran to convert your python function into a C one.
Parameter should be a list of float
Function name should be standard_dev
#code to insert in example4.py

#pythran export standard_dev(float list)
def standard_dev(lst):
Create a new virtual environment, activate it and install pythran.
Compile your code using pythran:
(node)$> pythran example4.py -e -o std.cpp # NEVER COMPILE ON ACCESS (only translate)
(node)$> pythran example4.py -o std.so # NEVER COMPILE ON ACCESS (compile)
(node)$> python -c "import std" # this imports the newly generated module with C implementation
Have a look at c_compare.py that contains the code to
import your module
and execute the mean function from this module on a random array
Execute your code on a node and compare the execution time to the other one.
QUESTIONS

What is the fastest execution? Why?
Where can I find the code that has been generated from my Python script?
HINT: If you run pythran example4.py -e -o std.cpp it will generate the C code. Have a look at the *.cpp files in your directory.

Overview graph of runtimes

alt-text

Install your own Python and create reproducible software environments with Conda

In this part we will use the conda package manager to install Python and the required packages.

Conda is an open source package management system and environment management system that runs on Windows, macOS and Linux. Conda quickly installs, runs and updates packages and their dependencies. Conda easily creates, saves, loads and switches between environments on your local computer. It was created for Python programs, but it can package and distribute software for any language.

Conda as a package manager helps you find and install packages. If you need a package that requires a different version of Python, you do not need to switch to a different environment manager, because conda is also an environment manager. With just a few commands, you can set up a totally separate environment to run that different version of Python, while continuing to run your usual version of Python in your normal environment.

— Conda manual

It can encapsulate software and packages in environments, so you can have multiple different versions of a software installed at the same time and avoid incompatibilities between different tools. It also has functionality to easily port and replicate environments, which is important to ensure reproducibility of analyses.

You can think of it as an extension of Python virtualenv to all software, not just Python packages.

Install conda on the cluster

Connect to the cluster and start an interactive job:

(laptop)$> ssh aion-cluster
(access)$> si
Create a backup of your .bashrc configuration file, since the conda installation will modify it:

(node)$> cp ~/.bashrc ~/.bashrc-$(date +%Y%m%d).bak
Install conda:

(node)$> wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
(node)$> chmod u+x Miniconda3-latest-Linux-x86_64.sh
(node)$> ./Miniconda3-latest-Linux-x86_64.sh
You need to specify your installation destination, e.g. /home/users/sdiehl/tools/miniconda3. You must use the full path and cannot use $HOME/tools/miniconda3. Answer yes to initialize Miniconda3.

The installation will modify your .bashrc to make conda directly available after each login. To activate the changes now, run

(node)$> source ~/.bashrc
Setup the environment

1. Update conda to the latest version:

(node)$> conda update conda
2. Create a new empty conda environment and activate it:

(node)$> conda create -n python_tutorial
(node)$> conda activate python_tutorial
After validation of the creation step and once activated, you can see that your prompt will now be prefixed with (python_tutorial) to show which environment is active.

3. Make sure Python does not pick up packages in your home directory:

(python_tutorial)(node)$> export PYTHONNOUSERSITE=True
Not applying this setting can cause erratic and unreproducible behaviour from conda, e.g. it will prefer outdated package versions in your home folder over newer ones in the active environment. If you are a regular (and exclusive) conda user, you might want to add this line to your ~/.bashrc or ~/.bash_profile.

4. Install Python and numpy:

(python_tutorial)(node)$> conda install python numpy
You can also just install Python with conda and then numpy with pip.

Working with conda environments

You can list the packages installed in your current environment with:

(python_tutorial)(node)$> conda list
You can export your current environment to a yaml file with:

(python_tutorial)(node)$> conda env export > environment.yaml
(python_tutorial)(node)$> cat environment.yaml
This file can be shared or uploaded to a repository, to allow other people to recreate the same environment.

It contains three main items:

name of the environment
a list of channels (repositories) from which to install the packages
a list of dependencies, the packages to install and optionally their version
When creating this environment file via export, it will list the packages you installed and also all their dependencies and the dependencies of their dependencies down to the lowest level. However, when manually creating the file, it's sufficient to specify the top-level required packages or tools. All the dependencies will be installed automatically.

For our environment with Python and numpy, the most simple definition - if we do not care about versions - would be:

name: python_tutorial
channels:
  - default
dependencies:
  - python
  - numpy
If you want to install numpy from pip instead, it would look like:

name: python_tutorial
channels:
  - default
dependencies:
  - python
  - pip
  - pip:
    - numpy
For reproducibility, it is advisable to always specify the versions, though.

name: python_tutorial
channels:
  - default
dependencies:
  - python=3.9.7
  - numpy=1.21.2
Let us deactivate the environment, delete it and recreate it from the yaml file. You may use the exported yaml or create a minimal one like shown above and use this one.

(python_tutorial)(node)$> conda deactivate
(base)(node)$> conda remove --name python_tutorial --all
(base)(node)$> conda env create -f environment.yaml
(base)(node)$> conda activate python_tutorial
You can list available conda environments with:

(python_tutorial)(node)$> conda env list
(Optional) Remove conda

If you want to stop conda from always being active:

(access)$> conda init --reverse
Alternatively, you can revert back to the backup of your .bashrc we created earlier. In case you want to get rid of conda completely, you can now also delete the directory where you installed it (default is $HOME/miniconda3).

(Deprecated) Use Scoop to parallelize execution of your Python code with Slurm

In this part, we will use Scoop library to parallelize our Python code and execute it on iris cluster.

WARNING: Scoop uses ssh to spawn workers instead of srun, so no slurm steps are created. This also means that workers will neither see any loaded modules nor the virtual environment, if you use any.

The second example used in this tutorial comes from Scoop example computation of pi. We will use a Monte-Carlo method to compute the value of pi. As written in the Scoop documentation, it spawns two pseudo-random numbers that are fed to the hypot function which calculates the hypotenuse of its parameters. This step computes the Pythagorean equation (\sqrt{x^2+y^2}) of the given parameters to find the distance from the origin (0,0) to the randomly placed point (which X and Y values were generated from the two pseudo-random values). Then, the result is compared to one to evaluate if this point is inside or outside the unit disk. If it is inside (have a distance from the origin lesser than one), a value of one is produced (red dots in the figure), otherwise the value is zero (blue dots in the figure). The experiment is repeated tries number of times with new random values.

alt text

The variable here will be the number of workers (cores on which the script runs) compared to the time of execution.

WARNING: We will need to create a wrapper around SCOOP to manage the loading of modules and virtualenv before calling SCOOP module. It is a tricky part that will need some additional steps to be performed before running your script.

We will first have to install the scoop library using pip:

(access)$> si
(node)$> module load lang/Python/3.8.6-GCCcore-10.2.0
(node)$> python3 -m pip install --no-cache --user filelock
(node)$> python3 -m pip install --no-cache --user scoop
Scoop comes with direct Slurm bindings. If you run your code on a single node, it will try to use the most cores that it can. If you have reserved several nodes, it will use all the nodes of your reservation and distribute work on it.

You can specify the number of cores to use with the -n option in scoop.

We will write a batch script to execute our python script. We want to compare time of execution to the number of workers used in scoop. We want to go from 1 worker (-n 1 for Scoop option) to 55 workers, increasing the worker number 1 by 1. As you can see, our script takes 1 parameter x in input which corresponds to the number of workers.

There will be 1 batch script. It should contain:

1 task per cpu
maximum execution time of 35m
name of the job should be scoop
a variable $NB_WORKERS=$1 for the number of workers to spawn, that takes the first command-line argument of the script as the values (use a value between 1 and 55 (maximal number of cores on 2 nodes on Iris is 56)
will give $NB_WORKERS as an option to the scoop script.
be the only user to use those resources to avoid conflicting with other scoop users (see --exclusive option of sbatch)
(only on Iris) only execute the script on Skylake CPU nodes.
HINT Have a look at tutorials/python/basics/example5/scoop_launcher.sh for the batch script example

Run this script with sbatch command. Check the content of scoop_*.log to see if everything is going well. Also use squeue -u $USER to see the pending array jobs.

When your job is over, you can use make graph command to generate the graph.

QUESTIONS

What is the correlation between number of workers and execution time ?
Use what you learned in the previous part to optimize your code!

UL HPC Tutorial: Use Jupyter notebook on UL HPC

  Copyright (c) 2021  UL HPC Team <hpc-team@uni.lu>
Authors: Clément Parisot (updated by Frédéric Pinel and Sébastien Varrette)



Python is a high-level interpreted language widely used in research. It lets you work quickly and comes with a lot of available packages which give more useful functionalities.

Reserve a node

ssh [aion,iris]-cluster    # connect to the cluster
# Once on the clusters, ask for a interactive job (here for 1h)
si --time=01:00:00
# OR si-gpu --time=01:00:00 if a GPU is needed
Load Python and install required modules in a virtualenv

If you have never used virtualenv before, please have a look at Python1 tutorial.

# load your prefered **3.x** version of Python - 2.7 DEPRECACTED
module load lang/Python/3.8.6-GCCcore-10.2.0   # Load default python
# create virtual environment - distinguish between clusters
python -m venv venv_${ULHPC_CLUSTER}
# Note: You may want to centralize your virtualenvs under '~/venv/<name>_${ULHPC_CLUSTER}'
source venv_${ULHPC_CLUSTER}/bin/activate
Now we need to install all the modules needed. They are listed in the requirements.txt at the root of our tutorial directory. Here is the list of essentials ones:

# jupyter himself
pip install jupyter
# matplotlib to plot the graph inside your notebook
pip install matplotlib
# To use our virtualenv in the notebook, we need to install this module
pip install ipykernel
To save the installed packages:

pip freeze -l > requirements.txt
To install everything:

pip install -r requirements.txt
Now everything is installed properly.

Create your own kernel and launch your Notebook

In order to access to all the modules we have installed inside your Notebook, we will need to create a new Kernel and use it inside Jupyter.

To do so, let's use ipykernel.

 python -m ipykernel install --user --name venv_${ULHPC_CLUSTER}
# The "venv" name here is to give your kernel a name and it will install it in your $HOME path. If a similarly named kernel exists, it will be overwritten.
# In case you would like your kernel to be installed into your active conda environment (<YOURCONDAPATH-PREFIX>/miniconda3/envs/jupyter_env/share/jupyter/kernels/), use the command below. This may be preferred as it encapsulates everything into a single environment, but would deviate from the virtualenv-based configuration above more than necessary for this tutorial.
#python -m ipykernel install --sys-prefix --name 'mylocalkernel'
# A completely custom specification of the path is *discouraged* as the resulting warning about the path not being in the "default" places and might hence not be found is very real. This means that the kernel can not be selected from the "New" dialog in the Jupyter interface. S. a. https://scipy-ipython.readthedocs.io/en/latest/install/kernel_install.html#kernels-for-different-environments for further information.
# Use only if you know what you do!!!
#python -m ipykernel install --prefix=./ --name 'myhyperlocalkernel'
Now everything is installed properly using conda.

Now we will have to start our first notebook. To have access to it from the outside, we will need to run it on the correct IP of the node. This simple command permits to start a new notebook with the correct IP. Please ensure that you are running the command inside the correct directory!

The --no-browser command is used to disable the openning of the browser after the start of the notebook. We use --generate-config at first to generate a default configuration. The default configuration is stored in ~/.jupyter/jupyter_notebook_config.py

To make things easier, we will protect our Notebook with a password. You have just to choose a password after typing the jupyter notebook password command. A hash of your password will be stored in the jupyter config file.

#cd tutorials/python/advanced/jupyter # Only needed if you do not follow from above
jupyter notebook --generate-config
jupyter notebook password
jupyter notebook --ip $(ip addr | egrep '172\.17|21'| grep 'inet ' | awk '{print $2}' | cut -d/ -f1) --no-browser
At the end of the command, you should see a link like this:

[I 17:45:05.756 NotebookApp] Serving notebooks from local directory: /mnt/irisgpfs/users/cparisot/Python2
[I 17:45:05.756 NotebookApp] 0 active kernels
[I 17:45:05.756 NotebookApp] The Jupyter Notebook is running at:
[I 17:45:05.757 NotebookApp] http://172.17.6.55:8888/
[I 17:45:05.757 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
Now you can access with FoxyProxy the notebook.

Run our first notebook

Just click on the Notebook jupyter/Monte-Carlo calculation of pi.ipynb.
Change the kernel for the venv one
Go onto Kernel tab
Choose Change kernel
Select our previously generated kernel called venv
Try to run the code of the notebook in the kernel by using 'Alt-Enter' keys.

-- mode: markdown;mode:visual-line; fill-column: 80 --

Authors: Clément Parisot

Copyright (c) 2018 UL HPC Team -- see https://hpc.uni.lu

UL HPC Tutorial: [Advanced] Python : Use Jupyter notebook on UL HPC

By ULHPC Licence GitHub issues  Github Documentation Status GitHub forks



Python is a high-level interpreted language widely used in research. It lets you work quickly and comes with a lot of available packages which give more useful functionalities.

Use Celery on Iris

Choose a broker

Redis broker

We need to run our own instance of Redis server on UL HPC on a node. We will download the executable from redis.io website and execute it locally on a node.

Reserve a node interactively and do the following:

wget http://download.redis.io/releases/redis-4.0.9.tar.gz
tar xzf redis-4.0.9.tar.gz
cd redis-4.0.9
make
Let's create a configuration file for redis-server with the following options:

port where the server is listening (default one): 6379
ip address of the server: we will listen on the main ethernet interface of the node. You can retrieve the IP address with this command hostname -i or by checking the output of ip addr show dev eno1.
we will protect the access to the node with a password to ensure that other experiments doesn't interact with us.
Which gives us the following config file:

cat > redis2.conf << END
protected-mode yes
port 6379
requirepass yourverysecurepassword
END
Now you should be able to run the Redis server on a node with this configuration.

The resources are by default shared with other users. You can't run a redis instance on the same resource (same IP) with the same port number. To avoid collision with other users, you should either reserve a full node to be sure to be the only one running a Redis instance with this IP or if you want to share the IP of your node with somebody else, make sure to use a different port number.

We will run our redis server on a different port number for each run by using this bash command: $(($SLURM_JOB_ID % 1000 + 64000)). It will give us a port number between 64000 and 64999 based on the last 3 digits of our job ID.

si -J redis-server
./src/redis-server $HOME/celery/redis/redis.conf --port $(($SLURM_JOB_ID % 1000 + 64000)) --bind $(hostname -i)
You should have the following output:

7625:C 24 May 14:06:06.280 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo
7625:C 24 May 14:06:06.280 # Redis version=4.0.9, bits=64, commit=00000000, modified=0, pid=7625, just started
7625:C 24 May 14:06:06.280 # Configuration loaded
7625:M 24 May 14:06:06.281 * Increased maximum number of open files to 10032 (it was originally set to 1024).
                _._
           _.-``__ ''-._
      _.-``    `.  `_.  ''-._           Redis 4.0.9 (00000000/0) 64 bit
  .-`` .-```.  ```\/    _.,_ ''-._
 (    '      ,       .-`  | `,    )     Running in standalone mode
 |`-._`-...-` __...-.``-._|'` _.-'|     Port: 64856
 |    `-._   `._    /     _.-'    |     PID: 7625
  `-._    `-._  `-./  _.-'    _.-'
 |`-._`-._    `-.__.-'    _.-'_.-'|
 |    `-._`-._        _.-'_.-'    |           http://redis.io
  `-._    `-._`-.__.-'_.-'    _.-'
 |`-._`-._    `-.__.-'    _.-'_.-'|
 |    `-._`-._        _.-'_.-'    |
  `-._    `-._`-.__.-'_.-'    _.-'
      `-._    `-.__.-'    _.-'
          `-._        _.-'
              `-.__.-'

7625:M 24 May 14:06:06.283 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.
7625:M 24 May 14:06:06.283 # Server initialized
7625:M 24 May 14:06:06.283 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.
7625:M 24 May 14:06:06.283 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.
7625:M 24 May 14:06:06.283 * DB loaded from disk: 0.000 seconds
7625:M 24 May 14:06:06.283 * Ready to accept connections
Now you should be able to connect to your redis server from the other nodes and from the access. You can test it simply with telnet from access.iris. Open a new connection to iris-cluster and type the following command:

telnet iris-001 64856 # Please replace iris-001 by your node name and 64856 by your port number
AUTH myverysecurepassword
PING
+PONG
^]
telnet> quit
Connection closed.
To exit telnet strike Ctrl-] keys.

Celery configuration

All information comes from the official documentation of celery

Installation

Reserve a node interactively on iris
Create a virtual environment
Install celery[redis] inside the virtualenv using pip installer
si
cd celery
module load lang/Python/3.7  # actual version depends on the selected software set
virtualenv venv
source venv/bin/activate
pip install "celery[redis]"
pip install redis
# (optional) Flower is a frontend for visualization of the queues status
pip install flower
Configuration

We need to give to celery 3 informations about our Redis: * password of the database * hostname of the node on which the server is running * port the port number of the database

As those parameters will change on each run, we will put the 3 value inside a configuration file and import it in the python code to create the broker address which will looks like this:

redis_broker = "redis://:{password}@{hostname}:{port}/0".format(**params)
In file celery.ini, fill the redis section like this:

[redis]
broker_password=<ỳourverysecurepassword>
broker_hostname=<hostname of the redis server that can be find with `squeue -u $USER`>
broker_port=<port that you have defined in the configuration (by default 64867)
We have created a list of tasks to execute in ulhpccelery/tasks.py. There are 3 tasks:

add(x, y) add x and y number
mul(x, y) multiplie x and y
xsum(numbers) return the sum of an array of numbers
We will start a worker on a full node that will run the code on the 28 cores of iris. For that, reserve a full node and 28 cores, load the virtual environment and run celery.

si -N1 -c28
cd celery
module load lang/Python/3.6.0
source venv/bin/activate
celery -A ulhpccelery worker
You should see the working starting on the 28 cores and connect to the redis instance successfully. If you have issue connecting to the redis instance, check that it is still running and that you have access to it from the node (via telnet command for example).

Launch several tasks

From the ulhpccelery module, simply reserve a node and execute the following commands.

si -N1 -c1
cd celery
module load lang/Python/3.6.0
source venv/bin/activate
python
>> from ulhpccelery import tasks
>> res = []
>> for i in range(10**6):
>>     res.append(tasks.add.delay(i, i+1))
>> for r in res:
>>     print(r.get())
You should see the results of the additions. The tasks have been distributed to all the available cores.

Monitor the experiment

You can use Flower to monitor the usage of the queues.

si -N1 -c28
cd celery
module load lang/Python/3.6.0
virtualenv venv
source venv/bin/activate
celery -A ulhpccelery flower --address="$(hostname -i)"
Now, directly access to the web interface of the node (after a tunnel redirection): http://172.17.6.55:5555/

You should see this kind of output:

Flower interface

To go further

Try to add / suppress workers during the execution
Try to stop restart redis server

Parallel machine learning with scikit-learn





scikit-learn is a python library dedicated to machine learning. This library allows you to tackle:

Preprocessing
Dimensionality reduction
Clustering
Classification
Regression
etc ...
In this tutorial, we are going to show how to perform parallel machine learning computations on a High Performance Computing platform such as the Iris cluster.

Dependencies

In this tutorial, we are going to code in python 3 and use the following libraries:

numpy
scikit-learn
ipyparallel
joblib
Creating the virtual environment

Be sure to start with a bare environment:

No interactive job running and thus no loaded modules
No python virtualenv already loaded
# Clone tutorial repository
git clone https://github.com/ULHPC/tutorials.git
# cd into the scripts folder
cd tutorials/python/advanced/scikit-learn/scripts
# First ask for an interactive SLURM job
si
# Load python 3.6 module
module load lang/Python 
# Create your virtual environment
python3 -m venv scikit_${ULHPC_CLUSTER}
# Activate your env
source ./scikit_${ULHPC_CLUSTER}/bin/activate
# Upgrade pip
pip install --upgrade pip
# Now install required packages
# jupyter himself
pip install ipython
# matplotlib to plot the graph inside your notebook
pip install matplotlib
# ipyparallel for parallel execution of your code on several thread and/or nodes
pip install ipyparallel
# joblib is used to start parrallel scikit-learn jobs
pip install joblib
# scikit-learn 
pip install scikit-learn
# pandas
pip install pandas
# Exit interactive job (setup completed)
exit
Using ipyparrallel with SLURM (generic slurm script)

Hereafter, a general script for using ipyparrallel with the SLURM scheduler is provided. We are going to use it in the remaining part of this tutorial. This is the file launcher.sh that you can find in the scripts directory.

Remark The launcher below requests 10 tasks on 2 nodes with 1 cpu per task. This is NOT an efficient use of the hardware but only for educational purpose. Please always try to maximize nodes usage, i.e., 28 tasks max on iris, 128 max on aion or decrease and increase multithreading if possible. You may use  --ntasks-per-nodesor --ntasks-per-socket for this purpose. Please also refer to the ULHPC documentation for more details.


#!/bin/bash -l

#BATCH -p batch           #batch partition 
#SBATCH -J ipy_engines      #job name
#SBATCH -N 2                # 2 node, you can increase it
#SBATCH -n 10                # 10 task, you can increase it
#SBATCH -c 1                # 1 cpu per task
#SBATCH -t 1:00:00         # Job is killed after 1h

module load lang/Python 

source scikit_${ULHPC_CLUSTER}/bin/activate

#create a new ipython profile appended with the job id number
profile=job_${SLURM_JOB_ID}

echo "Creating profile_${profile}"
ipython profile create ${profile}

# Number of tasks - 1 controller task - 1 python task 
export NB_WORKERS=$((${SLURM_NTASKS}-2))

LOG_DIR="$(pwd)/logs/job_${SLURM_JOBID}"
mkdir -p ${LOG_DIR}

#srun: runs ipcontroller -- forces to start on first node 
srun -w $(hostname) --output=${LOG_DIR}/ipcontroller-%j-workers.out  --exclusive -N 1 -n 1 -c ${SLURM_CPUS_PER_TASK} ipcontroller --ip="*" --profile=${profile} &
sleep 10

#srun: runs ipengine on each available core -- controller location first node
srun --output=${LOG_DIR}/ipengine-%j-workers.out --exclusive -n ${NB_WORKERS} -c ${SLURM_CPUS_PER_TASK} ipengine --profile=${profile} --location=$(hostname) &
sleep 25

#srun: starts job
echo "Launching job for script $1"
srun --output=${LOG_DIR}/code-%j-execution.out  --exclusive -N 1 -n 1 -c ${SLURM_CPUS_PER_TASK} python $1 -p ${profile} 

--ip=* instructs ZeroMQ to listen on all interfaces, but it does not contain the IP needed for engines / clients to know where the controller is. This can be specified with the --location argument, such as --location=10.0.0.1, or --location=server.local, the specific IP address or hostname of the controller, as seen from engines and/or clients. IPython uses socket.gethostname() for this value by default, but it may not always be the right value. Check the location field in your connection files if you are having connection trouble.

Now, we are going to show how to apply ipyparallel with machine learning algorithms implemented in scikit-learn. First, we will cluster some random generated data in parrallel and then we use parallel hyperparameter optimisation to find the best parameters for a SVM classification model.

Unsupervised learning: clustering a dataset

Given a dataset in which we do not known apriori how many clusters exist, we are going to perform multiple and parallel clustering in order to find the right number of clusters.

Some existing approaches (DBSCAN, OPTICS) are now able to detect this number automatically but it is required to have some prior knowlege on the density of the clusters.

Hereafter, we are going to use the very simple K-means clustering algorithm. We will start multiple K-means instances in parrallel with different number of clusters to be detected.

In the unsupervized folder, you can find two scripts:

some_funcs.py wrapping the Kmeans procedure of the scikit-learn library
main.py which is the main script calling our wrapper
some_funcs.py: add some logs to kmeans procedure

import os
import datetime
# Library to generate plots
import matplotlib as mpl
# Define Agg as Backend for matplotlib when no X server is running
mpl.use('Agg')
import matplotlib.pyplot as plt
# Importing scikit-learn functions
from sklearn.cluster import  KMeans
from sklearn.metrics.pairwise import pairwise_distances_argmin
from matplotlib.cm import rainbow
# Import the famous numpy library
import numpy as np
# We import socket to have access to the function gethostname()
import socket
import time

# alias to the now function
now = datetime.datetime.now

# To know the location of the python script
FILE_DIR = os.path.dirname(os.path.abspath(__file__))

# We decorate (wrap) the kmeans function
# in order to add some pre and post-processing
def kmeans(nbClusters,X,profile):
    # We create a log for the clustering task
    file_path = os.path.join(os.getcwd(),
                             '{0}_C{1:06}'.format(profile,nbClusters))
    #logging will not work from the HPC engines
    #need to write into a file manualy.
    with open(file_path+".log", 'a+') as f:
        f.write('job started on {0}\n'.format(socket.gethostname()))
        f.write('new task for nbClusters='+str(nbClusters)+'\n')

    t0 = now()
    with open(file_path+".log", 'a+') as f:
        f.write('Start clustering at {0}\n'.format(t0.isoformat()))

    # Original scikit-learn kmeans 
    k_means = KMeans(init='k-means++', n_clusters=nbClusters, n_init=100)
    k_means.fit(X)

    # After clustering has been performed, we record information to 
    # the log file

    t1 = now()
    h = (t1-t0).total_seconds()//3600
    m = (t1-t0).total_seconds()//60 - h*60
    s = (t1-t0).total_seconds() -m*60 - h*60
    with open(file_path+".log", 'a+') as f:
        f.write('Finished at {0} after '
                '{1}h {2}min {3:0.2f}s\n'.format(t1.isoformat(),h,m,s))
        f.write('kmeans\n')
        f.write('nbClusters: {0}\n'.format(str(nbClusters)))

    # We sort the centers
    k_means_cluster_centers = np.sort(k_means.cluster_centers_, axis=0)
    # We assign the labels
    k_means_labels = pairwise_distances_argmin(X, k_means_cluster_centers)

    # The previous part is useful in order to keep the same color for
    # the different clustering

    t_batch = (t1 - t0).total_seconds()

    # We generate a plot in 2D
    colors = rainbow(np.linspace(0, 1, nbClusters))
    fig=plt.figure()
    ax = fig.add_subplot(1, 1, 1)
    for k, col in zip(range(nbClusters), colors):
        my_members = k_means_labels == k
        cluster_center = k_means_cluster_centers[k]
        ax.plot(X[my_members, 0], X[my_members, 1], 'w',
            markerfacecolor=col, marker='.')
        ax.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,
            markeredgecolor='k', markersize=6)
        ax.set_title('KMeans')
        ax.set_xticks(())
        ax.set_yticks(())
    plt.text(-3.5, 1.8,  'clustering time: %.2fs\ninertia: %f' % (t_batch, k_means.inertia_))
    # We save the figure in png
    plt.savefig(file_path+".png")
    return (nbClusters,k_means.inertia_)


main.py: our main python script to parallelize clustering


import argparse
import logging
import os
import sys
from sklearn.datasets import make_blobs
from joblib import Parallel, parallel_backend
from joblib import register_parallel_backend
from joblib import delayed
from joblib import cpu_count
from ipyparallel import Client
from ipyparallel.joblib import IPythonParallelBackend
import numpy as np
import datetime
#module in the same directory
from some_funcs import kmeans

FILE_DIR = os.path.dirname(os.path.abspath(__file__))
sys.path.append(FILE_DIR)

#prepare the logger
parser = argparse.ArgumentParser()
parser.add_argument("-p", "--profile", default="ipy_profile",
                 help="Name of IPython profile to use")
args = parser.parse_args()
profile = args.profile
logging.basicConfig(filename=os.path.join(FILE_DIR,profile+'.log'),
                    filemode='w',
                    level=logging.DEBUG)
logging.info("number of CPUs found: {0}".format(cpu_count()))
logging.info("args.profile: {0}".format(profile))

#prepare the engines
c = Client(profile=profile)
NB_WORKERS = int(os.environ.get("NB_WORKERS",1))
# wait for the engines
c.wait_for_engines(NB_WORKERS)

#The following command will make sure that each engine is running in
# the right working directory to access the custom function(s).
c[:].map(os.chdir, [FILE_DIR]*len(c))
logging.info("c.ids :{0}".format(str(c.ids)))
bview = c.load_balanced_view()
register_parallel_backend('ipyparallel',
                          lambda : IPythonParallelBackend(view=bview))

#Create data
#prepare it for the custom function
X,_ = make_blobs(n_samples=5000,centers=np.random.randint(20))
#some parameters to test in parallel
param_space = {
    'NCLUSTERS': np.arange(2,20)
}


with parallel_backend('ipyparallel'):
    inertia = Parallel(n_jobs=len(c))(delayed(kmeans)(nbClusters,X,profile)
                               for nbClusters in param_space['NCLUSTERS'])


#write down the number of clusters and the total inertia in a file.
with open(os.path.join(FILE_DIR,'scores_kmeans.csv'), 'w') as f:
    f.write('nbClusters,inertia,\n')
    f.write("\n".join(','.join(str(c) for c in l) for l in inertia))
    f.write('\n')
c.shutdown()
Start parallel clustering

You only need to start the following command from the scripts directory: sbatch launcher.sh unsupervized/main.py

After job completion, use scp or rsync to retrieve your results on your laptop.



Supervised learning: SVM classification

This part is strongly based on the following tutorial. The mainstream is to apply parallel hyperoptimisation in order to find the optimal parameters of a SVC model. This part can be applied on many Machine Learning model and Metaheuristics algorithms that require generally many parameters.

Model parameters vs Hyperparameters

Model parameters are the intrinsic properties of the training data. Weights, biases are typically model parameters

Hyperparameters can be considered as meta-variables. They are respnsible for the training process and are condigured before training.



Hyperparameters tuning can be perfomed in scikit-learn using 4 differents approaches:

By defining a pre-defined set of hyperparameters to evaluate
By applying Grid-search
By applying Random search
Recognize hand-written digits

For this supervised learning example, we will train a SVM classification model to recognize images of hand-written digits. The SVM classifcation model will be C-Support Vector Classification based on the libsvm library. In order to discover the penalty hyperparameter C of the error term, we will rely on the Grid search approach implemented in scikit-learn.

The training data will be loaded from scikit-learn digits library.

source: https://towardsdatascience.com/understanding-hyperparameters-and-its-optimisation-techniques-f0debba07568

The SLURM launcher script remains the same than before. It has been especially designed to be as general as possible. We only need to write a script calling the Grid search procedure with the SVC model. Here we will not wrap the existing SVC algorithm. The script is located in the supervized folder.

main.py: using Grid search in parallel

import argparse
import logging
import os
import sys
import matplotlib as mpl
mpl.use('Agg')
import matplotlib.pyplot as plt
from matplotlib.colors import Normalize
from joblib import Parallel, parallel_backend
from joblib import register_parallel_backend
from joblib import delayed
from joblib import cpu_count
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from ipyparallel import Client
from ipyparallel.joblib import IPythonParallelBackend
import numpy as np
import pandas as pd
import datetime
from sklearn.model_selection import GridSearchCV

FILE_DIR = os.path.dirname(os.path.abspath(__file__))
sys.path.append(FILE_DIR)

#prepare the logger
parser = argparse.ArgumentParser()
parser.add_argument("-p", "--profile", default="ipy_profile",
                 help="Name of IPython profile to use")
args = parser.parse_args()
profile = args.profile
logging.basicConfig(filename=os.path.join(FILE_DIR,profile+'.log'),
                    filemode='w',
                    level=logging.DEBUG)
logging.info("number of CPUs found: {0}".format(cpu_count()))
logging.info("args.profile: {0}".format(profile))

#prepare the engines
c = Client(profile=profile)
NB_WORKERS = int(os.environ.get("NB_WORKERS",1))
# wait for the engines
c.wait_for_engines(NB_WORKERS)

#The following command will make sure that each engine is running in
# the right working directory to access the custom function(s).
c[:].map(os.chdir, [FILE_DIR]*len(c))
logging.info("c.ids :{0}".format(str(c.ids)))
bview = c.load_balanced_view()
register_parallel_backend('ipyparallel',
                          lambda : IPythonParallelBackend(view=bview))

#Get data
digits = load_digits()
#prepare it for the custom function
#it would be better to use cross-validation
#outside the scope of this tutorial
X_train, X_test, y_train, y_test = train_test_split(digits.data,
                                                    digits.target,
                                                    test_size=0.3)
#some parameters to test in parallel
param_space = {
    'C': np.logspace(-6, 6, 20),
    'gamma': np.logspace(-6,1,20)
}


svc_rbf = SVC(kernel='rbf',
              shrinking=False)

search = GridSearchCV(svc_rbf,
                      param_space,
                      return_train_score=True,
                      n_jobs=len(c))

with parallel_backend('ipyparallel'):
    search.fit(X_train, y_train)
results = search.cv_results_
results = pd.DataFrame(results)
results.to_csv(os.path.join(FILE_DIR,'scores_rbf_digits.csv'))


scores = search.cv_results_['mean_test_score'].reshape(len(param_space['C']),len(param_space['gamma']))

plt.figure()
#plt.subplots_adjust(left=.2, right=0.95, bottom=0.15, top=0.95)
plt.imshow(scores, interpolation='nearest', cmap=plt.cm.hot)
plt.xlabel('gamma')
plt.ylabel('C')
plt.colorbar()
plt.xticks(np.arange(len(param_space['gamma'])), map(lambda x : "%.2E"%(x),param_space['gamma']), fontsize=8, rotation=45)
plt.yticks(np.arange(len(param_space['C'])), map(lambda x : "%.2E"%(x),param_space['C']), fontsize=8, rotation=45)
plt.title('Validation accuracy')
plt.savefig(os.path.join(FILE_DIR,"validation.png"))
c.shutdown()
Start parallel supervized learning

In the scripts folder, enter the following command sbatch launcher.sh supervized/main.py.

After job completion, use scp or rsync to retrieve your results on your laptop.

Scores heatmap

Next

(Part 1.) Adapt the script for another clustering algorithm
(Part 2.) Increase the number of parameters to be searched by the GridSearchCV approach

Parallel evolutionary computing with Scoop/Deap





Description

Evolutionary computing is a class of global optimisation algorithms designed to tackle complex optimisation problems (e.g. combinatorial, non-linear, non-convex). One can find multiple python libraries such:

Deap
Pygmo
Pyvolution
pySTEP
Pyevolve
PyRobot
Inspyred
Deap through scoop distributed worflow

In this tutorial, we focus on the Deap library that is highly configurable and can be easily tuned. One of the main advantage of Deap is its capacities to rely on the Scoop library to distribute algorithms.

The Scalable COncurrent Operations in Python, aka Scoop, is a distributed task module allowing concurrent parallel programming on various environments, from heterogeneous grids to supercomputers. Its documentation is available on http://scoop.readthedocs.org/.

Scoop can be used on HPC platform but still requires some tricks to cross nodes.

Objectives

See how to use Deap and Scoop in concert to distribute evolutionary computations
See how to prepare a general launcher in order to use Scoop
Deap

For this tutorial, we are going to find the global minimum of the rastrigin function (see below for 2 variables x and y). This function is used generally as benchmark to test evolutionary algorithms. The global optima f(x)=0 with x=[0....0].


To optimise this continuous function, we are going to rely on the Covariance Matrix Adaptation - Evolutionary Strategy (CMA-ES). This algorithm is based on the maximum likelihood principle and adjusts the mean and covariance of the solution distribution in order to maximize the likelihood of finding promising candidates.


Setup

We are going to setup a python virtual environment in order to install all required python libraries. Please create a separate folder (ex. scoop-deap) and cd into it. Apply the following commands to setup your environment.

Be sure to start with a bare environment:

No interactive job running and thus no loaded modules
No python virtualenv already loaded
# Clone tutorial repository
git clone https://github.com/ULHPC/tutorials.git
# cd into the scripts folder
cd tutorials/python/advanced/scoop-deap/scripts
# Ask an interactive job
si
# Load python3 module (load by default Python3)
module load lang/Python
python -m venv scoop_env_${ULHPC_CLUSTER}
source scoop_env_${ULHPC_CLUSTER}/bin/activate
pip install --upgrade pip
pip install numpy deap scoop matplotlib
The CMA-ES optimisation script

The code of the following python script can be found in the file evolution.py located in the scripts folder. Actually, your current folder if you did not cd into another one.

import sys
import numpy
import random
import timeit
import json
import collections
import os
# Library to generate plots                                                                         
import matplotlib as mpl                                                                            
# Define Agg as Backend for matplotlib when no X server is running                                  
mpl.use('Agg')                                                                                      
import matplotlib.pyplot as plt 
from deap.algorithms import *
from deap import base
from deap import creator
from deap import tools
from deap import benchmarks
from deap import algorithms
from deap import cma

# Create new type dynalically
creator.create("FitnessMin", base.Fitness, weights=(-1.0,))
creator.create("Individual", list, fitness=creator.FitnessMin)

# Create a toolbox and overload existing functions
toolbox = base.Toolbox()
toolbox.register("evaluate", benchmarks.rastrigin)

def tree():
    ''' 
        Recursive dictionnary with defaultdict 
    '''
    return collections.defaultdict(tree)

def main(N,out_sol_dict):
    '''
        Procedure setting up all the necessary parameters and components for 
        CMAES evolution

        Parameters:
        -----------
        N: Dimension of the problem (number of variables)
        out_sol_dict: Dictionnary to store the results

    '''
    # CMAES strategy
    strategy = cma.Strategy(centroid=[5.0]*N, sigma=5.0, lambda_=20*N)
    # Register the generation and update procedure for the algorithm workflow
    toolbox.register("generate", strategy.generate, creator.Individual)
    toolbox.register("update", strategy.update)

    # Create a set containing the best individual recorded
    hof = tools.HallOfFame(1)
    # Create a statistical object and tell it what you want to monitor
    stats = tools.Statistics(lambda ind: ind.fitness.values)
    stats.register("avg", numpy.mean)
    stats.register("std", numpy.std)
    stats.register("min", numpy.min)
    stats.register("max", numpy.max)

    # Start the generation and update the population of solutions:w
    _,logbook=algorithms.eaGenerateUpdate(toolbox, ngen=250, stats=stats, halloffame=hof)
    # Get best solution and save it
    best_sol=tools.selBest(hof,1)[0]
    out_sol_dict["solution"]=list(best_sol)
    out_sol_dict["fit"]=float(best_sol.fitness.values[0])
    # Plot convergence
    gen, avg = logbook.select("gen", "avg")
    plt.figure()
    plt.title("Convergence curve")
    plt.xlabel("Generations")
    plt.ylabel("Best obtained Fitness value at gen N")
    plt.grid(True)
    plt.plot(gen,avg,"r--")
    plt.savefig("conv.pdf",dpi=600)

if __name__ == "__main__":
    # Check number of parameters
    assert len(sys.argv)==2, "Please enter the dimension of the problem"
    solutions=tree()
    # Evaluate the running time
    t=timeit.timeit("main({0},solutions)".format(sys.argv[1]),setup="from __main__ import main,solutions",number=1)
    solutions['time']=t
    solutions['cores']=int(os.environ["SLURM_NTASKS"])
    solutions['dimensions']=int(sys.argv[1])
    # Save to json file
    with open('solutions_c{0}_n{1}.json'.format(sys.argv[1],os.environ["SLURM_NTASKS"]), 'w') as json_file:
        json.dump(solutions, json_file,indent=True)

Since you are still in the interactive job, start an interactive optimisation using the following command python evolution.py [size] with [size] the number of variables to be considered. For example, you can run python evolution.py 10. While the optimisation of the rastrigin function is ongoing, you can see the evolution log for every evolutionary generations displayed on your terminal.

Note that if your interactive job ended, please start a new one and source again the python virtual environment.

Distributed evolution with scoop

If you increase [size], you will notice how it can be time-consuming to optimise the rastrigin function. This is mainly due to the dimensionality curse that forces population-based algorithm to consider much more candidates.

To cope with this issue, we can evaluate candidates in a distributed manner. To do this, you need to overload the map function of the algorithm using the toolbox class provided by Deap and replace the default map function with futures.map from the scoop library.

Modify the evolution.py script to include the scoop library and overload the map function using the Deap documentation. Please try yourself before looking at the solution below.

Solution:

import sys
import numpy
import random
import timeit
import json
import collections
import os
# Library to generate plots                                                                         
import matplotlib as mpl                                                                            
# Define Agg as Backend for matplotlib when no X server is running                                  
mpl.use('Agg')                                                                                      
import matplotlib.pyplot as plt 
from deap.algorithms import *
from deap import base
from deap import creator
from deap import tools
from deap import benchmarks
from deap import algorithms
from deap import cma
from scoop import futures # <-------------------- import futures module from scoop

# Create new type dynalically
creator.create("FitnessMin", base.Fitness, weights=(-1.0,))
creator.create("Individual", list, fitness=creator.FitnessMin)

# Create a toolbox and overload existing functions
toolbox = base.Toolbox()
toolbox.register("evaluate", benchmarks.rastrigin)
toolbox.register("map",futures.map) # <--------------- overload the map function

def tree():
    ''' 
        Recursive dictionnary with defaultdict 
    '''
    return collections.defaultdict(tree)

def main(N,out_sol_dict):
    '''
        Procedure setting up all the necessary parameters and components for 
        CMAES evolution

        Parameters:
        -----------
        N: Dimension of the problem (number of variables)
        out_sol_dict: Dictionnary to store the results

    '''
    # CMAES strategy
    strategy = cma.Strategy(centroid=[5.0]*N, sigma=5.0, lambda_=20*N)
    # Register the generation and update procedure for the algorithm workflow
    toolbox.register("generate", strategy.generate, creator.Individual)
    toolbox.register("update", strategy.update)

    # Create a set containing the best individual recorded
    hof = tools.HallOfFame(1)
    # Create a statistical object and tell it what you want to monitor
    stats = tools.Statistics(lambda ind: ind.fitness.values)
    stats.register("avg", numpy.mean)
    stats.register("std", numpy.std)
    stats.register("min", numpy.min)
    stats.register("max", numpy.max)

    # Start the generation and update the population of solutions:w
    _,logbook=algorithms.eaGenerateUpdate(toolbox, ngen=250, stats=stats, halloffame=hof)
    # Get best solution and save it
    best_sol=tools.selBest(hof,1)[0]
    out_sol_dict["solution"]=list(best_sol)
    out_sol_dict["fit"]=float(best_sol.fitness.values[0])
    # Plot convergence
    gen, avg = logbook.select("gen", "avg")
    plt.figure()
    plt.title("Convergence curve")
    plt.xlabel("Generations")
    plt.ylabel("Best obtained Fitness value at gen N")
    plt.grid(True)
    plt.plot(gen,avg,"r--")
    plt.savefig("conv.pdf",dpi=600)

if __name__ == "__main__":
    # Check number of parameters
    assert len(sys.argv)==2, "Please enter the dimension of the problem"
    solutions=tree()
    # Evaluate the running time
    t=timeit.timeit("main({0},solutions)".format(sys.argv[1]),setup="from __main__ import main,solutions",number=1)
    solutions['time']=t
    solutions['cores']=int(os.environ["SLURM_NTASKS"])
    solutions['dimensions']=int(sys.argv[1])
    # Save to json file
    with open('solutions_c{0}_n{1}.json'.format(sys.argv[1],os.environ["SLURM_NTASKS"]), 'w') as json_file:
        json.dump(solutions, json_file,indent=True)

Starting scoop-deap with Slurm

The following script may look complex but remains very general. Feel free to use it for any other python project where you need distributed computations with scoop. Starting distributed computation with Slurm requires a small trick (Thx to V. Plugaru for it).

First, we need to pass to scoop the different allocated host machines. Here, we save the hostname of each machines using scontrol show hostanmes > $HOSTFILE into the file hostfile. This allows scoop to find and use all allocated ressources. Last but not least, we need to decorate or wrap the python interpreter used by scoop to spawn workers. Indeed, scoop connects with ssh to the nodes and start the worker in place without loading module (using system python) and sourcing our virtual environment. To solve this issue, we need to replace the default python interpreter by a custom script that load the required python module but also source test_env before calling python. This permits to call the python loaded module and not the system one. Thus, it is important to start scoop as follows (see also last line):

python -m scoop --hostfile $HOSTFILE -n ${SLURM_NTASKS} --python-interpreter=$SCOOP_WRAPPER $INPUTFILE $@

#!/bin/bash -l


# Ensure process affinity is disabled
export SLURM_CPU_BIND=none

# Prepare in the current folder a worker launcher for Scoop 
# The scipt below will 'decorate' the python interpreter command
# Before python is called, modules are loaded
HOSTFILE=$(pwd)/hostfile
SCOOP_WRAPPER=$(pwd)/scoop-python.sh

cat << EOF > $SCOOP_WRAPPER
#!/bin/bash -l
module load lang/Python
export SLURM_NTASKS=${SLURM_NTASKS}
source $(pwd)/scoop_env_${ULHPC_CLUSTER}/bin/activate
EOF
echo 'python $@' >> $SCOOP_WRAPPER

chmod +x $SCOOP_WRAPPER

# Classical "module load" in the main script
module load lang/Python
source $(pwd)/scoop_env_${ULHPC_CLUSTER}/bin/activate

# Save the hostname of the allocated nodes
scontrol show hostnames > $HOSTFILE

# Start scoop with python input script
INPUTFILE=$(pwd)/evolution.py 
python -m scoop --hostfile $HOSTFILE -n ${SLURM_NTASKS} --python-interpreter=$SCOOP_WRAPPER $INPUTFILE $@
Finally in order to execute this script (launcher.sh) on multiple cores and nodes, you can use the sbatch command. For example, sbatch --ntasks=31 --cpus-per-task=1 --time=00:10:00 -p batch launcher.sh 50 will start the script with 31 cores allocated during 10 minutes to solve the rastrigin benchmark having 50 variables.

Remark The launcher requests 31 tasks with 1 cpu per task. This is NOT an efficient use of the hardware but only for educational purpose. Please always try to maximize nodes usage, i.e., 28 tasks max on iris, 128 max on aion or decrease and increase multithreading if possible. You may use --ntasks-per-nodesor --ntasks-per-socket for this purpose. Please also refer to the ULHPC documentation for more details.

After job completion, use scp or rsync to retrieve your results on your laptop.

Next

Restart the script with different size (number of variables)
Restart thie script with different core allocation
All results are saved in json file. You can use the results to compute speedup, etc ...
Try to optimize another multivariate function (e.g. shaffer function)
Read scoop documentation and adapt it for your research problem

Scalable computing with Dask





Description

Dask is a flexible library to perform parallel computing Data Science tasks in Python. Although multiple parallel and distributed computing libraries already exist in Python, Dask remains Pythonic while being very efficient (see Diagnosing Performance).

Dask is composed of two parts:

Dynamic task scheduling: Optimized computational workloads (see distributed dask)
Big Data collections: Parallel and distributed equivalent data collecting extending Numpy array, Pandas dataframes
An interesting feature of Dask is Python iterators for large-than-memory or distributed environments. Dask tries to provide different qualities:

Familiar: Provides parallelized NumPy array and Pandas DataFrame objects
Flexible: Provides a task scheduling interface for more custom workloads and integration with other projects.
Native: Enables distributed computing in pure Python with access to the PyData stack.
Fast: Operates with low overhead, low latency, and minimal serialization necessary for fast numerical algorithms
Scales up: Runs resiliently on clusters with 1000s of cores
Scales down: Trivial to set up and run on a laptop in a single process
Responsive: Designed with interactive computing in mind, it provides rapid feedback and diagnostics to aid humans


Task graphs

Dask solely rely on a graphs representation to encode algorithms. The main advantage of these structures is a clear and efficient approach for task scheduling. For Dask users, task graphs and operations are fully transparent unless you decide to develop a new module.

Code	Task graph
def inc(i):
    return i + 1

def add(a, b):
    return a + b

x = 1
y = inc(x)
z = add(y, 10)

# The graph is encoded as a dictionnary
d = {'x': 1,
     'y': (inc, 'x'),
     'z': (add, 'y', 10)}

Install Dask

Dask can be installed with pip or conda like any other python package.

Anaconda/Conda

conda install dask
# OR
conda install dask-core # Use it if you need a minimal version of dask
Please note that Dask is already included by default in the Anaconda distribution.

Pip

In order to install all dependencies (e.g. NumPy, Pandas, ...), use the following command:

python -m pip install "dask[complete]"
# OR simply
pip install "dask[complete]" 
Similarly to conda, dask core can be install with the command pip install dask. Note that additionnal modules like dask.array, dask.dataframe could be separately installed. However we strongly recommend to proceed with a full installation.

pip install "dask[array]"       # Install requirements for dask array
pip install "dask[dataframe]"   # Install requirements for dask dataframe
pip install "dask[diagnostics]" # Install requirements for dask diagnostics
pip install "dask[distributed]" # Install requirements for distributed dask
Install from sources

For those wishing to compile and optimize the library on a dedicated hardware, Dask can be compiled and installed as follows:

git clone https://github.com/dask/dask.git
cd dask
python -m pip install .
# OR
pip install ".[complete]" 
On the ULHPC platform

We strongly recommend to install Dask inside a virtual environment using the python versions included in the software set.

# See https://hpc-docs.uni.lu/connect/ssh/
ssh [aion,iris]-cluster    # assuming proper configuration
# Once on the clusters, ask for a interactive job
si --time=01:00:00 # OR si-gpu --time=01:00:00 if a GPU is needed
module load lang/Python # Load default python 
python -m venv dask_env_${ULHPC_CLUSTER}
source dask_env_${ULHPC_CLUSTER}/bin/activate
pip install --upgrade pip
pip install "dask[complete]"
Setup

Dask can be used on different hardware going from your laptop to a multi-node cluster. For this purpose, Dask considers two families of task schedulers. By default, if no client is instantiated, Dask will turn on the local schedule.

import dask.dataframe as dd
df = dd.read_csv(...)
df.x.sum().compute()  # This uses the single-machine scheduler by default
If you need more resources, dask.distributed will be needed to setup and connect to a distributed cluster.

from dask.distributed import Client
client = Client(...)  # Connect to distributed cluster and override default
df.x.sum().compute()  # This now runs on the distributed system
Setting a Dask cluster

In the remainder of this paper, we will only consider Distributed Dask cluster. Nevertheless, you can also consider a local cluster on your laptop to test your workflow at small scale. More details can be found in the dask.distributed documentation.

On the ULHPC platform, you have two strategies to create a Dask cluster:

Using SLURMCluster class
Starting manually the dask-scheduler and dask-workers
First, we are going to setup a python virtual environment in order to install all required python libraries.

Be sure to start with a bare environment:

No interactive job running and thus no loaded modules
No python virtualenv already loaded
Apply the following commands to setup your environment.

# Clone tutorial repository
git clone https://github.com/ULHPC/tutorials.git
# cd into the scripts folder
cd tutorials/python/advanced/dask-ml/scripts
# Ask an interactive job
si --time=01:00:00
# Load python3 module (load by default Python3)
module load lang/Python
python -m venv dask_env_${ULHPC_CLUSTER}
source dask_env_${ULHPC_CLUSTER}/bin/activate
pip install --upgrade pip
pip install -r requirements.txt
Automatic setup

We first create a generic launcher cluster_jobs_workers.sh. Be carefull, you will need to install two different virtualenv if you planned to run the code on both clusters, i.e., Aion and Iris.

#!/bin/bash -l

#SBATCH -p batch          
#SBATCH -J DASK_jobs_workers     
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1                
#SBATCH --cpus-per-task=1               
#SBATCH -t 00:30:00        

# Load the python version used to install Dask
module load lang/Python

# Make sure that you have an virtualenv dask_env installed
export DASK_VENV="$1" 
shift
if [ ! -d "${DASK_VENV}" ] || [ ! -f "${DASK_VENV}/bin/activate" ]; then

    echo "Error with virtualenv" && exit 1

fi

# Source the python env
source "${DASK_VENV}/bin/activate"

python -u $*
Below a small example to start workers as slurm jobs: cluster_jobs_workers.py.

from dask_jobqueue import SLURMCluster
from dask.distributed import Client
import dask
# Library to generate plots
import matplotlib as mpl
# Define Agg as Backend for matplotlib when no X server is running
mpl.use('Agg')
import matplotlib.pyplot as plt
import socket
import os

# Submit workers as slurm job
# Below we define the slurm parameters of a single worker
cluster = SLURMCluster(cores=os.environ.get("SLURM_CPUS_PER_TASK",1),
                       processes=1,
                       memory="4GB",
                       walltime="01:00:00",
                       queue="batch",
                       interface="ib0")

numworkers = os.environ("SLURM_NTASKS",1)
cluster.scale(numworkers)

# Connect to distributed cluster and override default
client = Client(cluster)
client.wait_for_workers()

# Decorator  
@dask.delayed
def inc(x):
    return x + 1

@dask.delayed
def double(x):
    return x * 2

@dask.delayed
def add(x, y):
    return x + y

data = [1, 2, 3, 4, 5]

output = []
for x in data:
    a = inc(x)
    b = double(x)
    c = add(a, b)
    output.append(c)

# Second approach as a delayed function
total = dask.delayed(sum)(output)
total.visualize(filename='task_graph.svg')
# parallel execution workers
results = total.compute()
print(results)
#### Very important ############
cluster.close()
The Dask delayed function decorates your functions so that they operate lazily. Rather than executing your function immediately, it will defer execution, placing the function and its arguments into a task graph.



You can execute the previous example with the following command: sbatch cluster_jobs_workers.sh dask_env_${ULHPC_CLUSTER} cluster_jobs_workers.py. Once the main job has started, you should see dask-workers spanning in the queue using squeue -u user. Please note also that each worker has his own slurm-jobid.out file which provide all necessary information to diagnose problems. An example is provided below.

distributed.nanny - INFO -         Start Nanny at: 'tcp://172.19.6.19:44324'
distributed.worker - INFO -       Start worker at:    tcp://172.19.6.19:37538
distributed.worker - INFO -          Listening to:    tcp://172.19.6.19:37538
distributed.worker - INFO -          dashboard at:          172.19.6.19:39535
distributed.worker - INFO - Waiting to connect to:    tcp://172.19.6.19:39227
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   3.73 GiB
distributed.worker - INFO -       Local Directory: /mnt/irisgpfs/users/ekieffer/Dask/dask-worker-space/worker-p1ij_9ar
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:    tcp://172.19.6.19:39227
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO - Stopping worker at tcp://172.19.6.19:37538
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://172.19.6.19:44324'
distributed.dask_worker - INFO - End worker
Manual setup

In this part, we show how to start manually the dask-scheduler and how to spawn the workers. Hereafter, workers will not be created as new jobs but new steps inside a main job. We advise to use this workflow to avoid filling the scheduler queue.

First, we create a new slurm launcher: cluster_steps_workers.sh

#!/bin/bash -l

#SBATCH -p batch    
#SBATCH -J DASK_steps_workers    
#SBATCH -N 2
#SBATCH -n 10     
#SBATCH -c 1    
#SBATCH -t 00:30:00    

# Load the python version used to install Dask
module load lang/Python

# Make sure that you have an virtualenv dask_env installed
export DASK_VENV="$1" 
shift
if [ ! -d "${DASK_VENV}" ] || [ ! -f "${DASK_VENV}/bin/activate" ]; then

        echo "Error with virtualenv" && exit 1

    fi

# Source the python env 
source "${DASK_VENV}/bin/activate"

# Dask configuration to store the scheduler file
DASK_CONFIG="${HOME}/.dask"
DASK_JOB_CONFIG="${DASK_CONFIG}/job_${SLURM_JOB_ID}"
mkdir -p ${DASK_JOB_CONFIG}
export SCHEDULER_FILE="${DASK_JOB_CONFIG}/scheduler.json"


# Number of tasks - 1 controller task - 1 python task
export NB_WORKERS=$((${SLURM_NTASKS}-2))


LOG_DIR="$(pwd)/logs/job_${SLURM_JOBID}"
mkdir -p ${LOG_DIR}

# Start scheduler on this first task
srun -w $(hostname) --output=${LOG_DIR}/scheduler-%j-workers.out  --exclusive -N 1 -n 1 -c ${SLURM_CPUS_PER_TASK} \
     dask-scheduler  --scheduler-file "${SCHEDULER_FILE}"  --interface "ib0" &
sleep 10

#srun: runs ipengine on each other available core
srun --output=${LOG_DIR}/ipengine-%j-workers.out \
     --exclusive -n ${NB_WORKERS} -c ${SLURM_CPUS_PER_TASK} \
     --cpu-bind=cores dask-worker  \
     --label \
     --interface "ib0" \
     --scheduler-file "${SCHEDULER_FILE}"  &

sleep 25 

srun --output=${LOG_DIR}/code-%j-execution.out  --exclusive -N 1 -n 1 -c ${SLURM_CPUS_PER_TASK} python -u $*

Remark The launcher below requests 10 tasks on 2 nodes with 1 cpu per task. This is NOT an efficient use of the hardware but only for educational purpose. Please always try to maximize nodes usage, i.e., 28 tasks max on iris, 128 max on aion or decrease and increase multithreading if possible. You may use  --ntasks-per-nodesor --ntasks-per-socket for this purpose. Please also refer to the ULHPC documentation for more details.

To illustrate this manual setting, we are going now to scale XGBoost using Dask. XGBoost is an optimized gradient boosting library designed to be highly efficient, flexible and portable. Gradient boosted trees can be distributed by making Dask and XGBoost working together. XGBoost provides a powerful prediction framework, and it works well in practice. It wins Kaggle contests and is popular in industry because it has good performance and can be easily interpreted (i.e., it’s easy to find the important features from a XGBoost model).

	
Suppose we have access to Dask cluster with a set of workers. The first task is to install the xgboost library.

pip install xgboost
then create the following script cluster_steps_workers.py:

from dask.distributed import Client
# Library to generate plots
import matplotlib as mpl
# Define Agg as Backend for matplotlib when no X server is running
mpl.use('Agg')
import matplotlib.pyplot as plt
import dask
import xgboost as xgb
import dask.array as da
import json
import os

data=[]
# Using the distributed shared file system, we can access to the Dask cluster
# configuration.
# We read the scheduler address and port from the scheduler file
with open(os.environ["SCHEDULER_FILE"]) as f:
        data = json.load(f)
        scheduler_address=data['address']

# Connect to the the cluster
client = Client(scheduler_address)
client.wait_for_workers()

# X and y must be Dask dataframes or arrays
num_obs = 1e5
num_features = 20
X = da.random.random(size=(num_obs, num_features), chunks=(1000, num_features))
y = da.random.random(size=(num_obs, 1), chunks=(1000, 1))

# Training
dtrain = xgb.dask.DaskDMatrix(client, X, y)

output = xgb.dask.train(
    client,
    {"verbosity": 2, "tree_method": "hist", "objective": "reg:squarederror"},
    dtrain,
    num_boost_round=10,
    evals=[(dtrain, "train")],
)

booster = output['booster']  # booster is the trained model
history = output['history']  # A dictionary containing evaluation results

ax = xgb.plot_importance(booster, height=0.8, max_num_features=9)
ax.grid(False, axis="y")
ax.set_title('Estimated feature importance')
plt.savefig("importance.png")

# Stop Dask cluster
client.shutdown()
You can try the manual setup example using the following command: sbatch cluster_steps_workers.sh dask_env_${ULHPC_CLUSTER} cluster_steps_workers.py.

Remark on the worker Thread Pool

Each worker sends computations to a thread in a concurrent.futures.ThreadPoolExecutor for computation. These computations occur in the same process as the Worker communication server so that they can access and share data efficiently between each other. For the purposes of data locality all threads within a worker are considered the same worker.

If your computations are mostly numeric in nature (for example NumPy and Pandas computations) and release the GIL entirely then it is advisable to run dask-worker processes with many threads and one process. This reduces communication costs and generally simplifies deployment.

If your computations are mostly Python code and don’t release the GIL then it is advisable to run dask-worker processes with many processes and one thread per process:

$ dask-worker scheduler:8786 --nprocs 8 --nthreads 1
This will launch 8 worker processes each of which has its own ThreadPoolExecutor of size 1.

If your computations are external to Python and long-running and don’t release the GIL then beware that while the computation is running the worker process will not be able to communicate to other workers or to the scheduler. This situation should be avoided. If you don’t link in your own custom C/Fortran code then this topic probably doesn’t apply.

DASK + Jupyter

Dask can be used in combination with Jupyter to perform Parallel interactive computations. The dask_jupyter.sh launcher is an example how to start a Jupyter Notebook with Dask support. Here, we will only consider dask-worker processes ... We start the notebook instance and the dask scheduler on the first node of the allocation and assign workers to remaining cores.

#!/bin/bash -l

#SBATCH -p batch    
#SBATCH -J DASK_JUPYTER
#SBATCH -N 2
#SBATCH -n 10     
#SBATCH -c 1    
#SBATCH -t 00:30:00    

# Load the python version used to install Dask
module load lang/Python

# Export Environment variables
# Set a environement which depends on which cluster you wish to start the notebook
export VENV="$HOME/.envs/jupyter_dask_${ULHPC_CLUSTER}"

# Replace default jupyter and environement variable by custom ones
# We add to the path the jobid for debugging purpose
export JUPYTER_CONFIG_DIR="$HOME/jupyter/$SLURM_JOBID/"
export JUPYTER_PATH="$VENV/share/jupyter":"$HOME/jupyter_sing/$SLURM_JOBID/jupyter_path"
export JUPYTER_DATA_DIR="$HOME/jupyter/$SLURM_JOBID/jupyter_data"
export JUPYTER_RUNTIME_DIR="$HOME/jupyter/$SLURM_JOBID/jupyter_runtime"

# We create the empty directory
mkdir -p $JUPYTER_CONFIG_DIR

# The Jupyter notebook will run on the first node of the slurm allocation (here only one anyway)
# We retrieve its address
export IP_ADDRESS=$(hostname -I | awk '{print $1}')

# Dask configuration to store the scheduler file
export DASK_CONFIG="${HOME}/.dask"
export DASK_JOB_CONFIG="${DASK_CONFIG}/job_${SLURM_JOB_ID}"
mkdir -p ${DASK_JOB_CONFIG}
export SCHEDULER_FILE="${DASK_JOB_CONFIG}/scheduler.json"


# Minimal virtualenv setup
# We create a minimal virtualenv with the necessary packages to start
if [ ! -d "$VENV" ];then
    echo "Building the virtual environment"
    # Create the virtualenv
    python3 -m venv $VENV 
    # Load the virtualenv
    source "$VENV/bin/activate"
    # Upgrade pip 
    python3 -m pip install pip --upgrade
    # Install minimum requirement
    python3 -m pip install dask[complete] matplotlib \
        dask-jobqueue \
        graphviz \
        xgboost \
        jupyter \
        jupyter-server-proxy

    # Setup ipykernel
    # "--sys-prefix" install ipykernel where python is installed
    # here next the python symlink inside the virtualenv
    python3 -m ipykernel install --sys-prefix --name custom_kernel --display-name custom_kernel
fi

export XDG_RUNTIME_DIR=""


# Source the python env 
source "${VENV}/bin/activate"


#create a new ipython profile appended with the job id number
echo "On your laptop: ssh -p 8022 -NL 8889:${IP_ADDRESS}:8889 ${USER}@access-${ULHPC_CLUSTER}.uni.lu " 

# Start jupyter on a single core
srun --exclusive -N 1 -n 1 -c 1 -w $(hostname) jupyter notebook --ip ${IP_ADDRESS} --no-browser --port 8889 &

sleep 5s

# No real need to use srun here ....
# We should only be careful to call the jupyter executable where the
# notebook instance has been started

srun --exclusive -N 1 -n 1 -c 1 -w $(hostname) jupyter notebook list
srun --exclusive -N 1 -n 1 -c 1 -w $(hostname) jupyter --paths
srun --exclusive -N 1 -n 1 -c 1 -w $(hostname) jupyter kernelspec list


# Start scheduler on this first task
srun -w $(hostname) --exclusive -N 1 -n 1 -c 1 \
     dask-scheduler  --scheduler-file "${SCHEDULER_FILE}"  --interface "ib0" &
sleep 10

# Number of tasks - 1 controller task - 1 jupyter task
export NB_WORKERS=$((${SLURM_NTASKS}-2))

#srun: runs ipengine on each other available core
srun  --exclusive -n ${NB_WORKERS} -c 1 \
     --cpu-bind=cores dask-worker  \
     --label \
     --interface "ib0" \
     --scheduler-file "${SCHEDULER_FILE}"  &

wait

You can start dask+jupyter by issuing the following command: sbatch dask_jupyter.sh. Using tail -F slurm-<idjob>.out, you can observe the different setups, i.e., notebook + dask cluster. Once all setups have been performed, you should see a similar output than the one below:

In your laptop: ssh -p 8022 -NL 8889:172.17.6.155:8889 ekieffer@access-iris.uni.lu
[I 16:08:17.627 NotebookApp] Writing notebook server cookie secret to /home/users/ekieffer/jupyter/2540915/jupyter_runtime/notebook_cookie_secret
[I 16:08:18.485 NotebookApp] Serving notebooks from local directory: /mnt/irisgpfs/users/ekieffer/HPC_SCHOOL_2021/python/advanced/dask-ml/scripts
[I 16:08:18.485 NotebookApp] Jupyter Notebook 6.4.5 is running at:
[I 16:08:18.485 NotebookApp] http://172.17.6.155:8889/?token=a32b995ddd86e73eac5a4e9d20cbc9907ac52a5afb92c1d7
[I 16:08:18.485 NotebookApp]  or http://127.0.0.1:8889/?token=a32b995ddd86e73eac5a4e9d20cbc9907ac52a5afb92c1d7
[I 16:08:18.485 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[C 16:08:18.490 NotebookApp]

    To access the notebook, open this file in a browser:
        file:///home/users/ekieffer/jupyter/2540915/jupyter_runtime/nbserver-117695-open.html
    Or copy and paste one of these URLs:
        http://172.17.6.155:8889/?token=a32b995ddd86e73eac5a4e9d20cbc9907ac52a5afb92c1d7
     or http://127.0.0.1:8889/?token=a32b995ddd86e73eac5a4e9d20cbc9907ac52a5afb92c1d7
Currently running servers:
http://172.17.6.155:8889/?token=a32b995ddd86e73eac5a4e9d20cbc9907ac52a5afb92c1d7 :: /mnt/irisgpfs/users/ekieffer/HPC_SCHOOL_2021/python/advanced/dask-ml/scripts
config:
    /home/users/ekieffer/jupyter/2540915/
    /home/users/ekieffer/.envs/jupyter_dask_iris/etc/jupyter
    /usr/local/etc/jupyter
    /etc/jupyter
data:
    /home/users/ekieffer/.envs/jupyter_dask_iris/share/jupyter
    /home/users/ekieffer/jupyter_sing/2540915/jupyter_path
    /home/users/ekieffer/jupyter/2540915/jupyter_data
    /home/users/ekieffer/.envs/jupyter_dask_iris/share/jupyter
    /usr/local/share/jupyter
    /usr/share/jupyter
runtime:
    /home/users/ekieffer/jupyter/2540915/jupyter_runtime
Available kernels:
  custom_kernel    /home/users/ekieffer/.envs/jupyter_dask_iris/share/jupyter/kernels/custom_kernel
  python3          /home/users/ekieffer/.envs/jupyter_dask_iris/share/jupyter/kernels/python3
distributed.scheduler - INFO - -----------------------------------------------
distributed.scheduler - INFO - -----------------------------------------------
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:   tcp://172.19.6.155:8786
distributed.scheduler - INFO -   dashboard at:         172.19.6.155:8787
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.19.6.155:46160'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.19.6.155:33160'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.19.6.155:46866'
distributed.worker - INFO -       Start worker at:   tcp://172.19.6.155:37158
distributed.worker - INFO -       Start worker at:   tcp://172.19.6.155:41399
distributed.worker - INFO -          Listening to:   tcp://172.19.6.155:37158
distributed.worker - INFO -       Start worker at:   tcp://172.19.6.155:41713
distributed.worker - INFO -          dashboard at:         172.19.6.155:37929
distributed.worker - INFO -          Listening to:   tcp://172.19.6.155:41713
distributed.worker - INFO - Waiting to connect to:    tcp://172.19.6.155:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.19.6.155:40746
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:    tcp://172.19.6.155:8786
Use the foolwing ssh command: ssh -p 8022 -NL 8889:${IP_ADDRESS}:8889 ${USER}@access-${ULHPC_CLUSTER}.uni.lu displayed in the output file to be able to access the notebook from your local machine. In order to access the notebook for the first time, please click on the link containing the token (ex:http://127.0.0.1:8889/?token=<token_value>). This link has been generated in the output file.

You should have now access to the DASK_JUPYTER.ipynb notebook (see below).



References

https://distributed.dask.org/en/latest/
https://jobqueue.dask.org/en/latest/
https://wiki.mpimet.mpg.de/doku.php?id=analysis:pot_pourri:sapphire:dask_parallel_postprocessing

Introduction to CUDA C/C++ (2023)

 Copyright (c) Pierre Talbot, 2023 UL HPC Team <hpc-team@uni.lu>


For this session, everything is explained in the slides given above. The code is available in different places:

gpu/cuda2023: contains the exercises (.cu files).
gpu/cuda2023/solutions: contains the solutions of the exercises.
gpu/cuda2023/demo: contains the fully functional code demo shown and explained during the session.

Introduction to GPU programming with CUDA (C/C++)

 Copyright (c) 2013-2021 UL HPC Team <hpc-sysadmins@uni.lu>


This tutorial will cover the following aspects of CUDA programming:

Write, compile and run C/C++ programs that both call CPU functions and launch GPU kernels.
Control parallel thread hierarchy using execution configuration.
Allocate and free memory available to both CPUs and GPUs.
Access memory on both GPU and CPU.
Profile and improve the performance of your application.
Solutions to some of the exercises can be found in the code sub-directory.

The tutorial is based on the Nvidia DLI course "Fundamentals of accelerated computing with CUDA C/C++".

More information can be obtained from the guide.

Pre-requisites

Ensure you are able to connect to the UL HPC clusters. In particular, recall that the module command is not available on the access frontends.

### Access to ULHPC cluster - here iris
(laptop)$> ssh iris-cluster
# /!\ Advanced (but recommended) best-practice:
#    always work within an GNU Screen session named with 'screen -S <topic>' (Adapt accordingly)
# IIF not yet done, copy ULHPC .screenrc in your home
(access)$> cp /etc/dotfiles.d/screen/.screenrc ~/
Now you'll need to pull the latest changes in your working copy of the ULHPC/tutorials you should have cloned in ~/git/github.com/ULHPC/tutorials (see "preliminaries" tutorial)

(access)$> cd ~/git/github.com/ULHPC/tutorials
(access)$> git pull
Access to a GPU-equipped node of the Iris cluster

Reserve a node with one GPU for interactive development, load the necessary modules, and save them for a quick restore.

As usual, more information can be found in the documentation.

### Have an interactive GPU job
# ... either directly
(access)$> si-gpu
# ... or using the HPC School reservation 'hpcschool-gpu' if needed  - use 'sinfo -T' to check if active and its name
# (access)$> si-gpu --reservation=hpcschool-gpu
$ nvidia-smi
$ nvcc  # ?
Driver is loaded, but we still need to load the CUDA development kit.

$ module av cuda  # av versus spider
----------------------------------------------------------- /opt/apps/resif/data/stable/default/modules/all ------------------------------------------------------------
   bio/GROMACS/2019.2-fosscuda-2019a                    mpi/impi/2018.4.274-iccifortcuda-2019a
   bio/GROMACS/2019.2-intelcuda-2019a                   numlib/FFTW/3.3.8-intelcuda-2019a
   compiler/Clang/8.0.0-GCCcore-8.2.0-CUDA-10.1.105     numlib/cuDNN/7.4.2.24-gcccuda-2019a
   data/h5py/2.9.0-fosscuda-2019a                       numlib/cuDNN/7.6.4.38-gcccuda-2019a                       (D)
   data/h5py/2.9.0-intelcuda-2019a                      system/CUDA/9.2.148.1
   devel/PyTorch/1.2.0-fosscuda-2019a-Python-3.7.2      system/CUDA/10.0.130
   lang/SciPy-bundle/2019.03-fosscuda-2019a             system/CUDA/10.1.105-GCC-8.2.0-2.31.1
   lang/SciPy-bundle/2019.03-intelcuda-2019a            system/CUDA/10.1.105-iccifort-2019.1.144-GCC-8.2.0-2.31.1
   lib/NCCL/2.4.7-gcccuda-2019a                         system/CUDA/10.1.105
   lib/TensorFlow/1.13.1-fosscuda-2019a-Python-3.7.2    system/CUDA/10.1.243                                      (D)
   lib/TensorRT/6.0.1.5-fosscuda-2019a-Python-3.7.2     system/CUDA/10.2.89
   lib/libgpuarray/0.7.6-fosscuda-2019a                 toolchain/fosscuda/2019a
   math/Keras/2.2.4-fosscuda-2019a-Python-3.7.2         toolchain/gcccuda/2019a
   math/Theano/1.0.4-fosscuda-2019a                     toolchain/iccifortcuda/2019a
   math/magma/2.5.1-fosscuda-2019a                      toolchain/intelcuda/2019a
   mpi/OpenMPI/3.1.4-gcccuda-2019a                      tools/Horovod/0.16.3-fosscuda-2019a-Python-3.7.2

  Where:
   D:  Default Module

Use "module spider" to find all possible modules.
Use "module keyword key1 key2 ..." to search for all possible modules matching any of the "keys".

$ module av gcc cuda
$ module load compiler/GCC system/CUDA  # defaults are fine
$ ml
$ module save cuda  # save our environment
$ module purge
$ module restore cuda
In case there is not enough GPU cards available, you can submit passive jobs, using sbatch. Below is an example sbatch file, to remote compile, run and profile a source file:

#!/bin/bash -l
#SBATCH --job-name="GPU build"
#SBATCH --ntasks=1
#SBATCH -c 1
#SBATCH --time=0-00:10:00
#SBATCH --partition=gpu
#SBATCH --gres=gpu:1

if [ -z "$1" ]
then
    echo "Missing required source (.cu), and optional execution arguments."
    exit
fi

src=${1}
exe=$(basename ${1/cu/out})
ptx=$(basename ${1/cu/ptx})
prf=$(basename ${1/cu/prof})
shift
args=$*

# after the module profile is saved (see above)
module restore cuda

# compile
srun nvcc -arch=compute_70 -o ./$exe $src
# save ptx
srun nvcc -ptx -arch=compute_70 -o ./$ptx $src
# execute
srun ./$exe $args
# profile
srun nvprof --log-file ./$prf ./$exe $args
echo "file: $prf"
cat ./$prf
Writing application for the GPU

CUDA provides extensions for many common programming languages, in the case of this tutorial, C/C++. There are several API available for GPU programming, with either specialization, or abstraction. The main API is the CUDA Runtime. Another, lower level API, is CUDA Driver, which also offers more customization options. Example of other APIs, built on top of the CUDA Runtime, are Thrust, NCCL.

Hello World

Below is a example CUDA .cu program (.cu is the required file extension for CUDA-accelerated programs). It contains two functions, the first which will run on the CPU, the second which will run on the GPU.

#include <cstdio>
#include "cuda.h"

void CPUFunction()
{
  printf("hello from the Cpu.\n");
}

__global__
void GPUFunction()
{
  printf("hello from the Gpu.\n");
}

int main()
{
  CPUFunction();

  GPUFunction<<<1, 1>>>();

  cudaDeviceSynchronize();

  return EXIT_SUCCESS;
}
Here are some important lines to highlight, as well as some other common terms used in accelerated computing:

__global__
void GPUFunction()
The __global__ keyword indicates that the following function will run on the GPU, and can be invoked globally, which means either by the CPU or GPU. Often, code executed on the CPU is referred to as host code, and code running on the GPU is referred to as device code. Notice the return type void. It is required that functions defined with the __global__ keyword return type void.

GPUFunction<<<1, 1>>>();
Typically, when calling a function to run on the GPU, we call this function a kernel, which is launched. When launching a kernel, we must provide an execution configuration, which is done by using the <<< ... >>> syntax just prior to passing the kernel any expected arguments. At a high level, execution configuration allows programmers to specify the thread hierarchy for a kernel launch, which defines the number of thread groupings (called blocks), as well as how many threads to execute in each block.

The execution configuration allows programmers to specify details about launching the kernel to run in parallel on multiple GPU threads. More precisely, the execution configuration allows programmers to specifiy how many groups of threads - called thread blocks, or just blocks - and how many threads they would like each thread block to contain. The simplest syntax for this is: (there are 2 othe parameters available)

<<< NUMBER_OF_BLOCKS, NUMBER_OF_THREADS_PER_BLOCK>>>
The kernel code is executed by every thread in every thread block configured when the kernel is launched.

This illustrates the data parallel model of CUDA. The same function is executed on all threads.

cudaDeviceSynchronize();
Unlike much C/C++ code, launching kernels is asynchronous: the CPU code will continue to execute without waiting for the kernel launch to complete. A call to `cudaDeviceSynchronize, a function provided by the CUDA runtime, will cause the host (CPU) code to wait until the device (GPU) code completes, and only then resume execution on the CPU.

Compiling and running CUDA code

This section contains details about the nvcc command you issue to compile and run your .cu program.

The CUDA platform ships with the NVIDIA CUDA Compiler nvcc, which can compile CUDA accelerated applications, both the host, and the device code they contain. After completing the lab, For anyone interested in a deeper dive into nvcc, start with the documentation (nvcc --help).

Compiling and executing some-CUDA.cu source file:

$nvcc -arch=sm_70 -o out some-CUDA.cu -run
nvcc is the command line command for using the nvcc compiler. some-CUDA.cu is passed as the source file to compile. The o flag is used to specify the output file for the compiled program. The arch flag indicates for which architecture the files must be compiled. For the present case sm_70 will serve to compile specifically for the Volta GPUs. This will be further explained in a following section. As a matter of convenience, providing the -run flag will execute the successfully compiled binary.

nvcc parses the C++ language (it used to be C).

Practice: Launch parallel kernels

The following program currently makes a function call that prints a message, but it is incorrect.

Fix and refactor the code such that helloGPU kernel to execute in parallel on 5 threads, all executing in a single thread block. You should see the output message printed 5 times after compiling and running the code.

Refactor the helloGPU kernel again, this time to execute in parallel inside 5 thread blocks, each containing 5 threads. You should see the output message printed 25 times now after compiling and running.

/*
 * FIXME
 * (hello.cu)
 */

void helloCPU()
{
  std::cout<<"Hello from Cpu.\n";
}

void helloGPU()
{
  printf("Hello also from Gpu.\n");
}

int main()
{

  helloCPU();
  helloGPU();

  return EXIT_SUCCESS;
}

Compiling and running CUDA code, continued

The following compilation command works:

$ nvcc some-CUDA.cu
However, there is a potential problem with this code. Cuda uses a two stage compilation process, to PTX, and to binary.

To produce the PTX for the cuda kernel, use:

$ nvcc -ptx -o out.ptx some-CUDA.cu
Brief inspection of the generated PTX file reports a target real platform of sm_30.

// Based on LLVM 3.4svn
//

.version 6.4
.target sm_30
.address_size 64
The Volta GPU implements sm_70. So, we could be missing some features from the GPU.

To specify an instruction set, use the -arch option:

$ nvcc -o out.ptx -ptx -arch=compute_70 some-CUDA.cu
To produce an executable, instead of just the PTX code, and specify an instruction set, use the -arch option, for example:

$ nvcc -o out -arch=compute_70 some-CUDA.cu
This actually produces an executable that embeds the kernels' code as PTX, of the specified instruction set. The PTX code will then be JIT compiled when executed, matching the real GPU instruction set. To see this, search for the target PTX instruction in the executable:

$ strings out | grep target
The code option specifies what the executable contains. The following nvcc options specify that the executables contains the binary code for the real GPU sm_70.

$ nvcc -o out -arch=compute_70 -code=sm_70 some-CUDA.cu
The following nvcc options specify that the executables contains the binary code for the real GPU sm_70, and the PTX code for the sm_70.

$ nvcc -o out -arch=compute_70 -code=sm_70,compute_70 some-CUDA.cu
To observe the difference, search for the target PTX command, in both commands:

$ strings out | grep target
Actually, the first compilation instruction:

$ nvcc -o out -arch=sm_70 some-CUDA.cu
is a shorthand for the full command:

$ nvcc -o out -arch=compute_70 -code=sm_70,compute_70 some-CUDA.cu
In summary, if you want to package the PTX to allow for JIT compilation across different real GPU:

$ nvcc -o out -arch=compute_70 some-CUDA.cu  # or sm_70
Error handling

It is strongly recommended to check for errors when calling CUDA API.a

cudaError_t rc;  # cudaSuccess => ok
rc = cudaDeviceSynchronize();
printf("%s\n", cudaGetErrorString(rc));

// for asynchronous calls:
rc = cudaGetLastError();  # call after synchronization for post-launch kernel errors
For convenience, the following macros can help:

#define CUDIE(result) { \
        cudaError_t e = (result); \
        if (e != cudaSuccess) { \
                std::cerr << __FILE__ << ":" << __LINE__; \
                std::cerr << " CUDA runtime error: " << cudaGetErrorString(e) << '\n'; \
                exit((int)e); \
        }}

#define CUDIE0() CUDIE(cudaGetLastError())
For example, try compiling and executing code on iris (Volta GPU) with -arch=sm_75 (or compute_75).

CUDA thread hierarchy

Each thread will execute the same kernel function. Therefore, some coordination mechanism is needed for each thread to work on different memory locations. A mechanism involves builtin constants, that are used to uniquely identify threads of an execution configuration. Other aspects involve thread synchronization, streams, events, cooperative groups (not covered here besides the host-side synchronization function), see the reference.

Thread and block indices

Each thread is given an index within its thread block, starting at 0. Additionally, each block is given an index, starting at 0. Just as threads are grouped into thread blocks, blocks are grouped into a grid, which is the highest entity in the CUDA thread hierarchy. In summary, CUDA kernels are executed in a grid of 1 or more blocks, with each block containing the same number of 1 or more threads.

CUDA kernels have access to special variables identifying both the index of the thread (within the block) that is executing the kernel, and, the index of the block (within the grid) that the thread is within. These variables are threadIdx.x and blockIdx.x respectively.

The .x suggests that there more dimensions to these variables, they can be up to 3 dimensions. We will only see examples with one dimension here. Refer to the programming guide for more information.

Within a block, the thread ID is the same as the threadIdx.x. There is a maximum of 1024 threads allowed per block. Within a one-dimensional grid, the block ID is the same as the blockIdx.x. Together, the number of blocks and number of threads allow to exceed the 1024 threads limit.

Exercise: use specific thread and block indices

The program below contains a working kernel that is not printing a success message. Edit the source code to update the execution configuration so that the success message will print.

/*
 * FIXME
 * (indices.cu)
 */

#include <cstdio>

void printif()
{
  if (threadIdx.x == 1023 && blockIdx.x == 255) {
    printf("Success!\n");
  }
}

int main()
{
  /*
   * Update the execution configuration so that the kernel
   * will print `"Success!"`.
   */

  printif<<<1, 1>>>();
}
Accelerating for loops

For loops in CPU applications can sometimes be accelerated: rather than run each iteration of the loop sequentially, each iteration of the loop can be run in parallel in its own thread. Consider the following for loop, and notice that it controls how many times the loop will execute, as well as defining what will happen for each iteration of the loop:

int N = 2<<10;
for (int i = 0; i < N; ++i) {
  printf("%d\n", i);
}
In order to parallelize this loop, 2 steps must be taken:

A kernel must do the work of a single iteration of the loop.
Because the kernel will ignore other running kernels, the execution configuration must ensure that the kernel executes the correct number of times, for example, the number of times the loop would have iterated.
Exercise: Accelerating a For Loop with a Single Block of Threads

Currently, the loop function runs a for loop that will serially print the numbers 0 through 9. Modify the loop function to be a CUDA kernel which will launch to execute N iterations in parallel. After successfully refactoring, the numbers 0 through 9 should still be printed.

/*
 * FIXME
 * (loop.cu)
 * Correct, and refactor 'loop' to be a CUDA Kernel.
 * The new kernel should only do the work
 * of 1 iteration of the original loop.
 */

#include <cstdio>

void loop(int N)
{
  for (int i = 0; i < N; ++i) {
    printf("This is iteration number %d\n", i);
  }
}

int main()
{
  /*
   * When refactoring 'loop' to launch as a kernel, be sure
   * to use the execution configuration to control how many
   * "iterations" to perform.
   *
   * Use 1 block of threads.
   */

  int N = 10;
  loop(N);
}
Using block dimensions for more parallelization

CUDA Kernels have access to another special variable that gives the number of threads in a block: blockDim.x. Using this variable, in conjunction with blockIdx.x and threadIdx.x, increased parallelization can be accomplished by organizing parallel execution accross multiple blocks of multiple threads with the idiomatic expression threadIdx.x + blockIdx.x * blockDim.x.

Exercise: accelerating a for loop with multiple blocks of threads

Make further modifications to the previous exercise, but with a execution configuration that launches at least 2 blocks. After successfully refactoring, the numbers 0 through 9 should still be printed.

/*
 * FIXME
 * (loop2.cu)
 * Fix and refactor 'loop' to be a CUDA Kernel, launched with 2 or more blocks
 * The new kernel should only do the work of 1 iteration of the original loop.
 */

#include <cstdio>

void loop(int N)
{
  for (int i = 0; i < N; ++i) {
    printf("This is iteration number %d\n", i);
  }
}

int main()
{
  /*
   * When refactoring 'loop' to launch as a kernel, be sure
   * to use the execution configuration to control how many
   * "iterations" to perform.
   *
   * For this exercise, be sure to use more than 1 block in
   * the execution configuration.
   */

  int N = 10;
  loop(N);
}
Memory allocation

Allocating memory to be accessed on the GPU and the CPU

For any meaningful work to be done, we need to access memory. The GPU has a distinct memory from the CPU, which requires data transfers to and from CPU. However, recent versions of CUDA (version 6 and later) have simplified memory allocation that is available to both the CPU host and any number of GPU devices, and while there are many intermediate and advanced techniques for memory management that will support the most optimal performance in accelerated applications, the most basic CUDA memory management technique we will now cover supports good performance gains over CPU-only applications. The simplified memory allocation is called Unified Memory System, and has many implications on multi-GPU systems.

To allocate and free memory, and obtain a pointer that can be referenced in both host and device code, replace calls to malloc and free with cudaMallocManaged and cudaFree as in the following example:

//--- CPU-only ---

int N = 2<<20;
size_t size = N * sizeof(int);

int *a;
a = (int *)malloc(size);

// Use 'a' in CPU-only program.

free(a);

//--- GPU/CPU ---

int N = 2<<20;
size_t size = N * sizeof(int);

int *a;
// Note the address of 'a' is passed as first argument.
cudaMallocManaged(&a, size);

// Use 'a' on the CPU and/or on any GPU in the accelerated system.

cudaFree(a);
For completeness, the alternative to unified memory is to:

manually allocate memory on a device
copy data from host to device memory.
This can be done with:

float* d_C;
cudaMalloc(&d_C, size);
// Copy vectors from host memory to device memory
cudaMemcpy(d_C, h_C, size, cudaMemcpyHostToDevice);  // to mimick d_C = h_C;
cudaFree(d_C);
Exercise: array manipulation on both the host and device

The source code below allocates an array, initializes it with integer values on the host, attempts to double each of these values in parallel on the GPU, and then confirms whether or not the doubling operations were successful, on the host. Currently the program will not work: it is attempting to interact on both the host and the device with an array at pointer a, but has only allocated the array (using malloc) to be accessible on the host. Refactor the application to meet the following conditions:

a should be available to both host and device code.
The memory at a should be correctly freed.
/*
 * FIXME
 * (alloc.cu)
 */

#include <cstdio>

/*
 * Initialize array values on the host.
 */

void init(int *a, int N)
{
  for (int i = 0; i < N; ++i) {
    a[i] = i;
  }
}

/*
 * Double elements in parallel on the GPU.
 */

__global__
void doubleElements(int *a, int N)
{
  int i = blockIdx.x * blockDim.x + threadIdx.x;
  if (i < N) {
    a[i] *= 2;
  }
}

/*
 * Check all elements have been doubled on the host.
 */

bool checkElementsAreDoubled(int *a, int N)
{
  for (int i = 0; i < N; ++i) {
    if (a[i] != i*2) {
      return false;
    }
  }
  return true;
}

int main()
{
  int N = 100;
  int *a;

  size_t size = N * sizeof(int);

  /*
   * Refactor this memory allocation to provide a pointer
   * 'a' that can be used on both the host and the device.
   */

  a = (int *)malloc(size);

  init(a, N);

  size_t threads_per_block = 10;
  size_t number_of_blocks = 10;

  /*
   * This launch will not work until the pointer 'a' is also
   * available to the device.
   */

  doubleElements<<<number_of_blocks, threads_per_block>>>(a, N);
  cudaDeviceSynchronize();

  bool areDoubled = checkElementsAreDoubled(a, N);
  printf("All elements were doubled? %s\n", areDoubled ? "TRUE" : "FALSE");

  /*
   * Refactor to free memory that has been allocated to be
   * accessed by both the host and the device.
   */

  free(a);
}
A solution can be found in the next exercise.

Data sets larger than the grid

Either by choice, often to create the most performant execution configuration, or out of necessity, the number of threads in a grid may be smaller than the size of a data set. Consider an array with 1000 elements, and a grid with 250 threads. Here, each thread in the grid will need to be used 4 times. One common method to do this is to use a grid-stride loop within the kernel.

In a grid-stride loop, each thread will calculate its unique index within the grid using tid+bid*bdim, perform its operation on the element at that index within the array, and then, add to its index the number of threads in the grid and repeat, until it is out of range of the array. For example, for a 500 element array and a 250 thread grid, the thread with index 20 in the grid would:

Perform its operation on element 20 of the 500 element array.
Increment its index by 250, the size of the grid, resulting in 270.
Perform its operation on element 270 of the 500 element array.
Increment its index by 250, the size of the grid, resulting in 520.
Because 520 is now out of range for the array, the thread will stop its work.
CUDA provides a special builtin variable giving the number of blocks in a grid: gridDim.x. Calculating the total number of threads in a grid then is simply the number of blocks in a grid multiplied by the number of threads in each block, gridDim.x * blockDim.x. With this in mind, here is a verbose example of a grid-stride loop within a kernel:

__global__
void kernel(int *a, int N)
{
  int indexWithinTheGrid = threadIdx.x + blockIdx.x * blockDim.x;
  int gridStride = gridDim.x * blockDim.x;

  for (int i = indexWithinTheGrid; i < N; i += gridStride)
  {
    // do work on a[i];
  }
}
Exercise: use a grid-stride loop to manipulate an array larger than the grid

Refactor the previous code to use a grid-stride loop in the doubleElements kernel, in order that the grid, which is smaller than N, can reuse threads to cover every element in the array. The program will print whether or not every element in the array has been doubled, currently the program accurately prints FALSE.

/*
 * FIXME
 * (loop-stride.cu)
 * using strides
 */

#include <cstdio>

void init(int *a, int N)
{
  for (int i = 0; i < N; ++i) {
    a[i] = i;
  }
}

/*
 * In the current application, 'N' is larger than the grid.
 * Refactor this kernel to use a grid-stride loop in order that
 * each parallel thread work on more than one element of the array.
 */

__global__
void doubleElements(int *a, int N)
{
  int i = blockIdx.x * blockDim.x + threadIdx.x;
  if (i < N) {
    a[i] *= 2;
  }
}

bool checkElementsAreDoubled(int *a, int N)
{
  for (int i = 0; i < N; ++i) {
    if (a[i] != i*2) {
      return false;
    }
  }
  return true;
}

int main()
{
  /*
   * 'N' is greater than the size of the grid (see below).
   */

  int N = 10000;
  int *a;

  size_t size = N * sizeof(int);
  cudaMallocManaged(&a, size);

  init(a, N);

  /*
   * The size of this grid is 256*32 = 8192.
   */

  size_t threads_per_block = 256;
  size_t number_of_blocks = 32;

  doubleElements<<<number_of_blocks, threads_per_block>>>(a, N);
  cudaDeviceSynchronize();

  bool areDoubled = checkElementsAreDoubled(a, N);
  printf("All elements were doubled? %s\n", areDoubled ? "TRUE" : "FALSE");

  cudaFree(a);
}
One solution is in file sol-array-stride.cu.

Handling block configuration mismatches to number of needed threads (minor)

It may be the case that an execution configuration cannot be expressed to create the exact number of threads needed for parallelizing a loop.

A common example has to do with the desire to choose optimal block sizes. For example, due to GPU hardware traits, blocks that contain a number of threads that are a multiple of 32 are often desirable for performance benefits. Assuming that we wanted to launch blocks each containing 256 threads (a multiple of 32), and needed to run 1000 parallel tasks, then there is no number of blocks that would produce an exact total of 1000 threads in the grid, since there is no integer value 32 can be multiplied by to equal exactly 1000.

This scenario can be easily addressed in the following way:

Write an execution configuration that creates more threads than necessary to perform the allotted work. Pass a value as an argument into the kernel (N) that represents to the total size of the data set to be processed, or the total threads that are needed to complete the work. After calculating the thread's index within the grid (using tid+bid*bdim), check that this index does not exceed N, and only perform the pertinent work of the kernel if it does not. Here is an example of an idiomatic way to write an execution configuration when both N and the number of threads in a block are known, and an exact match between the number of threads in the grid and N cannot be guaranteed. It ensures that there are always at least as many threads as needed for N, and only 1 additional block's worth of threads extra, at most:

// Assume 'N' is known
int N = 100000;

// Assume we have a desire to set 'threads_per_block' exactly to '256'
size_t threads_per_block = 256;

// Ensure there are at least 'N' threads in the grid, but only 1 block's worth extra
size_t number_of_blocks = (N + threads_per_block - 1) / threads_per_block;

some_kernel<<<number_of_blocks, threads_per_block>>>(N);
Because the execution configuration above results in more threads in the grid than N, care will need to be taken inside of the some_kernel definition so that some_kernel does not attempt to access out of range data elements, when being executed by one of the "extra" threads:

__global__
some_kernel(int N)
{
  int idx = threadIdx.x + blockIdx.x * blockDim.x;

  if (idx < N) // Check to make sure 'idx' maps to some value within 'N'
  {
    // Only do work if it does
  }
}
Shared memory

On first approximation, there are 3 memories available to your kernel code:

global memory, large but slow,
block shared memory, small but fast,
registers.
Within a kernel code:

automatic variable are stored in registers
__shared__ specifier indicates the block memory, faster than global memory (limited, 96KB).
Shared memory can be declared outside the kernel code with:

extern __shared__ float shared[];
However, then the size of the shared memory must be declared in the kernel configuration:

kernel<<<2, 32, 16*sizeof(int)>>>();
extern __shared__ int a_blk[];

__global__ void doubleElements(int *a, int N) {
  int i = blockIdx.x * blockDim.x + threadIdx.x;
  if (i<N) {
          a_blk[threadIdx.x] = a[i];
          a_blk[threadIdx.x] *= 2;  // or something more complex
          a[i] = a_blk[threadIdx.x];
  }
}
// ...
doubleElements<<<10, 16, 16*sizeof(*a)>>>(a, N);
cudaDeviceSynchronize();
// ...
Note: __syncthreads() can be used to synchronize threads of a thread block: waits for all shared and global access to be visible from all threads (in a block).

Performance considerations

In this section, we will investigate how to improve the performance (runtime) of a CUDA application.

We'll be looking at:

Measuring the performance.
Features that affect peformance: execution configuration and memory management.
Preparation

The starting point for all experiments is a simple vector addition program vectoradd.cu, which can be found in the samples sub-directory. The file is included here:

#include <stdio.h>
#include "cuda.h"

/*
 * Host function to initialize vector elements. This function
 * simply initializes each element to equal its index in the
 * vector.
 */

void initWith(float num, float *a, int N)
{
  for(int i = 0; i < N; ++i) {
    a[i] = num;
  }
}

/*
 * Device kernel stores into 'result' the sum of each
 * same-indexed value of 'a' and 'b'.
 */

__global__
void addVectorsInto(float *result, float *a, float *b, int N)
{
  int index = threadIdx.x + blockIdx.x * blockDim.x;
  int stride = blockDim.x * gridDim.x;

  for(int i = index; i < N; i += stride) {
    result[i] = a[i] + b[i];
  }
}

/*
 * Host function to confirm values in 'vector'. This function
 * assumes all values are the same 'target' value.
 */

void checkElementsAre(float target, float *vector, int N)
{
  for(int i = 0; i < N; i++) {
    if(vector[i] != target)
    {
      printf("FAIL: vector[%d] - %0.0f does not equal %0.0f\n", i, vector[i], target);
      exit(1);
    }
  }
  printf("Success! All values calculated correctly.\n");
}

int main()
{
  const int N = 2<<24;
  size_t size = N * sizeof(float);

  float *a, *b, *c;

  cudaMallocManaged(&a, size);
  cudaMallocManaged(&b, size);
  cudaMallocManaged(&c, size);

  initWith(3, a, N);
  initWith(4, b, N);
  initWith(0, c, N);

  size_t threadsPerBlock;
  size_t numberOfBlocks;

  threadsPerBlock = 1;
  numberOfBlocks = 1;

  addVectorsInto<<<numberOfBlocks, threadsPerBlock>>>(c, a, b, N);

  cudaDeviceSynchronize();

  checkElementsAre(7, c, N);

  cudaFree(a);
  cudaFree(b);
  cudaFree(c);
}
In case you need to setup the environment, issue the same interactive reservation as before:

> si-gpu
$ module r cuda  # restores our saved 'cuda' modules
Or use the sbatch script presented at the beginning of this tutorial.

Profiling

We'll be using nvprof for this tutorial (documentation).

To profile your application simply:

$ nvprof ./a.out  # you can also add --log-file prof
The default output includes 2 sections:

one related to kernel and API calls
another related to memory.
Execution configuration

First, we look at the top part of the profiling result, related to function calls.

After profiling the application, can we answer the following questions:

What was the name of the only CUDA kernel called in this application?
How many times did this kernel run?
How long did it take this kernel to run? Record this time somewhere: you will be optimizing this application and will want to know how much faster you can make it.
Experiment with different values for the number of threads, keeping only 1 block.

Experiment with different values for both the number of threads and number of blocks.

The GPUs that CUDA applications run on have processing units called streaming multiprocessors, or SMs. During kernel execution, blocks of threads are given to SMs to execute. In order to support the GPU's ability to perform as many parallel operations as possible, performance often improves by choosing a grid size that has a number of blocks that is a multiple of the number of SMs on a given GPU.

Additionally, SMs create, manage, schedule, and execute groupings of 32 threads from within a block called warps. i A more in depth coverage of SMs and warps is beyond the scope of this course, however, it is important to know that performance gains can also be had by choosing a block size that has a number of threads that is a multiple of 32.

Unified Memory details

Now, we turn to the memory related performance counters.

You have been allocating memory intended for use either by host or device code with cudaMallocManaged and up until now have enjoyed the benefits of this method - automatic memory migration, ease of programming - without diving into the details of how the Unified Memory (UM) allocated by cudaMallocManaged actual works. nvprof provides details about UM management in accelerated applications, and using this information, in conjunction with a more-detailed understanding of how UM works, provides additional opportunities to optimize accelerated applications.

When Unified Memory is allocated, the memory is not resident yet on either the host or the device. When either the host or device attempts to access the memory, a page fault will occur, at which point the host or device will migrate the needed data in batches. Similarly, at any point when the CPU, or any GPU in the accelerated system, attempts to access memory not yet resident on it, page faults will occur and trigger its migration.

The ability to page fault and migrate memory on demand is helpful for ease of development in your accelerated applications. Additionally, when working with data that exhibits sparse access patterns, for example when it is impossible to know which data will be required to be worked on until the application actually runs, and for scenarios when data might be accessed by multiple GPU devices in an accelerated system with multiple GPUs, on-demand memory migration is remarkably beneficial. There are times - for example when data needs are known prior to runtime, and large contiguous blocks of memory are required - when the overhead of page faulting and migrating data on demand incurs an overhead cost that would be better avoided.

We'll be working with the sample code below (also in um.cu):

__global__
void deviceKernel(int *a, int N)
{
  int idx = threadIdx.x + blockIdx.x * blockDim.x;
  int stride = blockDim.x * gridDim.x;

  for (int i = idx; i < N; i += stride)
  {
    a[i] = 1;
  }
}

void hostFunction(int *a, int N)
{
  for (int i = 0; i < N; ++i)
  {
    a[i] = 1;
  }
}

int main()
{

  int N = 2<<24;
  size_t size = N * sizeof(int);
  int *a;
  cudaMallocManaged(&a, size);

  /*
   * Conduct experiments to learn more about the behavior of
   * `cudaMallocManaged`.
   *
   * What happens when unified memory is accessed only by the GPU?
   * What happens when unified memory is accessed only by the CPU?
   * What happens when unified memory is accessed first by the GPU then the CPU?
   * What happens when unified memory is accessed first by the CPU then the GPU?
   *
   * Hypothesize about UM behavior, page faulting specificially, before each
   * experiement, and then verify by running `nvprof`.
   */

  cudaFree(a);
}
For each of the 4 questions below, first hypothesize about what kind of page faulting should happen, then, edit the program to create a scenario that will allow you to test your hypothesis.

Be sure to record your hypotheses, as well as the results, obtained from nvprof output, specifically CPU and GPU page faults, for each of the 4 experiments you are conducting.

What happens when unified memory is accessed only by the CPU?
What happens when unified memory is accessed only by the GPU?
What happens when unified memory is accessed first by the CPU then the GPU?
What happens when unified memory is accessed first by the GPU then the CPU?
GPU-side initialization

With this in mind, refactor your vectoradd.cu program to instead be a CUDA kernel, initializing the allocated vector in parallel on the GPU. After successfully compiling and running the refactored application, but before profiling it, hypothesize about the following:

How do you expect the refactor to affect UM page-fault behavior?
How do you expect the refactor to affect the reported run time of addVectorsInto?
Once again, record the results.
File vectoradd2.cu is one implementation.

Asynchronous memory prefetching

A powerful technique to reduce the overhead of page faulting and on-demand memory migrations, both in host-to-device and device-to-host memory transfers, is called asynchronous memory prefetching. Using this technique allows programmers to asynchronously migrate unified memory (UM) to any CPU or GPU device in the system, in the background, prior to its use by application code. By doing this, GPU kernels and CPU function performance can be increased on account of reduced page fault and on-demand data migration overhead.

Prefetching also tends to migrate data in larger chunks, and therefore fewer trips, than on-demand migration. This makes it an excellent fit when data access needs are known before runtime, and when data access patterns are not sparse.

CUDA Makes asynchronously prefetching managed memory to either a GPU device or the CPU easy with its cudaMemPrefetchAsync function. Here is an example of using it to both prefetch data to the currently active GPU device, and then, to the CPU:

int deviceId;
cudaGetDevice(&deviceId);                                         // The ID of the currently active GPU device.

cudaMemPrefetchAsync(pointerToSomeUMData, size, deviceId);        // Prefetch to GPU device.
cudaMemPrefetchAsync(pointerToSomeUMData, size, cudaCpuDeviceId); // Prefetch to host. 'cudaCpuDeviceId' is a
                                                                  // built-in CUDA variable.
At this point, your vectoradd.cu program should be (a) launching a CUDA kernel to add 2 vectors into a third solution vector, all which are allocated with cudaMallocManaged, and (b) initializing each of the 3 vectors in parallel in a CUDA kernel.

Conduct 3 (or 4) experiments using cudaMemPrefetchAsync inside of your vectoradd.cu application to understand its impact on page-faulting and memory migration.

What happens when you prefetch one of the initialized vectors to the device?
What happens when you prefetch two of the initialized vectors to the device?
What happens when you prefetch all three of the initialized vectors to the device?
Hypothesize about UM behavior, page faulting specificially, as well as the impact on the reported run time of the initialization kernel, before each experiement, and then verify by running nvprof.

Image Convolution with GPU and CUDA

 Copyright (c) 2020-2021 L. Koutsantonis UL HPC Team <hpc-team@uni.lu>


This tutorial will cover the following aspects of CUDA programming:

GPU Global Memory Allocation
Dynamic Shared Memory Allocation
Thread Indexing
Thread Synchronization



Pre-requisites

Ensure you are able to connect to the UL HPC clusters. In particular, recall that the module command is not available on the access frontends.

### Access to ULHPC cluster - here iris
(laptop)$> ssh iris-cluster
# /!\ Advanced (but recommended) best-practice:
#    always work within an GNU Screen session named with 'screen -S <topic>' (Adapt accordingly)
# IIF not yet done, copy ULHPC .screenrc in your home
(access)$> cp /etc/dotfiles.d/screen/.screenrc ~/
Now you'll need to pull the latest changes in your working copy of the ULHPC/tutorials you should have cloned in ~/git/github.com/ULHPC/tutorials (see "preliminaries" tutorial)

(access)$> cd ~/git/github.com/ULHPC/tutorials
(access)$> git pull
You should have followed the Introduction to GPU programming with CUDA

Laplacian of Gaussian (LoG): A convolution kernel for edge detection

Derivative Filter used to find rapid changes in signals and especially images
Used for edge detection and noise detection
Mathematical Formula:
H(x,y)=−1πσ4(1−x2+y22σ2)e−x2+y22σ2
Original Image

Convoluted Image




Convolution Operator

The discrete convolution operator is define by the double sum:

Gm,n=F∗H=∑i∑jFm−i,n−jHi,j
where F is the original image, H is the convolution kernel and G is the resulted image. Convoluted Image




CPU Implementation

A serial code implementing the image convolution on a CPU employs two loops to compute the values of the pixels of the output image. The convolution operator is calculated at each iteration for each image pixel using the double sum provided in the equation above.

//CPU function: conv_img_cpu
//Parameters: float *img, float *kernel, float *imgf, int Nx, int Ny, int kernel_size
//center: center of kernel
  for (int i = center; i<(Ny-center); i++)
    for (int j = center; j<(Nx-center); j++){
        //Convolution Operator:
        sum = 0;
        for (int ki = 0; ki<kernel_size; ki++)
           for (int kj = 0; kj<kernel_size; kj++){
               ii = j + kj - center;
               jj = i + ki - center;
               sum+=img[jj*Nx+ii]*kernel[ki*kernel_size + kj];
               }
        imgf[i*Nx +j] = sum;
        }



GPU Implementation

The parallel implementation of convolution of GPU is described by the following figure. Multiple threads are used to calculate the convolution operator of multiple pixels simultaneously. The total number of calculated pixels at each step will be equal to the total number of launched threads (NumberofBlocks×BlockThreads). Each thread having access to the coefficients of the convolution kernel calculates the double sum of the convolution operator. The kernel coefficients being constant during the whole execution can be stored into the shared memory (accessible by the block threads).

Convoluted Image




Hands On Image Convolution with CUDA

Get the source files

Task 1: If you do not have yet the UL HPC tutorial repository, clone it. Update to the latest version.

ssh iris-cluster
mkdir -p ~/git/github.com/ULHPC
cd  ~/git/github.com/ULHPC
git clone https://github.com/ULHPC/tutorials.git
cd tutorials/cuda/exercises/convolution
git stash && git pull -r && git stash pop


Get an interactive GPU job

See also documentation

### ... either directly - dedicate 1/4 of available cores to the management of GPU card
$> si-gpu -c7
# /!\ warning: append -G 1 to really reserve a GPU
# salloc -p interactive --qos debug -C gpu -c7 -G 1 --mem-per-cpu 27000

### ... or using the HPC School reservation 'hpcschool-gpu'
salloc --reservation=hpcschool-gpu -p interactive -C gpu --ntasks-per-node 1 -c7 -G 1

### Load the required modules
module load system/CUDA
module load compiler/GCC
A CUDA kernel for the Convolution Operator

Task 2: Following the steps 1 to 3 provided bellow write a CUDA kernel for the computation of the convolution operator.

Open the source file LoG_gpu_exercise.cu with your favorite editor (e.g. emacs LoG_gpu_exercise.cu). The CUDA kernel is already defined:

void conv_img_cpu(float *img, float *kernel, float *imgf, int Nx, int Ny, int kernel_size)
where *img is a pointer to the original image vector, *kernel is a pointer to the convolution kernel vector, *imgf is a pointer to the convoluted image, Nx and Ny are the dimensions of both the original and convoluted image, and kernel_size is the dimension of the convolution kernel.

Step 1. CUDA Threads and Blocks indices

The CUDA kernel will be executed by each thread. Thus, a mapping mechanism is needed for each thread to compute a specific pixel of the output image and store the result to the corresponding memory location.

CUDA kernels have access to device variables identifying both the thread index within the block and the block index. These variables are threadIdx.x and blockIdx.x respectively. In this example, each block of threads will compute a row of the output image and each block thread will compute a single pixel value on this row. Thus, the index of a pixel in the image can be defined throught:

//each block is assigned to a row of an image, iy integer index of y
  int iy = blockIdx.x + (kernel_size - 1)/2;

  //each thread is assigned to a pixel of a row, ix integer index of x
  int ix = threadIdx.x + (kernel_size - 1)/2;
The offset (kernel_size - 1)/2 is added to the iy, ix variables as the convolution will not be computed for the image pixels lying at the boundary layers of the original image (computations are performed only when the discrete filter kernel lies completely within the original image). We can define the center of the convolution kernel as it will be used in differnet calculations:

//center of kernel in both dimensions
  int center = (kernel_size -1)/2;
It is important to say that the kernel_size must be an odd number so that its center has an integer value.

For each block thread, the memory location of the corresponding pixel can be calculated by:

int idx = iy*Nx +ix;
Step 2. Dynamically Allocated Shared Memory

Shared memory is allocated per thread block, so all threads in the block have access to the same shared memory. The best practice is to use the shared memory for parameters that remain constant during the execution of the CUDA kernel and used in multiple calculations. In our example, these parameters are the coefficient of the convolution kernel. To statically allocate shared memory we use:

__shared__ float sdata[9];
where in this case the kernel_size is 9. As we want our kernel to run with convolution kernels of different size, we can dynamically allocate shared memory using the prefix extern:

extern __shared__ float sdata[];
where in this case, the size of the allocated shared memory is passed through the launch of the kernel (from the main function).

For each block, the block threads are used to copy the coefficients of the convolution kernel from the global memory to the shared memory. The index of each thread is used to define the location in shared memory where the convolution coefficients from the global memory will be copied. It also provides the location where these coefficients are stored in the global memory. For our case, these mappings are identical:

int tid = threadIdx.x;
int K2 = kernel_size*kernel_size;
extern __shared__ float sdata[];
if (tid<K2)
    sdata[tid] = kernel[tid];
 __syncthreads();
As the vector containing the convolution coefficients is of size of K2 = kernel_size*kernel_size, only the threads with index in the range[0 ,K2) can access the global and shared memory. This is ensured using the if (tid<K2) statement. At the end of the transfer we have to syncronize all block threads using ___syncthreads(). This synchronization is required to ensure that all of the block threads will perform calculations after all convolution coefficients are completely transfered to the shared memory.

Step 3. Calculate the output image

Each thread calculates the corresponding pixel value of the convoluted image. The convolution operation is performed through a nested loop implementing a double summation. At each iteration, each block thread calculates the multiplication of a pixel value of the original image lying within the convolution window with the corresponding coefficient of the convolution kernel stored in shared memory (see image above). The result is used to update the value of a local summation (sum). The value of the output pixel is the result of the double summation. The CUDA code implementing the above procedure is given bellow:

if (idx<Nx*Ny){
    for (int ki = 0; ki<kernel_size; ki++)
        for (int kj = 0; kj<kernel_size; kj++){
           ii = kj + ix - center;
           jj = ki + iy - center;
    sum+=img[jj*Nx+ii]*sdata[ki*kernel_size + kj];
    }
    imgf[idx] = sum;
  }
As you can see the code is exactly the same with the code implementing the convolution on a CPU. What is missing is the double for loop running on the pixels of the output image.

In the above code, the if (idx<Nx*Ny) statement is used to ensure that each thread has access to an allocated memory location.



Complete the main function

In main, the arrays img, imgf and kernel storing the original images pixel values, the convoluted image pixel values and the convolution kernel coefficients respectively are float arrays allocated in the host memory (CPU). The original image is preloaded into the img through the host function:

void load_image(char *fname, int Nx, int Ny, float  *img)
The convolution kernel coefficients are calculated for a given sigma value  sigma and convolution kernel size kernel_size through the host function:

void calculate_kernel(int kernel_size, float sigma, float *kernel)


Allocate Device Memory

The array d_img storing the original image pixel values in device global memory is allocated using:

cudaMalloc(&d_img,Nx*Ny*sizeof(float));
Task 3a Based on the above instruction allocate device memory for the arrays d_imgf and d_kernel storing the output image pixels and convolution kernel coeffients respectively. The size of the output image in Nx \times Ny and the size of the convolution kernel is kernel_size \times kernel_size.

Transfer the filter coefficients from the host memory to the device memory

The original image pixel values are copied from the host memory to the device memory through:

cudaMemcpy(d_img, img, Nx*Ny*sizeof(float),cudaMemcpyHostToDevice);
Task 3b Utilize the above instruction to tranfer the convolution kernel coefficients from the host memory kernel to the global memory d_kernel.

Configure the Execution and Launch the CUDA kernel

The kernel code will by executed by every thread after its launch. The total number of blocks Nblocks will be equal to the number of the calculated rows of the output image (Ny - kernel_size + 1) and the number of block threads Nthreads equal to the number of the calculated pixel elements per row (Nx - kernel size + 1). In general, a CUDA kernel is configured and launched using the following instruction:

my_gpu_kernel<<<Nblocks, Nthreads>>>(vars);
When shared memory is dynamically allocated within the kernel, an extra argument size defining the number of bytes of the shared memory that is dynamically allocated per thread block has to be passed through the configuration of the kernel:

conv_img_gpu<<<Nblocks, Nthreads, size>>>(d_img, d_kernel, d_imgf, Nx, Ny, kernel_size);
In our case, the size of the allocated shared memory is equal to the size of the convolution kernel Size =  kernel_size*kernel_size*sizeof(float).

Task 4 Configure the Execution and Launch the GPU kernel from the main function.

De-allocation of host and device memory

At the end of main, the host memory is deallocated with free(var). Similarly, the device memory is deallocated with cudaFree(var).

  free(img);
  free(imgf);
  free(kernel);

  cudaFree(d_img);
  cudaFree(d_imgf);
  cudaFree(d_kernel);



Compile and run your code

The NVIDIA CUDA compiler 'nvcc' is used to compile the source code containing both the host and device functions. The non CUDA part of the code will be forwarded to a general purpose host compiler (e.g. gcc). As you have seen, the GPU functionsare declared using some annotations (e.g. __global__, __device__) distinguishing them from the host code.

In simple words, you can compile your code using:

nvcc -arch=compute_70 -o ./$exe $src
where nvcc is the keyword for the nvcc compiler, $src is the name of the source file to compile ( e.g.LoG_gpu.cu is passed as the source file to compile, the o flag is used to specify the name $exe of the compiled program, and the arch points to the GPU architecture for which the source file must be compiled. sm_70 indicates the Volta GPU architecture. Use -lm to link the math library to your executable.

To run your executable file interactively, just use:

./$exe $arg1 $arg2 ... $argn
where $arg1, $arg2, ..., $argn are the appropriate arguments (if any).




Additional Exercises

Profiling

You can use  nvprof (documentation) to profile your GPU application.

To profile your application simply:

nvprof ./$exe  # you can also add --log-file prof


Experimentation with convolution parameters

Try to change the parameters sigma and kernel_size in your main function. Try to use a large enough convolution kernel size. Compile the modified source code and use nvprof to profile your application. Do you observe any difference in the execution time of the GPU kernel?

Try to implement the GPU kernel without using the shared memory. In this case the CUDA kernel is implemented as follows:


if (idx<Nx*Ny){
    for (int ki = 0; ki<kernel_size; ki++)
        for (int kj = 0; kj<kernel_size; kj++){
           ii = kj + ix - center;
           jj = ki + iy - center;
    sum+=img[jj*Nx+ii]*kernel[ki*kernel_size + kj];
    }
    imgf[idx] = sum;
  }
Again, recompile your source file and profile your application. Can you observe any difference in the execution time of the GPU kernel?



Plotting your convoluted image

A jupyter notebook show_images.ipynb is available for plotting your results. You can access the notebook directly from the iris-cluster by building a virtual environment for the jupyter notebook.

First connect to the iris-cluster with a SOCK proxy opened on port 1080:

ssh -D 1080 iris-cluster
Reserve a single node interactively:

si-gpu
Prepare virtual environment for the notebook:

module load lang/Python/3.7.2-GCCcore-8.2.0
python -m venv ~/venv/jupyter
source ~/venv/jupyter/bin/activate
pip install -U pip
pip install jupyter
python -m ipykernel install --user --name=jupiter
pip install matplotlib
pip install numpy
Generate your jupyter notebook:

jupyter notebook --generate-config
You can protect your notebook with a password (not required):

jupyter notebook password
Launch your notebook:

jupyter notebook --ip $(ip addr | grep '172.17' | grep 'inet ' | awk '{print $2}' | cut -d/ -f1) --no-browser
The IP address of the node launcing the notebook is shown:

[]> Jupyter Notebook 6.1.5 is running at:
[]> http://172.17.6.196:8888/
On your local machine, you can access the notebook using an SSH tunnel. Open a terminal and type:

ssh -NL 8000:$IP iris-cluster
where $IP is the IP address of the node where the jupyter notebook was launched.

Open a browser and access your jupyter notebook on your localhost: http://127.0.0.1:8000. Run the python script to reproduce the results of your CUDA application.

Introduction to OpenCL Programming (C/C++)

 Copyright (c) T. Carneiro, L. Koutsantonis, 2021 UL HPC Team <hpc-team@uni.lu>


In the HPC school, the students had the opportunity to work with a higher-level heterogeneous programming model based on directives, called OpenACC. In this programming model, the parallelism is implicit, which means that the compiler is responsible for the parallelization, which might not work in all scenarios. However, OpenACC is a proprietary parallel programming model and it is supported by a limited set of devices, such as NVIDIA GPUs.

What's OpenCL?

OpenCL came as a standard for heterogeneous programming that enables a code to run in different platforms, such as multicore CPUs, GPUs (AMD, Intel, ARM), FPGAs, Apple M1, tensor cores, and ARM processors with minor or no modifications.

Furthermore, differently from OpenACC, the programmer has full control of the hardware and is entirely responsible for the parallelization process.

However, this portability has a cost, that’s the reason why OpenCL exposes the programmer to a much lower level compared to OpenACC or even CUDA.

OpenCL's target audience:

The target audience of OpenCL consists of programmers that aim at programming portable heterogeneous code and that want full control of the parallelization process.

In this introductory tutorial, we teach how to perform the sum of two vectors C=A+B on the OpenCL device and how to retrieve the results from the device memory.

Objectives of this tutorial:

The main objective of this tutorial is to introduce for students of the HPC school the heterogeneous programming standard - OpenCL. A secondary objective is to show what is behind the higher-level heterogeneous programming libraries, so it is possible to understand how they work.

This tutorial covers the following aspects of OpenCL programming:

Check for OpenCL-capable device(s);
Memory allocation on the device;
Data transfer to the device;
Retrieve data from the device;
Compile C/C++ programs that launch OpenCL kernels.
References:

This tutorial is based on the following content from the Internet:

Tutorial: Simple start with OpenCL and C++
Khronos OpenCL Working Group. The OpenCL Specification (Oct. 2021)
Smistad, E. Getting started with OpenCL and GPU Computing, Feb. 22, 2018 (Access on Oct. 28, 2021).
Mattson, T., McIntosh-Smith, S., Koniges, A. OpenCL: a Hands-on Introduction
Programming Pre-requisites:

C/C++ language and, due to the lower level of OpenCL,
(preferable) knowledge in other frameworks for heterogeneous programming, such as CUDA, OpenACC or SYCL.
Connecting to a GPU-enable node on the Iris Cluster

Ensure you can connect to the UL HPC clusters. In particular, recall that the module command is not available on the access frontends.

Access to the ULHPC iris cluster (here it is the only one featuring GPU nodes):

(laptop)$>  ssh iris-cluster 
Now you'll need to pull the latest changes in your working copy of the ULHPC/tutorials you should have cloned in ~/git/github.com/ULHPC/tutorials (see "preliminaries" tutorial)

(access)$> cd ~/git/github.com/ULHPC/tutorials
(access)$> git pull
Accessing a GPU-equipped node of the Iris cluster

This practical session assumes that you reserve a node on iris with one GPU for interactive development. See documentation

### Have an interactive GPU job
# ... either directly
(access)$> si-gpu
# ... or using the HPC School reservation 'hpcschool-gpu' if needed  - use 'sinfo -T' to check if active and its name
# (access)$> si-gpu --reservation=hpcschool-gpu
$ nvidia-smi
Driver is loaded, but we still need to load the CUDA development kit from the latest software set.

$ resif-load-swset-devel
$ module load system/ULHPC-gpu/2020b
Verifying the OpenCL Installation

First of all, it is necessary to verify if there is an OpenCL-capable device correctly installed by using the clinfo command. This command is not available on the Iris cluster.

If OpenCL is correctly installed, the output of clinfo should return the number of OpenCL-capable devices, the OpenCL version and the name(s) of the device(s), as one can see below.

This is an image

In this example of output, the OpenCL 3.0 version from the CUDA toolkit is installed. Moreover, there is one OpenCL-capable device, a NVIDIA Quadro RTX 5000 GPU.

Refer to the code folder for the complete example.

The OpenCL platform model

The platform model of OpenCL is similar to the one of the CUDA programming model. In short, according to the OpenCL Specification, "The model consists of a host (usually the CPU) connected to one or more OpenCL devices (e.g., GPUs, FPGAs). An OpenCL device is divided into one or more compute units (CUs) which are further divided into one or more processing elements (PEs). Computations on a device occur within the processing element"

An OpenCL program consists of two parts: host code and device code. As the name suggests, the host code is executed by the host and also "submits the kernel code as commands from the host to OpenCL devices".

Finally, such as in the CUDA programming model, the host communicates with the device(s) through the global memory of the device(s). As in the CUDA programming model, there is a memory hierarchy on the device. However, we have omitted these details for the sake of greater simplicity.

Writing a First OpenCL Program

In this tutorial, we will learn how to perform C = A+B in OpenCL.

The initial step is to add the OpenCL headers to the code.

Headers:

For the sake of greater readability, the examples are written using the C++ bindings of OpenCL (<CL/cl.hpp>).

#define CL_USE_DEPRECATED_OPENCL_2_0_APIS
#include <CL/cl.hpp>
Verifying the installed OpenCL platforms and setting up a device

One of the key features of OpenCL is its portability. So, for instance, there might be situations in which both the CPU and the GPU can run OpenCL code. Thus, a good practice is to verify the OpenCL platforms (cl::Platform) to choose on which the compiled code will run.


    cl::Platform default_platform = all_platforms[0];
    std::cout << "Using platform: " <<default_platform.getInfo<CL_PLATFORM_NAME>() << "\n";

An OpenCL platform might have several devices. The next step is to ensure that the code will run on the first device (device 0) of the platform, if found.

  std::vector<cl::Device> all_devices;
    default_platform.getDevices(CL_DEVICE_TYPE_ALL, &all_devices);
    if (all_devices.size() == 0) {
        std::cout << " No devices found.\n";
        exit(1);
    }

    cl::Device default_device = all_devices[0];
    std::cout << "Using device: " << default_device.getInfo<CL_DEVICE_NAME>() << "\n";

The OpenCL Context

According to the OpenCL Parallel Programming Development Cookbook, "contexts are used by the OpenCL runtime for managing objects such as command queues (the object that allows you to send commands to the device), memory, program, and kernel objects, and for executing kernels on one or more devices specified in the context".

    cl::Context context({default_device});
Now we need to define the code object which is going to be executed on the device, called kernel.

   cl::Program::Sources sources;
Allocating Memory on the Device

The host communicates with the device by using its global memory. However, it is required, first, to allocate, on the host portion of the code, to allocate memory on the device. In OpenCL, the memory allocated on the device is called Buffer.

In our example, we have to host vectors, A and B of SIZE=10.

int A_h[] = { 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 };
int B_h[] = { 10, 9, 8, 7, 6, 5, 4, 3, 2, 1 };
The next step is to allocate two regions on the device memory for A_h, B_h and C_h. It is a good practice to name the variables on the GPU with a suffix _d and _h on the host. This way, A_h is the version of the vector on the host and A_d its copy on the device.

cl::Buffer A_d(context, CL_MEM_READ_WRITE, sizeof(int) * SIZE);
In this example, buffer A_d is connected to the context, and it has the size of SIZE elements of four bytes (sizeof(int)).

A buffer can be of several types. In this tutorial, we focus on CL_MEM_READ_WRITE and CL_MEM_READ_ONLY. These flags say the actions will perform in the buffer.

In this tutorial, it is necessary to create three buffers: one CL_MEM_READ_ONLY for vector A, one CL_MEM_READ_ONLY for vector B, and a CL_MEM_WRITE_ONLY for vector C.

Creating a Queue

In OpenCL, it is required to create a queue to push commands onto the device. For those who program in CUDA, OpenCL queues are similar CUDA streams.

cl::CommandQueue queue(context, default_device);
Writing into the Device Memory

Once the CommandQueue queue is created, it is possible to execute commands on the device side.

Using the queue, connected to the context and the device default_device, it is possible to initialize the vectors A_d and B_d with the values from A_h and B_h.

queue.enqueueWriteBuffer(buffer_A, CL_TRUE, 0, sizeof(int) * SIZE, A_h);
queue.enqueueWriteBuffer(buffer_B, CL_TRUE, 0, sizeof(int) * SIZE, B_h);
Pay attention that the function is writing from the host to the buffer: enqueueWriteBuffer.

It is not required to initialize vector C as it will receive the values of A+B.

Building the OpenCL Kernel

In OpenCL, there are several ways to build the kernel function and enqueue its execution on the device. Unfortunately, to the best of our knowledge, these ways are lower level than CUDA, for which one just needs to define the block size, number of threads and call the kernel as a function.

In this tutorial, we present a way that programmers familiarized with CUDA might understand. In OpenCL, for the sake of higher portability, the kernel function is presented as a string, which is appended to the program in the runtime, as one can see in the code below.

The kernel simple_add is a substring of the global variable of type std::string kernel_code.

std::string kernel_code =
    "   void kernel simple_add(global const int* A, global const int* B, global int* C){ "
    "       C[get_global_id(0)]=A[get_global_id(0)]+B[get_global_id(0)];                 "
    "   } 

In OpenCL, kernel functions must return void, and the global keyword means that the variable points to the global memory.

Organizing the source code for compilation

Initially, it is required to append the kernel, which is presented here as a string, to the OpenCL source code.

  cl::Program::Sources sources;
  sources.push_back({ kernel_code.c_str(),kernel_code.length() });
Then, we create an OpenCL program, linking the OpenCL source code to the context.

  cl::Program program(context, sources);
Then, the OpenCL code is ready to be compiled in execution time


if (program.build({ default_device }) != CL_SUCCESS) {
        std::cout << " Error building: " << program.getBuildInfo<CL_PROGRAM_BUILD_LOG>(default_device) << "\n";
        exit(1);
    }

It is important to point out that compilation errors are found, in execution time at this point. If compilation errors are found, the program output the message Error building, and, then, outputs the compilation errors.

It is worth to mention that there are other ways to define a kernel in OpenCL. Furthermore, it is also possible to perform offline compilation of OpenCL kernels.

Launching the kernel on the device

From the program, which contains the simple_add kernel, create a kernel for execution with three cl:buffers as arguments.


    cl::make_kernel<cl::Buffer, cl::Buffer, cl::Buffer> simple_add(cl::Kernel(program, "simple_add"));
    cl::NDRange global(SIZE);
    simple_add(cl::EnqueueArgs(queue, global), A_d, B_d, C_d).wait();

Next, it is possible to call simple_add as a function. However, note that in cl::EnqueueArgs(queue, global) the queue queue is passed and cl::NDRange global(SIZE) is the number of threads the execution is going to spawn on the device. The remaining arguments are the three buffers on the global memory of the device.

Kernel execution on the device

The kernel is a simple function that performs C[i]=A[i]+B[i]. In this case, OpenCL provides the function get_global_id(0), which returns the id i of the thread in a 1D organization.

Retrieving data from the device

In this example, it is only required to retrieve data from C_d to C_h.

queue.enqueueReadBuffer(C_d, CL_TRUE, 0, sizeof(int) * SIZE, C_h);
In the line above, we read, from the buffer  C_d, sizeof(int) * SIZE bytes using the enqueueReadBuffer function.

Compiling an OpenCL code:

It is simple to compile an OpenCL code with gcc or g++. In short, it is only required to add -lOpenCL flag to the compilation command.

If it is not possible to find the OpenCL library, it is required, first, to locate libOpenCL and append the directory to the LD_LIBRARY_PATH or explicitly indicate the location of libOpenCL in the compilation commands.

g++ -lOpenCL exercise1.cpp
Exercise 1: whats the output of exercise1.cpp on the Iris cluster? Is it using the GPUs by Default?

On the Iris cluster, if the programmer compiles the code by only adding the -lOpenCL flag, the compiler uses the Portable Computing Language (PoCL) implementation of OpenCL that runs on the CPU.

So, in the output, this first example, the platform is the Portable Computing Language the device is pthread-Intel(R) Xeon(R) Gold 6132 CPU @ 2.60GHz. For the purpose of this tutorial, it is OK to use the PCL implementation of OpenCL.

If one wants to program the example for the NVIDIA GPUs, it is required to manually point the include and the library directories for the compiler:

g++ -I/mnt/irisgpfs/apps/resif/iris/2019b/gpu/software/CUDA/10.1.243-GCC-8.3.0/include -L/mnt/irisgpfs/apps/resif/iris/2019b/gpu/software/CUDA/10.1.243-GCC-8.3.0/lib64 -lOpenCL exercise1.cpp
Here /mnt/… is the directory in which the module has been loaded.

Exercise 2: compile the code using the NVIDIA implementation of OpenCL.

Compilation errors

For the sake of higher portability, the OpenCL code is compiled in execution time. For instance, in the kernel, adding AAA to the const keyword returns the following compiler error in execution time:

Error building: <kernel>:1:34: error: unknown type name 'constAAA'
   void kernel simple_add(global constAAA int* A, global const int* B, global int* C){        C[get_global_id(0)]=A[get_global_id(0)]+B[get_global_id(0)];                    }                                                                               
...
It is important to point out that this runtime error was returned using the NVIDIA implementation of OpenCL 3.0.

Exercise 3: D=A+B+C

Extend the previous example so a vector D receives the sum of three other vectors, A,B, and C.

Refer to the code folder for the solution.

Introduction to OpenACC Programming Model (C/C++ and Fortran)

 Copyright (c) E. Krishnasamy, 2013-2023 UL HPC Team <hpc-sysadmins@uni.lu>


Objectives

Understanding the OpenACC programming model
How to use some of the directives from OpenACC to parallelize the code
compute constructs, loop constructs, data clauses
Implementing OpenACC parallel strategy in C/C++ and FORTRAN programming languages
Simple mathematical examples to support and understand the OpenACC programming model
Finally, show you how to run these examples using Iris cluster (ULHPC) - both interactively and using a batch job script
Prerequisite:

C/C++ and/or FORTRAN languages
OpenMP or some basic parallel programming concept (advantage not necessary)
Important: PRACE MOOC

NOTE: This lecture is limited to just 45 min; it only a covers from basics to intermediate tutorial about OpenACC. To know more about (from basic to advanced) CUDA programming and OpenACC programming model, please refer to PRACE MOOC GPU Programming for Scientific Computing and Beyond - Dr. Ezhilmathi Krishnasamy and Prof. Pascal Bouvry

Pre-requisites

Ensure you are able to connect to the UL HPC clusters. In particular, recall that the module command is not available on the access frontends. For all tests and compilation, you MUST work on a computing node

Now you'll need to pull the latest changes in your working copy of the ULHPC/tutorials you should have cloned in ~/git/github.com/ULHPC/tutorials (see "preliminaries" tutorial)

(access)$ cd ~/git/github.com/ULHPC/tutorials
(access)$ git pull
Now configure a dedicated directory ~/tutorials/openacc for this session

# return to your home
(access)$ mkdir -p ~/tutorials/openacc
(access)$ cd ~/tutorials/openacc
# create a symbolic link to the top reference material
(access)$ ln -s ~/git/github.com/ULHPC/tutorials/gpu/openacc/basics ref.d # Symlink to the reference tutorial material
# copy / synchronize a copy of the exercises
(access)$ rsync -avzu ref.d/exercises .   # DO NOT forget the trailing .
Advanced users (eventually yet strongly recommended), create a Tmux session (see Tmux cheat sheet and tutorial) or GNU Screen session you can recover later. See also "Getting Started" tutorial .

Connect to Iris cluster and get an interactive GPU job

See also GPU jobs documentation.

$ ssh iris-cluster   # The only cluster featuring GPU
$ si-gpu -G 1 --ntasks-per-node 1 -c 7 -t 00:30:00   # (eventually) --reservation=hpcschool-gpu
$ si-gpu -G 1 --reservation=hpcschool-gpu -N 1 -c 14 -t 01:00:00 
Load the OpenACC compiler (Nvidia HPC SDK)

$> module spider nvhpc
$> module load compiler/NVHPC/21.2
Difference between CPU and GPU

CPU vs GPU

A CPU frequency is higher compared to a GPU.
But a GPU can run many threads in parallel compared to a CPU.
On a GPU, the cores are grouped and called "Streaming Multiprocessor - SM".
Even the Nvidia GPU has a "Tensor Process Unit - TPU" to handle the AI/ML computations in an optimized way.
GPUs are based on the "Single Instruction Multiple Threads".
Threads are executed in a group on the GPU; typically they have 32 threads. This is called "warps" on the Nvidia GPU and "wavefronts" on the AMD GPU.
CPU vs GPU



Thread Hierarchy

 

How GPUs are used for computations

Step 1: application preparation, initialize the memories on both CPU and GPU
Step 2: transfer the data to a GPU
Step 3: do the computation on a GPU
Step 4: transfer the data back to a CPU
Step 5: finalize the application and delete the memories on both CPU and GPU


OpenACC

Few points about OpenACC

OpenACC is not GPU programming
OpenACC is expressing the parallelism in your existing code
OpenACC can be used in both Nvidia and AMD GPUs
“OpenACC will enable programmers to easily develop portable applications that maximize the performance and power efficiency benefits of the hybrid CPU/GPU architecture of Titan.”
Buddy Bland, Titan Project Director, Oak Ridge National Lab
“OpenACC is a technically impressive initiative brought together by members of the OpenMP Working Group on Accelerators, as well as many others. We look forward to releasing a version of this proposal in the next release of OpenMP.”
Michael Wong, CEO of OpenMP Directives Board
Ways to accelerate applications on the GPU



Ways to accelerate applications on the GPU

Libraries: easy to use with very limited knowledge with GPU programming
cuBLAS, cuFFT, CUDA Math Library, etc.
Directive based programming model: will accelerate the application with using directives in the existing code
OpenACC and OpenMP (might be applicable in the future)
Programming languages: low level programming languages that will further optimize the application on the accelerator
CUDA, OpenCL, etc.
Compilers and directives (only a few of them are listed here)

OpenACC is supported by the Nvidia, PGI, GCC, and HPE Gray (only for FORTRAN) compilers
Now PGI is part of Nvidia, and it is available through Nvidia HPC SDK
Compute constructs:
parallel and kernel
Loop constructs:
loop, collapse, gang, worker, vector, etc.
Data management clauses:
copy, create, copyin, copyout, delete and present
Others:
reduction, atomic, cache, etc.
More information about the OpenACC directives can be found in here
Basic programming structure

// C/C++
#include "openacc.h"
#pragma acc <directive> [clauses [[,] clause] . . .] new-line
<code>
!! Fortran
use openacc
!$acc <directive> [clauses [[,] clause] . . .]
<code>
Compute and loop constructs in OpenACC

kernels in C/C++

// Hello_World.c                    | // Hello_World_OpenACC.c
void Print_Hello_World()            | void Print_Hello_World()
{                                   | {
                                    | #pragma acc kernels
  for(int i = 0; i < 5; i++)        |    for(int i = 0; i < 5; i++)
    {                               |      {
      printf("Hello World!\n");     |        printf("Hello World!\n");
    }                               |      }
}                                   | }
compilation: nvc -fast -Minfo=all -acc=gpu -gpu=cc70 Hello_World.c
The compiler will already give much info; what do you see?
$> nvc -fast -Minfo=all -acc=gpu -gpu=cc70 Hello_World.c
Print_Hello_World:
      6, Loop not parallelized: contains call
main:
     14, Print_Hello_World inlined, size=4 (inline) file Hello_World.c (5)
           6, Loop not vectorized/parallelized: contains call
Now add either kernels or parallel directives to vectorize/parallelize the loop
$> nvc -fast -Minfo=all -acc=gpu -gpu=cc70 Hello_World_OpenACC.c

print_hello_world:
      6, Loop is parallelizable
         Generating Tesla code
          6, !$acc loop gang, vector(32) ! blockidx%x threadidx%x

As we can see above the loop is parallelized!.
kernels in FORTRAN

!! Hello_World.f90                 | !! Hello_World_OpenACC.f90
subroutine Print_Hello_World()     | subroutine Print_Hello_World()
  integer :: i                     |   integer :: i
                                   |   !$acc kernels
  do i = 1, 5                      |   do i = 1, 5
     print *, "hello world"        |     print *, "hello world"
  end do                           |   end do
                                   |  !$acc end kernels
end subroutine Print_Hello_World   | end subroutine Print_Hello_World
Compile the Hello_World.f90 and compiler tells us that the loop is not parallelized.
$> nvfortran -fast -Minfo=all -acc=gpu -gpu=cc70 Hello_World.f90

print_hello_world:
      5, Loop not vectorized/parallelized: contains call
Now run the Hello_World_OpenACC.f90 either using kernels or parallel and we can already notice that loop is vectorized/parallelized.
$> nvfortran -fast -Minfo=all -acc=gpu -gpu=cc70 Hello_World_OpenACC.f90

print_hello_world:
      6, Loop is parallelizable
         Generating Tesla code
          6, !$acc loop gang, vector(32) ! blockidx%x threadidx%x
Note: this above example shows you how to create the parallel region using parallel and kernels. It is quite useful when multiple regions need to be parallelized. However, the above example has just one parallel region.

loop and data management clauses

for C/C++

Here we can consider simple vector addition example for the OpenACC loop directive


// Vector_Addition.c                  | // Vector_Addition_OpenACC.c
float * Vector_Addition               | float * Vector_Addition
(float *restrict a, float *restrict b,| (float *restrict a, float *restrict b, 
float *restrict c, int n)             | float *restrict c, int n)
{                                     | {
                                      | #pragma acc kernels loop
                                      | copyin(a[:n], b[0:n]) copyout(c[0:n])
  for(int i = 0; i < n; i ++)         |   for(int i = 0; i < n; i ++)
    {                                 |     {
      c[i] = a[i] + b[i];             |       c[i] = a[i] + b[i];
    }                                 |     }
  return c;                           |
}                                     | }
The loop will parallelize the for loop plus also accommodate other OpenACC clauses, for example here copyin and copyout.
The above example needs two vectors to be copied to GPU and one vector needs to send the value back to CPU.
copyin will create the memory on the GPU and transfer the data from CPU to GPU.
copyout will create the memory on the GPU and transfer the data from GPU to CPU.
for FORTRAN


!! Vector_Addition.f90                       | !! Vector_Addition_OpenACC.f90
module Vector_Addition_Mod                   | module Vector_Addition_Mod
  implicit none                              |   implicit none
contains                                     | contains
  subroutine Vector_Addition(a, b, c, n)     |   subroutine Vector_Addition(a, b, c, n)
    !! Input vectors                         |     !! Input vectors
    real(8), intent(in), dimension(:) :: a   |     real(8), intent(in), dimension(:) :: a
    real(8), intent(in), dimension(:) :: b   |     real(8), intent(in), dimension(:) :: b
    real(8), intent(out), dimension(:) :: c  |     real(8), intent(out), dimension(:) :: c
    integer :: i, n                          |     integer :: i, n
                                             |     !$acc kernels loop copyin(a(1:n), b(1:n))
                                             |      copyout(c(1:n))
    do i = 1, n                              |     do i = 1, n
       c(i) = a(i) + b(i)                    |        c(i) = a(i) + b(i)
    end do                                   |     end do
                                             |     !$acc end kernels
  end subroutine Vector_Addition             |   end subroutine Vector_Addition
end module Vector_Addition_Mod               | end module Vector_Addition_Mod
Now compile and run the above code as we did previously.
reduction clause in dot product



for C/C++

// Matrix_Multiplication.c
for(int row = 0; row < width ; ++row)           // Matrix_Multiplication_OpenACC.c
    {                                           | pragma acc kernels loop collapse(2)                      
      for(int col = 0; col < width ; ++col)     |                        reduction(+:sum)                      
        {                                       |   for(int row = 0; row < width ; ++row)                     
          sum=0;                                |      {                    
          for(int i = 0; i < width ; ++i)       |        for(int col = 0; col < width ; ++col)                     
            {                                   |          {                     
              sum += a[row*width+i]             |            sum=0;
                   * b[i*width+col];            |            for(int i = 0; i < width ; ++i)    
            }                                   |              {                     
          c[row*width+col] = sum;               |                sum += a[row*width+i]                     
        }                                       |                     * b[i*width+col];                     
    }                                           |              }
                                                |            c[row*width+col] = sum;
                                                |          }
                                                |      }                                
=======
// Vector_Addition.c                  | // Vector_Addition_OpenACC.c
float * Vector_Addition               | float * Vector_Addition
(float *restrict a, float *restrict b,| (float *restrict a, float *restrict b, 
float *restrict c, int n)             | float *restrict c, int n)
{                                     | { float sum=0;
                                      | #pragma acc kernels loop
                                      | reduction(+:sum) copyin(a[:n], b[0:n]) copyout(c[0:n])
  for(int i = 0; i < n; i ++)         |   for(int i = 0; i < n; i ++)
    {                                 |     {
      c[i] = a[i] + b[i];             |       c[i] = a[i] + b[i];
    }                                 |       sum+=c[i];
  return c;                           |     }
}                                     | }
for FORTRAN

!! Matrix_Multiplication.f90                 |!! Matrix_Multiplication_OpenACC.f90
    do row = 0, width-1                      |   !$acc loop collapse(2) reduction(+:sum)
       do col = 0, width-1                   |   do row = 0, width-1
          sum=0                              |      do col = 0, width-1                           
          do i = 0, width-1                  |         sum=0                           
             sum = sum + (a((row*width)+i+1) |         do i = 0, width-1
                       * b((i*width)+col+1)) |            sum = sum + (a((row*width)+i+1)  
          enddo                              |                      * b((i*width)+col+1))                        
          c(row*width+col+1) = sum           |         enddo                        
       enddo                                 |         c(row*width+col+1) = sum
    enddo                                    |      enddo
                                             |   enddo 
                                             |   !$acc end loop 
reduction clause is needed when we want to sum the array or any counting inside the parallel region; this will increase the performance and avoid the error in the total sum.
The above example shows how to use them in C/C++ and FORTRAN languages.
Practical session:

Try simple Hello_World_OpenACC.c and Hello_World_OpenACC.f90 with OpenACC parallel constructs; and try to understand what compiler producing.
Do simple Vector_Addition_OpenACC.c and Vector_Addition_OpenACC.f90 with OpenACC parallel constructs and use data clauses for data management.
Similarly, try Vector_Addition_OpenACC.c and Vector_Addition_OpenACC.f90 with parallel OpenACC parallel constructs and use data clauses for data management. And include thread clauses for creating threads.
Finally, do Matrix_Multiplication_OpenACC.c and Matrix_Multiplication_OpenACC.f90 and use reduction clause along with parallel constructs and data management clauses.
Similarly include the thread blocks for Matrix_Multiplication_OpenACC.c and Matrix_Multiplication_OpenACC.f90.
!To follow up more about CUDA and OpenACC programming, please visit:MOOC course: GPU programming for scientific computing and beyond!

Solving the Laplace Equation on GPU with OpenACC

Copyright (c) 2020-2021 L. Koutsantonis, T. Carneiro, UL HPC Team hpc-team@uni.lu






Pre-requisites

Ensure you are able to connect to the UL HPC cluster. In particular, recall that the module command is not available on the access frontends.

Access to the ULHPC iris cluster (here it is the only one featuring GPU nodes):

(laptop)$> ssh iris-cluster
/!\ Advanced (but recommended) best-practice: Always work within a GNU Screen session named with 'screen -S ' (Adapt accordingly) IF not yet done, copy ULHPC .screenrc in your home:

(access)$> cp /etc/dotfiles.d/screen/.screenrc ~/
Now, you need to pull the latest changes in your working copy of the ULHPC/tutorials you should have cloned in ~/git/github.com/ULHPC/tutorials (see "preliminaries" tutorial)

(access)$> cd ~/git/github.com/ULHPC/tutorials
(access)$> git pull
Access to a GPU-equipped node of the Iris cluster

This practical session assumes that you have reserved a node on iris with one GPU for interactive development. See documentation

### Have an interactive GPU job
# ... either directly
(access)$> si-gpu
# ... or using the HPC School reservation 'hpcschool-gpu' if needed - use 'sinfo -T' to check if active and its name
# (access)$> si-gpu --reservation=hpcschool-gpu
$ nvidia-smi



Objectives

This tutorial aims to show how the OpenAcc directives can be used to accelerate a numerical solver commonly used in engineering and scientific applications. After completing the exercise of this tutorial, you would be able to:

Transfer data from the host to device using the data directives,
Accelerate a nested loop application with the loop directives, and,
Use the reduction clause to perform summation on variables or elements of a vector.



The Laplace Equation

The Laplace differential equation in 2D is given by:
∇2F=d2Fdx2+d2Fdy2=0
It models a distribution at steady-state or equilibrium in a 2D space (e.g., Temperature Distribution).
The Laplace differential equation can be solved using the Jacobi method if the boundary conditions are known (e.g., the temperature at the edges of the physical region of interest)
An example of a 2D problem is demonstrated in the figures below. The first figure presents the temperature at the edges of a plane. The solution of the Laplacian equation providing the steady-state temperature distribution was calculated for the given boundary condition using the Jacobi method and is shown in the second figure.



Boundary Conditions





Solution



The Jacobi method

Iterative method for solving a system of equations:
Ax=b
where, the elements A and b are constants, and x is the vector with the unknowns.

At each iteration, the elements x are updated using their previous estimations by:
xi=1Aii(bi−∑i≠jAijxk−1j)
An error metric is calculated at each iteration k over the elements xi:
Error=∑i(xki−x(k−1)i)2
The algorithm terminates when the error value becomes smaller than a predefined threshold:
Error<Threshold



Solving the Laplace Equation using the Jacobi method

Second-order derivatives can be calculated numerically for a small enough value of δ by:
d2Fdx2=1δ2(f(x+δ,y)−2f(x,y)+f(x−δ,y))
d2Fdy2=1δ2(f(x,y+δ)−2f(x,y)+f(x,y−δ))
Substituting the numerical second-order derivatives in the Laplace equation gives:
f(x,y)=14(f(x,y+δ)+f(x,y−δ)+f(x+δ,y)+f(x−δ,y))
The above equation results in a stencil pattern where the new value of an element is calculated using its neighbors. A stencil of four points is shown in the figure below. The Jacobi iterative method can lead to the solution of the Laplace equation by successively calculating the above stencil output for each element fi in the vector of unknowns which in this case is the vectorized representation of the examined distribution F (e.g Temperature distribution) .

Stencil of 4 points





Serial Implementation of the Jacobi Method in C

A serial code implementing the Jacobi method employs a nested loop to compute the elements of a matrix at each iteration. At each iteration, the error value (distance metric) is calculated over these elements. This calculated error value is monitored in the main loop to terminate the iterative Jacobi algorithm.
while ((iter < miter )&& (error > thres))
{
error = calcTempStep(T, Tnew, n, m);

update(T, Tnew, n, m);

if(iter % 50 == 0) printf("Iterations = %5d, Error = %16.10f\n", iter, error);

iter++;
}
The nested loop is implemented in function calcTempStep(float *restrict F, float *restrict Fnew, int n, int m). n and m are the dimensions of the 2D matrix F, F is a vector containing the current estimations, and Fnew is a buffer storing the new elements' values as resulted from the stencil calculations. The error value is calculated for each new stencil calculation using the corresponding elements of vectors F and Fnew. The error is summed (reduced) for all elements and returned to the function main.
float calcTempStep(float *restrict F, float *restrict Fnew, int n, int m)
{
float Fu, Fd, Fl, Fr;
float error = 0.0;


for (int i = 1; i < n-1; i++){
for (int j = 1; j < m-1; j++){
Fu = F[(i-1)*m + j];
Fd = F[(i+1)*m + j];
Fl = F[i*m + j - 1];
Fr = F[i*m + j + 1];
Fnew[i*m+j] = 0.25*(Fu + Fd + Fl + Fr);
error += (Fnew[i*m+j] - F[i*m+j])*(Fnew[i*m+j] - F[i*m+j]);
}
}


return error;
}
The function update(float *restrict F, float *restrict Fnew, int n, int m) implements a nested loop which is used to update the current estimates of F with the stencil calculations stored in Fnew.
void update(float *restrict F, float *restrict Fnew, int n, int m)
{

for (int i = 0; i < n; i++)
for (int j = 0; j < m; j++ )
F[i*m+j] = Fnew[i*m+j];


}





Exercise:

Parallelize the Jacobi iteration with OpenACC

Follow the steps below to accelerate the Jacobi solver using the OpenAcc directives.

Task 1: If you do not yet have the UL HPC tutorial repository, clone it. Update to the latest version.

ssh iris-cluster
mkdir -p ~/git/github.com/ULHPC
cd ~/git/github.com/ULHPC
git clone https://github.com/ULHPC/tutorials.git
cd tutorials/gpu/openacc/laplace/exercise
git stash && git pull -r && git stash pop


Task 2: Get an interactive GPU job on iris cluster:

### ... either directly - dedicate 1/4 of available cores to the management of GPU card
$> si-gpu -c7
# /!\ warning: append -G 1 to reserve a GPU
# salloc -p interactive --qos debug -C gpu -c7 -G 1 --mem-per-cpu 27000

### ... or using the HPC School reservation 'hpcschool-gpu'
salloc --reservation=hpcschool-gpu -p interactive -C gpu --ntasks-per-node 1 -c7 -G 1



Task 3: Load the required modules:

module load compiler/PGI/19.10-GCC-8.3.0-2.32
module load compiler/GCC



Task 4: Use the source file jacobi.c under the folder OpenAccExe/exercise/ with your favorite editor (e.g. emacs). The C code implementing the method is already developed. Use the data directive to copy T and Tnew vectors in device memory before the loop in the function main:

//Code Here ! (1 line)
while ((iter < miter )&& (error > thres))
{
error = calcTempStep(T, Tnew, n, m);

update(T, Tnew, n, m);

if(iter % 50 == 0) printf("Iterations = %5d, Error = %16.10f\n", iter, error);

iter++;
}



Task 5: Use the parallel loop directive to parallelize the nested loop in function calcTempStep. Use a reduction clause (sum) to sum over the error values calculated for the pairs of elements T[i*m+j] and Tnew[i*m+j]:

float calcTempStep(float *restrict F, float *restrict Fnew, int n, int m)
{
float Fu, Fd, Fl, Fr;
float error = 0.0;

//Code Here! (1 line)
for (int i = 1; i < n-1; i++){
//Code Here! (1 line)
for (int j = 1; j < m-1; j++){
Fu = F[(i-1)*m + j];
Fd = F[(i+1)*m + j];
Fl = F[i*m + j - 1];
Fr = F[i*m + j + 1];
Fnew[i*m+j] = 0.25*(Fu + Fd + Fl + Fr);
error += (Fnew[i*m+j] - F[i*m+j])*(Fnew[i*m+j] - F[i*m+j]);
}
}


return error;
}



Task 6: Use the parallel loop directive to parallelize the nested loop in function update:

void update(float *restrict F, float *restrict Fnew, int n, int m)
{
//Code Here! (1 line)
for (int i = 0; i < n; i++)
//Code Here! (1 line)
for (int j = 0; j < m; j++ )
F[i*m+j] = Fnew[i*m+j];


}



Task 6: Compile and run your code

The PGI compiler 'pgcc' can be used to compile the source code containing OpenAcc directives. The -acc flag is required to enable the OpenAcc directives. The target architecture (Nvidia in this case) is defined using the -ta flag:

pgcc -acc -ta=nvidia -Minfo jacobi.c -o $exe
The compilation output indicates the part of the code that has been parallelized with the OpenAcc directives and provides information on the data transfers between the host and device. An example compilation output is given below:

calcTempStep:
41, Generating copyin(F[:n*m]) [if not already present]
Generating Tesla code
42, #pragma acc loop gang /* blockIdx.x */
Generating reduction(+:error)
44, #pragma acc loop vector(128) /* threadIdx.x */
Generating reduction(+:error)
41, Generating implicit copy(error) [if not already present]
Generating copyout(Fnew[:n*m]) [if not already present]
42, FMA (fused multiply-add) instruction(s) generated
44, Loop is parallelizable
update:
65, Generating copyin(Fnew[:n*m]) [if not already present]
Generating copyout(F[:n*m]) [if not already present]
Generating Tesla code
67, #pragma acc loop gang /* blockIdx.x */
69, #pragma acc loop vector(128) /* threadIdx.x */
69, Loop is parallelizable
Memory copy idiom, loop replaced by call to __c_mcopy4
main:
127, Generating copy(Tnew[:m*n],T[:m*n]) [if not already present]



To run the executable file interactively, use:

./$exe
where, $exe is the name of the executable after compilation. After running the script, the program prints to the screen the calculated error value at each iteration:

Iterations = 0, Error = 191.4627685547
Iterations = 50, Error = 0.3710202873
Iterations = 100, Error = 0.1216623262
Iterations = 150, Error = 0.0623667538
Iterations = 200, Error = 0.0387549363
Iterations = 250, Error = 0.0268577076
Iterations = 300, Error = 0.0199674908
Iterations = 350, Error = 0.0155865224
Iterations = 400, Error = 0.0126058822
Iterations = 450, Error = 0.0104726302
Iterations = 500, Error = 0.0088840043
Iterations = 550, Error = 0.0076624807
Iterations = 600, Error = 0.0066990838



The total time and number of iterations required for reducing the error value below the threshold value is printed when the program finishes its execution:

Total Iterations = 10000
Error = 0.0000861073
Total time (sec) = 2.976




Task 7: Compile and run the code without using the OpenAcc directives. This task can be performed by just compiling the code without using the -acc and -ta=nvidia flags:

pgcc -Minfo jacobi.c -o $exe
Run the serial application. What you observe? What is the acceleration achieved using the OpenAcc directives?

MATLAB (interactive, passive and sequential jobs) execution on the UL HPC platform

  Copyright (c) 2013-2019 UL HPC Team  <hpc-sysadmins@uni.lu>


The objective of this tutorial is to exemplify the execution of MATLAB - a high-level language and interactive environment for numerical computation, visualization and programming, on top of the UL HPC platform.

The tutorial will show you:

how to run MATLAB in interactive mode, with either the full graphical interface or the text-mode interface
how to check the available toolboxes and licenses used
how to run MATLAB in passive (batch) mode, enabling unattended execution on the clusters
how to use MATLAB script (.m) files
how to plot data, saving the plots to file
how to take advantage of some of the paralelization capabilities of MATLAB to speed up your tasks
For the tutorial we will use the UL HPC Iris cluster that includes nodes with GPU accelerators.

Prerequisites

As part of this tutorial two Matlab example scripts have been developed and you will need to download them, along with their dependencies, before following the instructions in the next sections:

    (access-iris)$> mkdir -p ~/matlab-tutorial/code
    (access-iris)$> cd ~/matlab-tutorial/code
    (access-iris)$> wget --no-check-certificate https://raw.github.com/ULHPC/tutorials/devel/maths/matlab/basics/code/example1.m
    (access-iris)$> wget --no-check-certificate https://raw.github.com/ULHPC/tutorials/devel/maths/matlab/basics/code/example2.m
    (access-iris)$> wget --no-check-certificate https://raw.github.com/ULHPC/tutorials/devel/maths/matlab/basics/code/google_finance_data.m
    (access-iris)$> wget --no-check-certificate https://raw.github.com/ULHPC/tutorials/devel/maths/matlab/basics/code/file_data_source.m
    (access-iris)$> wget --no-check-certificate https://raw.github.com/ULHPC/tutorials/devel/maths/matlab/basics/code/AAPL.csv
Or simply clone the full tutorials repository and make a link to this part of the MATLAB tutorial:

    (access-iris)$> git clone https://github.com/ULHPC/tutorials.git
    (access-iris)$> ln -s tutorials/maths/matlab/basics ~/matlab-tutorial
Matlab execution in interactive mode

Launching the full graphical environment

Running the full MATLAB environment on the Iris cluster will require you to enable X11 forwarding in order for the graphical environment to be shown on your local machine:

on Linux simply follow the commands below
on OS X (depending on version) you may not have the X Window System installed, and thus will need to install XQuartz if the first command below returns an 'X11 forwarding request failed on channel 0' error
on Windows you will either need to use MobaXTerm or if you use Putty, run VcXsrv first then configure Putty (Connection -> SSH -> X11 -> Enable X11 forwarding) before logging in to Iris.

# Connect to Iris with X11 forwarding enabled (Linux/OS X):
(yourmachine)$> ssh yourlogin@access-iris.uni.lu -p 8022 -X

# Request an interactive job (the default parameters get you 1 core for 2 hours) with an X11 tunnel:
(access-iris)$> salloc -p interactive bash -c 'ssh -X $(scontrol show hostnames | head -n 1)'

# Check the Matlab versions installed on the clusters:
(node)$> module avail matlab

# Load a specific MATLAB version:
(node)$> module load base/MATLAB/2018a

# Check that its profile has been loaded and thus we can start to use it:
(node)$> module list

# Launch MATLAB
(node)$> matlab
After a delay, the full Matlab interface will be displayed on your machine and you will be able to run commands, load and edit scripts and generate plots. An alternative to the graphical interface is the command-line (text-mode) interface, which is enabled through specific parameters, described in the following section.

Note: to request a full Iris node for a large interactive experiment you can:

Use all 28 cores within a node from the interactive partition, with 4GB RAM/core

salloc -p interactive -N 1 -n 1 -c 28 bash -c 'ssh -X $(scontrol show hostnames | head -n 1)'
Use all 112 cores within a node from the bigmem partition, with ~27GB RAM/core

salloc -p bigmem -N 1 -n 1 -c 112 bash -c 'ssh -X $(scontrol show hostnames | head -n 1)'
Launching the command-line environment

Running the text-mode MATLAB interface in an interactive session, is much faster than using the full graphical environment through the network and is useful for commands/scripts testing and quick executions:

    # Connect to Iris, start an interactive job with 14 cores for 1 hour:

    (yourmachine)$> ssh yourlogin@access-iris.uni.lu -p 8022
    (access-iris)$> si -n 1 -c 14 --time 1:0:0
    (node)$> module load base/MATLAB/2018a

    # Launch MATLAB with the graphical display mode disabled (critical parameters):
    (node)$> matlab -nodisplay -nosplash
    Opening log file:  /home/users/vplugaru/java.log.46818

                                   < M A T L A B (R) >
                         Copyright 1984-2018 The MathWorks, Inc.
                          R2018a (9.4.0.813654) 64-bit (glnxa64)
                                    February 23, 2018

    To get started, type one of these: helpwin, helpdesk, or demo.
    For product information, visit www.mathworks.com.

    >> version()
    ans =
        '9.4.0.813654 (R2018a)'
In this command line you are now able to run Matlab commands, load and edit scripts, but cannot display plots - they can however be generated and exported to file, which you will need to transfer to your own machine for visualisation. While the text mode interface is spartan, you still benefit from tab-completion (type the first few letters of a command then press TAB twice to see possible completions) and can run the integrated help with help command_name (e.g. help plot3).

Example usage of Matlab in interactive mode

At this point you should have downloaded the example scripts and started Matlab either with the graphical or the text-mode interface. We will now test some Matlab commands by using the google_finance_data function defined in google_finance_data.m. This function downloads stock market data through the Google Finance API, and we will use it to get 1 month worth of stock data for IBM (whose stock symbol is 'IBM'):

     >> cd('~/matlab-tutorial/code/')
     >> [hist_date, hist_high, hist_low, hist_open, hist_close, hist_vol] = google_finance_data('IBM', '2017-05-01', '2017-06-02');
     >> size(hist_date)
     ans =
         24     1
     >> [hist_date{1} ' ' hist_date{end}]
     ans =
         '1-May-17 2-Jun-17'
     >> min(hist_low)
     ans =
       149.7900
     >> max(hist_high)
     ans =
       160.4200
     >> mean(hist_close)
     ans =
       153.2879
     >> std(hist_close)
     ans =
         2.7618
Note: If the Google Finance API is not available, you can use the file_data_source.m function with the AAPL ticker to use pre-downloaded data.

Through these commands we have seen that the function returns column vectors, we were able to get 24 days' worth of information and we used simple statistic functions to get an idea of how the stock varied in the given period.

Now we will use the example1.m script that shows: - how to use different plotting methods on the data retrieved with the google_finance_data function - how to export the plots in different graphic formats instead of displaying them (which is only available when running the full graphical environment and also allows the user to visually interact with the plot)

     >> example1
     Elapsed time is 1.709865 seconds.
     >> quit
     (node)$>
     (node)$> ls *pdf *eps
     example1-2dplot.eps  example1-2dplot.pdf  example1-scatter.eps
Note: You'll need to edit example1.m to use the offline data source file_data_source.m in place of the Google Finance API, if running example1 shows an error.

We have run the example1.m script which has downloaded Apple ('AAPL' ticker) stock data for the year 2016 and generated three plots:

example1-2dplot.pdf : color PDF generated with the saveas function, plotting dates (x-axis) vs closing stock price (y-axis)
example1-2dplot.eps : high quality black and white Enhanced PostScript (EPS) generated with the print function, same data as above
example1-scatter.eps : high quality color EPS generated with the print function, showing also the trading volume (z-axis) and using different color datapoints (red) where the closing share price was above 100
The script has also used the tic/toc Matlab commands to time it's execution and we can see it took less than 2 seconds to download and process data from the Google Finance API and generate the plots.

Finally, we have closed our Matlab session and were returned to the cluster's command line prompt where we found the generated plots.

A PNG version of the latter two plots is shown below: 2D Plot 3D Scatter Plot

Further examples showing serial and parallel executions are given below in the 'Example usage of Matlab in passive mode' section.

Checking available toolboxes and license status

In order to be able to run MATLAB and specific features provided through the various MATLAB toolboxes, sufficient licenses need to be available. The state of the licenses can be checked with the lmstat utility.

First, we will check that the license server is running (an UP status should be shown in the output of lmutil):

     (node)$> module load base/MATLAB
     (node)$> $EBROOTMATLAB/etc/glnxa64/lmutil lmstat -c $EBROOTMATLAB/licenses/network.lic
Next, we can check the total number of MATLAB licenses available (issued) and how many are used:

     (node)$> $EBROOTMATLAB/etc/glnxa64/lmutil lmstat -c $EBROOTMATLAB/licenses/network.lic -f MATLAB
To check for a specific feature and its usage (e.g. the financial toolbox if we know its name):

     (node)$> $EBROOTMATLAB/etc/glnxa64/lmutil lmstat -c $EBROOTMATLAB/licenses/network.lic -f Financial_toolbox
To see all available toolboxes:

     (node)$> $EBROOTMATLAB/etc/glnxa64/lmutil lmstat -c $EBROOTMATLAB/licenses/network.lic -a
Checking the availability of statistics toolboxes (if we don't know the exact name, but that 'stat' is in the name):

     (node)$> $EBROOTMATLAB/etc/glnxa64/lmutil lmstat -c $EBROOTMATLAB/licenses/network.lic -a | grep -i statistics
Finally, checking the available toolboxes (but with no information on the specific # of available/used licenses), can be done directly from MATLAB with the ver command. We will load the development (experimental/testing) software set that as of June 2019 contains the newest MATLAB available (R2019a) and see this information:

     (node)$> module load swenv/default-env/devel
     (node)$> module load base/MATLAB/2019a
     (node)$> matlab -nodesktop -nodisplay
     Opening log file:  /home/users/vplugaru/java.log.16925

                             < M A T L A B (R) >
                   Copyright 1984-2019 The MathWorks, Inc.
                   R2019a (9.6.0.1072779) 64-bit (glnxa64)
                                March 8, 2019
     To get started, type doc.
     For product information, visit www.mathworks.com.

     >> ver
     -----------------------------------------------------------------------------------------------------
     MATLAB Version: 9.6.0.1072779 (R2019a)
     MATLAB License Number: 886910
     Operating System: Linux 3.10.0-693.21.1.el7.x86_64 #1 SMP Wed Mar 7 19:03:37 UTC 2018 x86_64
     Java Version: Java 1.8.0_181-b13 with Oracle Corporation Java HotSpot(TM) 64-Bit Server VM mixed mode
     -----------------------------------------------------------------------------------------------------
     MATLAB                                                Version 9.6         (R2019a)
     Simulink                                              Version 9.3         (R2019a)
     5G Toolbox                                            Version 1.1         (R2019a)
     AUTOSAR Blockset                                      Version 2.0         (R2019a)
     Aerospace Blockset                                    Version 4.1         (R2019a)
     Aerospace Toolbox                                     Version 3.1         (R2019a)
     Antenna Toolbox                                       Version 4.0         (R2019a)
     Audio Toolbox                                         Version 2.0         (R2019a)
     Automated Driving Toolbox                             Version 2.0         (R2019a)
     Bioinformatics Toolbox                                Version 4.12        (R2019a)
     Communications Toolbox                                Version 7.1         (R2019a)
     Computer Vision Toolbox                               Version 9.0         (R2019a)
     Control System Toolbox                                Version 10.6        (R2019a)
     Curve Fitting Toolbox                                 Version 3.5.9       (R2019a)
     DSP System Toolbox                                    Version 9.8         (R2019a)
     Database Toolbox                                      Version 9.1         (R2019a)
     Datafeed Toolbox                                      Version 5.8.1       (R2019a)
     Deep Learning Toolbox                                 Version 12.1        (R2019a)
     Econometrics Toolbox                                  Version 5.2         (R2019a)
     Embedded Coder                                        Version 7.2         (R2019a)
     Filter Design HDL Coder                               Version 3.1.5       (R2019a)
     Financial Instruments Toolbox                         Version 2.9         (R2019a)
     Financial Toolbox                                     Version 5.13        (R2019a)
     Fixed-Point Designer                                  Version 6.3         (R2019a)
     Fuzzy Logic Toolbox                                   Version 2.5         (R2019a)
     GPU Coder                                             Version 1.3         (R2019a)
     Global Optimization Toolbox                           Version 4.1         (R2019a)
     HDL Coder                                             Version 3.14        (R2019a)
     HDL Verifier                                          Version 5.6         (R2019a)
     Image Acquisition Toolbox                             Version 6.0         (R2019a)
     Image Processing Toolbox                              Version 10.4        (R2019a)
     Instrument Control Toolbox                            Version 4.0         (R2019a)
     LTE HDL Toolbox                                       Version 1.3         (R2019a)
     LTE Toolbox                                           Version 3.1         (R2019a)
     MATLAB Coder                                          Version 4.2         (R2019a)
     MATLAB Compiler                                       Version 7.0.1       (R2019a)
     MATLAB Compiler SDK                                   Version 6.6.1       (R2019a)
     MATLAB Report Generator                               Version 5.6         (R2019a)
     Mapping Toolbox                                       Version 4.8         (R2019a)
     Mixed-Signal Blockset                                 Version 1.0         (R2019a)
     Model Predictive Control Toolbox                      Version 6.3         (R2019a)
     Optimization Toolbox                                  Version 8.3         (R2019a)
     Parallel Computing Toolbox                            Version 7.0         (R2019a)
     Partial Differential Equation Toolbox                 Version 3.2         (R2019a)
     Phased Array System Toolbox                           Version 4.1         (R2019a)
     Powertrain Blockset                                   Version 1.5         (R2019a)
     Predictive Maintenance Toolbox                        Version 2.0         (R2019a)
     RF Blockset                                           Version 7.2         (R2019a)
     RF Toolbox                                            Version 3.6         (R2019a)
     Reinforcement Learning Toolbox                        Version 1.0         (R2019a)
     Risk Management Toolbox                               Version 1.5         (R2019a)
     Robotics System Toolbox                               Version 2.2         (R2019a)
     Robust Control Toolbox                                Version 6.6         (R2019a)
     Sensor Fusion and Tracking Toolbox                    Version 1.1         (R2019a)
     SerDes Toolbox                                        Version 1.0         (R2019a)
     Signal Processing Toolbox                             Version 8.2         (R2019a)
     SimBiology                                            Version 5.8.2       (R2019a)
     SimEvents                                             Version 5.6         (R2019a)
     Simscape                                              Version 4.6         (R2019a)
     Simscape Driveline                                    Version 2.16        (R2019a)
     Simscape Electrical                                   Version 7.1         (R2019a)
     Simscape Fluids                                       Version 2.6         (R2019a)
     Simscape Multibody                                    Version 6.1         (R2019a)
     Simulink 3D Animation                                 Version 8.2         (R2019a)
     Simulink Check                                        Version 4.3         (R2019a)
     Simulink Code Inspector                               Version 3.4         (R2019a)
     Simulink Coder                                        Version 9.1         (R2019a)
     Simulink Control Design                               Version 5.3         (R2019a)
     Simulink Coverage                                     Version 4.3         (R2019a)
     Simulink Design Optimization                          Version 3.6         (R2019a)
     Simulink Design Verifier                              Version 4.1         (R2019a)
     Simulink Report Generator                             Version 5.6         (R2019a)
     Simulink Requirements                                 Version 1.3         (R2019a)
     Simulink Test                                         Version 3.0         (R2019a)
     SoC Blockset                                          Version 1.0         (R2019a)
     Stateflow                                             Version 10.0        (R2019a)
     Statistics and Machine Learning Toolbox               Version 11.5        (R2019a)
     Symbolic Math Toolbox                                 Version 8.3         (R2019a)
     System Composer                                       Version 1.0         (R2019a)
     System Identification Toolbox                         Version 9.10        (R2019a)
     Text Analytics Toolbox                                Version 1.3         (R2019a)
     Trading Toolbox                                       Version 3.5.1       (R2019a)
     Vehicle Dynamics Blockset                             Version 1.2         (R2019a)
     Vehicle Network Toolbox                               Version 4.2         (R2019a)
     Vision HDL Toolbox                                    Version 1.8         (R2019a)
     WLAN Toolbox                                          Version 2.1         (R2019a)
     Wavelet Toolbox                                       Version 5.2         (R2019a)
Matlab execution in passive mode

For non-interactive or long executions, MATLAB can be ran in passive mode, reading all commands from an input file you provide (e.g. named INPUTFILE.m) and saving the results in an output file (e.g. named OUTPUTFILE.out), by either:

using redirection operators:

$> matlab -nodisplay -nosplash < INPUTFILE.m > OUTPUTFILE.out
running the input file as a command (notice the missing '.m' extension) and copying output (as a log) to the output file:

$> matlab -nodisplay -nosplash -r INPUTFILE -logfile OUTPUTFILE.out
The second usage mode is recommended as it corresponds to the batch-mode execution. In the first case your output file will contain the '>>' characters generated by Matlab as if ran interactively, along with the results of your own commands.

However as the second usage mode runs your script as a command, it must contain the quit command at the end in order to close Matlab, otherwise after the script has executed Matlab will stay open, waiting for further input until the end of the walltime you set for the passive job, tying up compute resources needlessly.

The following minimal launcher example shows how to run a serial (1 core) MATLAB script for 24 hours in passive mode:

   #!/bin/bash -l
   #SBATCH -J MATLAB
   #SBATCH -N 1
   #SBATCH -n 1
   #SBATCH -c 1
   #SBATCH --time=1-0:0:0
   #SBATCH -p batch

   module load base/MATLAB/2018a
   matlab -nodisplay -nosplash < /path/to/your/inputfile > /path/to/your/outputfile
If your MATLAB code is (ideally) using parfor constructs for parallel loops, change the -c above to e.g. 28 to use all the cores available in a compute node from the batch partition. The most that you can get on a single node on Iris would be in the bigmem partition, which has Skylake CPUs and 112 cores per node with ~27 GB RAM/core.

Example usage of Matlab in passive mode

In this section we will use the example2.m script which shows: - the serial execution of time consuming operations; 1 core on 1 node - the parallel execution (based on the parfor command) and relative speedup vs serial execution, setting the maximum number of parallel threads through environment variables; up to 1 full node - GPU-based parallel execution; available only on GPU-enabled nodes

By default the parallel section of the script uses up to 4 threads, thus for a first test you will:

create a launcher script called matlab-launcher.sh:
use the launcher shown above, changing number of requested cores to 4 and a walltime of 5 minutes
have MATLAB take its input from the example2.m m-script, and store output in example2.out
submit the job to the scheduler with sbatch
wait until the job completes its execution (see its status with squeue -l -j $JOBID and runtime details with sacct -l -j $JOBID):
(access-iris)$> sbatch matlab-launcher.sh
(access-iris)$> cat example2.out
                    < M A T L A B (R) >
          Copyright 1984-2018 The MathWorks, Inc.
           R2018a (9.4.0.813654) 64-bit (glnxa64)
                     February 23, 2018

 To get started, type one of these: helpwin, helpdesk, or demo.
 For product information, visit www.mathworks.com.

 -- Will perform 200 iterations on a 1000x1000 matrix

 -- Serial test
 -- Execution time: 115.532743s.
 -- Parallel tests with up to 4 cores

 tmpJobStorageLocation =

     '/scratch/users/vplugaru/matlab.457594'

 -- Parallel test using 2 cores
 Starting parallel pool (parpool) using the 'local' profile ...
 connected to 2 workers.
 Parallel pool using the 'local' profile is shutting down.
 -- Execution time: 74.798509s.
 -- Execution time with overhead: 99.103444s.

 -- Parallel test using 4 cores
 Starting parallel pool (parpool) using the 'local' profile ...
 connected to 4 workers.
 Parallel pool using the 'local' profile is shutting down.
 -- Execution time: 39.275277s.
 -- Execution time with overhead: 56.273791s.

 -- Number of processes, parallel execution time (s), parallel execution time with overhead(s), speedup, speedup with overhead:
     1.0000  115.5327  115.5327    1.0000    1.0000
     2.0000   74.7985   99.1034    1.5446    1.1658
     4.0000   39.2753   56.2738    2.9416    2.0530

 -- GPUs not available on this system. Not running GPU-parallel test.
We will now adapt this launcher to use one of the GPU nodes of Iris, requesting part of its 28 cores and 1 of its 4 GPUs:

   #!/bin/bash -l
   #SBATCH -J MATLAB
   #SBATCH -N 1
   #SBATCH -n 1
   #SBATCH -c 7
   #SBATCH --gres=gpu:1
   #SBATCH --time=0:10:0
   #SBATCH -p gpu

   module load base/MATLAB/2018a
   matlab -nodisplay -nosplash -r example2 -logfile example2-gpu.out
Start a job with this launcher, and now you will see that the GPU test of example2.m will also be carried out. What are the 1 GPU vs CPU-only speedups?

Relative to the fast execution of the inner instruction (which calculates the eigenvalues of a matrix) the overhead given by the creation of the parallel pool and the task assignations can be quite high. You will need to be careful how you create a parallel pool as spawning the workers may not make sense if the overhead is higher compared to the computational time taken by the worker startup.

Additional work

Additional experiments are suggested: - try the different nodes of Iris, e.g. the difference in obtained speed on Broadwell CPUs (use -C broadwell in your launcher) vs Skylake CPUs (-C skylake) - try the scaling limits of this example, e.g. on one of the large memory nodes (use the -p bigmem bigmem partition in your launcher, with up to 112 cores) - combine parfor with gpuArray and use the multi-GPU capabilities of Matlab (use --gres=gpu:4 for submission, you'll need to edit the example2.m file as wel)

Useful references

Getting Started with Parallel Computing Toolbox
Parallel for-Loops (parfor) documentation
GPU Computing documentation
Multi-GPU computing examples

Advanced MATLAB: checkpointing and parallel jobs execution on the UL HPC platform

  Copyright (c) 2013-2018 UL HPC Team  <hpc-sysadmins@uni.lu>


The objective of this tutorial is to cover advanced usage of MATLAB, in particular:

enable checkpointing
allow for parallel execution of matlab code.
Pre-requisites

You should have followed the basic Matlab tutorial

Useful references

Getting Started with Parallel Computing Toolbox
Parallel for-Loops (parfor) documentation
GPU Computing documentation

Reproducible pipelines in R

By
ULHPC Licence GitHub
issues  Github Documentation
Status GitHub
forks

R Tutorial

  Copyright (c) 2013-2023 Aurelien Ginolhac, UL HPC Team  <hpc-sysadmins@uni.lu>


Through this tutorial you will learn how to use the R package {targets}: A function-oriented Make-like workflow manager.

Warning: this tutorial does not focus on the learning of R language. If you’re also looking for a good tutorial on R’s data structures you can take a look at: Hadley Wickham’s page. Another bookdown’s book is available for free: R for Data Science by Hadley Wickham, Mine Çetinkaya-Rundel & Garrett Grolemund.

Pre-requisites

Ensure you are able to connect to the UL HPC clusters

you MUST work on a computing node

# /!\ FOR ALL YOUR COMPILING BUSINESS, ENSURE YOU WORK ON A COMPUTING NODE
(access-iris)$> si -c 2 -t 1:00:00
On HPC, using Singularity

A Singularity image was prepared that contains an Ubuntu-based R 4.2.2 but with all necessary packages. Of note this was created using renv::restore(). renv helps to manage R dependency to projects.

Once on a node, load Singularity
module load tools/Singularity

Cloning the demo repository

The demo repository is targets_demos.

cd $HOME
git clone --branch hpc https://gitlab.lcsb.uni.lu/aurelien.ginolhac/targets_demos.git
Check you can tart a container inside this newly fetched folder
singularity run -H /home/users/${USER}/targets_demos/ --contain \
  /scratch/users/aginolhac/targets_hpc/r-targets.sif ls
Here we bind as home only the targets_demos folder (specifying -H this new home and --contain to not bind the rest of your home) and list its content:

LICENSE.md  README.md      _targets_ds_1.R  _targets_ds_2_crew.R  _targets_ds_fun1.R   circles  ds1.Rmd  ds3.Rmd  lines   renv.lock
R           _targets.yaml  _targets_ds_2.R  _targets_ds_3.R       _targets_packages.R  data     ds2.Rmd  img      others  run.R
Using VScode to connect remotely to the HPC

I have followed this tutorial: R-VScode by Roland Krasser

Useful links

CRAN Archive
CRAN HPC Packages
Tidyverse Documentation
4-days tidyverse workshop.uni.lu
Advanced R programming by Hadley Wickham

Distributed Mixed-Integer Programming (MIP) optimization with Cplex and Gurobi





Cplex is an optimization software for mathematical programming. The Cplex optimizer can solve:

Mixed-Integer programming problems (MIP)
Very large linear programming problems (LP)
Non-convex quadratic programming problems (QP)
Convex quadratically constrained problems (QCP)
Gurobi is a powerful optimization software and an alternative to Cplex for solving. Gurobi has some additionnal features compared to Cplex. For example, it can perform Mixed-Integer Quadratic Programming (MIQP) and Mixed-Integer Quadratic Constrained Programming (MIQCP).

In this tutorial, we are going to see how to leverage distributed optimization on a High Computing Platform such as Slurm.

Mathematical programming

Decision and optimization problems can be representend as mathematical models called programs.



It exists several formats used by Cplex and Gurobi to provide your problems to these solvers:

The LP format
The MPS format
The AMPL format
Using both optimization software on the ULHPC platform

The cplex optimizer can be accessed throught the module command once ressources have been requested through the SLURM scheduler.

module load math/CPLEX/12.8-foss-2018a
The Gurobi optimizer can be accessed throught the module command once ressources have been requested through the SLURM scheduler.

module load math/Gurobi/8.1.1-intel-2018a-Python-3.6.4
The resolution of optmization problem can be either done using the command line interface (CLI) or by using the different APis (e.g. C/C++, Python, Java). In this tutorial, we only consider the CLI for each optimizer.

Ressources for this tutorial

In order to test cplex and gurobi, we need an optimization instance. Hereafter, we are going to rely on instances from the miplib. For example, let us the following instance ex10.mps.gz described in details here for the interested readers.

If you did not already clone the tutorial repository, enter the following commands in your HOME directory:

# cd into scratch and clone tutorial repository
cd $SCRATCH
git clone https://github.com/ULHPC/tutorials.git
# cd into the scripts folder
cd tutorials/maths/Cplex-Gurobi/scripts
Cplex

Multi-threaded optimization with Cplex

In order to solve mathematical programs, cplex allows users to define a command line script that can be passed to the executable. On the Iris cluster, the following launcher can be used to perform multi-threaded MIP optimzation. A good practice is to request as many threads as available cores on the node. If you need more computing power, you have to consider a distributed version.

#!/bin/bash -l
#SBATCH -J Multi-threaded_cplex
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=28
#SBATCH --time=0-01:00:00
#SBATCH -p batch
#SBATCH --qos=normal

# Explicit cpus-per-task to srun
export SRUN_CPUS_PER_TASK=${SLURM_CPUS_PER_TASK}

# Load cplex 
module load math/CPLEX/12.8-foss-2018a

# Some variable
MPS_FILE=$1
RES_FILE=$2
CPLEX_COMMAND_SCRIPT="command_job${SLURM_JOBID}.lst"



# Create cplex command script
cat << EOF > ${CPLEX_COMMAND_SCRIPT}
set threads ${SLURM_CPUS_PER_TASK}
read ${MPS_FILE} 
mipopt
write "${RES_FILE}.sol" 
quit
EOF
chmod +x ${CPLEX_COMMAND_SCRIPT}

# Cplex will access use the required number of thread
cplex -f ${CPLEX_COMMAND_SCRIPT}
rm ${CPLEX_COMMAND_SCRIPT}
Use the script cplex_mtt.slurm and launch a batch job using the sbatch command as follows sbatch cplex_mtt.slurm ex10.mps.gz cplex_mtt.

Distributed optimization with Cplex

When you require more computing power (e.g. more cores), distributed computations is the way to go. The cplex optimization software embeds a feature that allows you to perform distributed MIP. Using the Message Passing Interface (MPI), cplex will distribute the exploration of the tree search to multiple workers. The below launcher is an example showing how to reserve ressources on multiple nodes through the Slurm scheduler. In this example, 31 tasks will be distributed over 2 nodes.

#!/bin/bash -l
#SBATCH -J Distrbuted_cplex
#SBATCH --nodes=2
#SBATCH --ntasks=14
#SBACTH --cpus-per-task=2
#SBATCH --time=0-01:00:00
#SBATCH -p batch
#SBATCH --qos=normal
module load math/CPLEX/12.8-foss-2018a

export SRUN_CPUS_PER_TASK=${SLURM_CPUS_PER_TASK}

# Some variables
MPS_FILE=$1
RES_FILE=$2
CPLEX_COMMAND_SCRIPT="command_job${SLURM_JOBID}.lst"



# Create cplex command script
cat << EOF > ${CPLEX_COMMAND_SCRIPT}
set distmip config mpi
set threads ${SLURM_CPUS_PER_TASK}
read ${MPS_FILE} 
mipopt
write "${RES_FILE}.sol" 
quit
EOF
chmod +x ${CPLEX_COMMAND_SCRIPT}

# Start Cplex with MPI
# On first host, the master is running 
mpirun -np 1 cplex -f ${CPLEX_COMMAND_SCRIPT} -mpi : -np $((SLURM_NTASKS - 1)) cplex -mpi
rm ${CPLEX_COMMAND_SCRIPT}
Use the script cplex_dist.slurm and launch a batch job using the sbatch command as follows sbatch cplex_dist.slurm ex10.mps.gz cplex_dist.

Gurobi

Multi-threaded optimization with Gurobi

The script below allows you to start multi-threaded MIP optimization with Gurobi.

#!/bin/bash -l
#SBATCH -J Multi-threaded_gurobi
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=28
#SBATCH --time=0-01:00:00
#SBATCH -p batch
#SBATCH --qos=normal

# Explicit cpus-per-task to srun
export SRUN_CPUS_PER_TASK=${SLURM_CPUS_PER_TASK}

# Load Gurobi 
module load math/Gurobi/8.1.1-intel-2018a-Python-3.6.4

# Some variable
MPS_FILE=$1
RES_FILE=$2

# Gurobi will access use the required number of thread
gurobi_cl Threads=${SLURM_CPUS_PER_TASK} ResultFile="${RES_FILE}.sol" ${MPS_FILE}
Use the script gurobi_mtt.slurm and launch a batch job using the sbatch command as follows sbatch gurobi_mtt.slurm ex10.mps.gz gurobi_mtt.

Distributed optimization with Gurobi

#!/bin/bash -l
#SBATCH -J Distrbuted_gurobi
#SBATCH --ntasks=3
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=5
#SBATCH --time=00:15:00
#SBATCH -p batch
#SBATCH --qos normal
#SBATCH -o %x-%j.log

# Explicit cpus-per-task to srun
export SRUN_CPUS_PER_TASK=${SLURM_CPUS_PER_TASK}

# Load personal modules
mu
# Load gurobi
module load math/Gurobi/8.1.1-intel-2018a-Python-3.6.4

export MASTER_PORT=61000
export SLAVE_PORT=61000
export MPS_FILE=$1
export RES_FILE=$2
export GUROBI_INNER_LAUNCHER="inner_job${SLURM_JOBID}.sh"

if [[ -f "grb_rs.cnf" ]];then
    sed -i "s/^THREADLIMIT.*$/THREADLIMIT=${SLURM_CPUS_PER_TASK}/g" grb_rs.cnf
else
    $GUROBI_REMOTE_BIN_PATH/grb_rs init
    echo "THREADLIMIT=${SLURM_CPUS_PER_TASK}" >> grb_rs.cnf
fi


cat << 'EOF' > ${GUROBI_INNER_LAUNCHER}
#!/bin/bash
MASTER_NODE=$(scontrol show hostname ${SLURM_NODELIST} | head -n 1)
    ## Load configuration and environment
    if [[ ${SLURM_PROCID} -eq 0 ]]; then
        ## Start Gurobi master worker in background
         $GUROBI_REMOTE_BIN_PATH/grb_rs --worker --port ${MASTER_PORT} &
         wait
    elif [[ ${SLURM_PROCID} -eq 1 ]]; then
        sleep 5
        grbcluster nodes --server ${MASTER_NODE}:${MASTER_PORT} 
        gurobi_cl Threads=${SLURM_CPUS_PER_TASK} ResultFile="${RES_FILE}.sol" Workerpool=${MASTER_NODE}:${MASTER_PORT} DistributedMIPJobs=$((SLURM_NNODES -1)) ${MPS_FILE}
    else
        sleep 2
        ## Start Gurobi slave worker in background
        $GUROBI_REMOTE_BIN_PATH/grb_rs --worker --port ${MASTER_PORT} --join ${MASTER_NODE}:${MASTER_PORT} &
        wait
fi
EOF
chmod +x ${GUROBI_INNER_LAUNCHER}

## Launch Gurobi and wait for it to start
srun ${GUROBI_INNER_LAUNCHER} &
while [[ ! -e "${RES_FILE}.sol" ]]; do
    sleep 5
done
rm ${GUROBI_INNER_LAUNCHER}
Use the script gurobi_dist.slurm and launch a batch job using the sbatch command as follows sbatch gurobi_dist.slurm ex10.mps.gz gurobi_dist.

Next

Download and try to solve other miplib instances using Cplex and/or Gurobi
Modify the number of parallel tasks

An introduction to numerical methods with BLAS

Copyright (c) 2024 UL HPC Team hpc-sysadmins@uni.lu
Author: Georgios Kafanas

This is tutorial is a succinct introduction to numerical methods using the BLAS library.

Resources

Presentation
Tutorial notes
Source code repositories

There are a few repositories with supporting material.

A clone of the official Netlib LAPACK release: checkout the blas-tutorial tag for a configuration that installs the components relevant to the BLAS tutorial.
A parser for Matrix Market Exchange Format files: provides a library reading matrix files from the Matrix Market collection of benchmark problems.
Examples and exercises: a collection of programming problems covered during the tutorial.

Bioinformatics software on the UL HPC platform

Copyright (c) 2014-2018 UL HPC Team hpc-sysadmins@uni.lu

Authors: Valentin Plugaru and Sarah Peter



The objective of this tutorial is to exemplify the execution of several Bioinformatics packages on top of the UL HPC platform.

The targeted applications are:

ABySS
Gromacs
Bowtie2 / TopHat
mpiBLAST
The tutorial will:

show you how to load and run pre-configured versions of these applications on the clusters
show you how to download and use updated versions of Bowtie2/TopHat
discuss the parallelization capabilities of these applications
Prerequisites

When you look at the software page you will notice that some of the applications are part of the bioinfo software set. The modules in this set are not visible by default. To use them within a job you have to do:

(node)$> module use /opt/apps/resif/data/stable/bioinfo/modules/all
If you want them to always be available, you can add the following line to your .bash_private:

command -v module >/dev/null 2>&1 && module use /opt/apps/resif/data/stable/bioinfo/modules/all
This tutorial relies on several input files for the bioinformatics packages, thus you will need to download them before following the instructions in the next sections:

(access)$> mkdir -p ~/bioinfo-tutorial/gromacs ~/bioinfo-tutorial/tophat ~/bioinfo-tutorial/mpiblast
(access)$> cd ~/bioinfo-tutorial
(access)$> wget --no-check-certificate https://raw.github.com/ULHPC/tutorials/devel/bio/basics/gromacs/pr.tpr -O gromacs/pr.tpr
(access)$> wget --no-check-certificate https://raw.github.com/ULHPC/tutorials/devel/bio/basics/tophat/test_data.tar.gz -O tophat/test_data.tar.gz
(access)$> wget --no-check-certificate https://raw.github.com/ULHPC/tutorials/devel/bio/basics/tophat/test2_path -O tophat/test2_path
(access)$> wget --no-check-certificate https://raw.github.com/ULHPC/tutorials/devel/bio/basics/mpiblast/test.fa -O mpiblast/test.fa
Or simply clone the full tutorials repository and make a link to the Bioinformatics tutorial:

(access)$> git clone https://github.com/ULHPC/tutorials.git
(access)$> ln -s tutorials/advanced/Bioinformatics/ ~/bioinfo-tutorial
ABySS

Characterization: CPU intensive, data intensive, native parallelization

Description

ABySS: Assembly By Short Sequences

ABySS is a de novo, parallel, paired-end sequence assembler that is designed for short reads. The single-processor version is useful for assembling genomes up to 100 Mbases in size. The parallel version is implemented using MPI and is capable of assembling larger genomes [*].

Example

This example will be ran in an interactive session, with batch-mode executions being proposed later on as exercises.

Gaia

# Connect to Gaia (Linux/OS X):
(yourmachine)$> ssh access-gaia.uni.lu

# Request 1 full node in an interactive job:
(access-gaia)$> oarsub -I -l nodes=1,walltime=00:30:00
Iris

# Connect to Iris (Linux/OS X):
(yourmachine)$> ssh access-iris.uni.lu

# Request half a node in an interactive job:
(access-iris)$> si -t 0-0:30:0 -N 1 -c 1 --ntasks-per-node=14
# Load bioinfo software set
(node)$> module use /opt/apps/resif/data/stable/bioinfo/modules/all

# Check the ABySS versions installed on the clusters:
(node)$> module avail 2>&1 | grep -i abyss

# Load the default ABySS version:
(node)$> module load bio/ABySS

# Check that it has been loaded, along with its dependencies:
(node)$> module list

# All the ABySS binaries are now in your path (check with TAB autocompletion)
(node)$> abyss-<TAB>
In the ABySS package only the ABYSS-P application is parallelized using MPI and can be run on several cores (and across several nodes) using the abyss-pe launcher.

# Create a test directory and go to it
(node)$> mkdir ~/bioinfo-tutorial/abyss
(node)$> cd ~/bioinfo-tutorial/abyss

# Set the input files' directory in the environment
(node)$> export ABYSSINPDIR=/mnt/isilon/projects/ulhpc-tutorials/bioinformatics/abyss

# Give a name to the experiment
(node)$> export ABYSSNAME='abysstest'
Gaia

# Set the number of cores to use based on OAR's host file
(node)$> export ABYSSNPROC=$(cat $OAR_NODEFILE | wc -l)

# Launch the paired end assembler:
(node)$> abyss-pe mpirun="mpirun -x PATH -x LD_LIBRARY_PATH -hostfile $OAR_NODEFILE" name=${ABYSSNAME} np=${ABYSSNPROC} k=31 n=10 lib=pairedend pairedend="${ABYSSINPDIR}/SRR001666_1.fastq.bz2 ${ABYSSINPDIR}/SRR001666_2.fastq.bz2" > ${ABYSSNAME}.out 2> ${ABYSSNAME}.err
Iris

# Set the number of cores to use based on SLURM environment variables
(node)$> export ABYSSNPROC=$(expr $SLURM_NNODES \* $SLURM_NTASKS_PER_NODE \* $SLURM_CPUS_PER_TASK)

# Create a hostfile
(node)$> srun hostname | sort -n > hostfile

# Launch the paired end assembler:
(node)$> abyss-pe mpirun="mpirun -x PATH -x LD_LIBRARY_PATH -hostfile hostfile" name=${ABYSSNAME} np=${ABYSSNPROC} k=31 n=10 lib=pairedend pairedend="${ABYSSINPDIR}/SRR001666_1.fastq.bz2 ${ABYSSINPDIR}/SRR001666_2.fastq.bz2" > ${ABYSSNAME}.out 2> ${ABYSSNAME}.err
Question: Why do we use the -x VARIABLE parameters for mpirun?

Several options seen on the abyss-pe command line are crucial:

we explicitly set the mpirun command
we export several environment variables to all the remote nodes, otherwise required paths (for the binaries, libraries) would not be known by the MPI processes running there
we do not specify -np $ABYSSNPROC in the mpirun command, as it set with abyss-pe's np parameter and internally passed on to mpirun
The execution should take around 12 minutes, meanwhile we can check its progress by monitoring the .out/.err output files:

(access)$> tail -f ~/bioinfo-tutorial/abyss/abysstest.*
# We exit the tail program with CTRL-C
On Gaia, we can also connect to the job (recall oarsub -C $JOBID) from a different terminal or Screen window and see the different ABySS phases with htop.

Because the abyss-pe workflow (pipeline) includes several processing steps with different applications of which only ABYSS-P is MPI-parallel, the speedup obtained by using more than one node will be limited to ABYSS-P's execution. Several of the other applications that are part of the processing stages are however parallelized using OpenMP and pthreads and will thus take advantage of the cores available on the node where abyss-pe was started.

The used input dataset is a well known Illumina run of E. coli.

Proposed exercises

Several exercises are proposed for ABySS:

create a launcher for ABySS using the commands shown in the previous section
launch jobs using 1 node: 4, 8 and 12 cores, then 2 and 4 nodes and measure the speedup obtained
unpack the two input files and place them on a node's /dev/shm, then rerun the experiment with 4, 8 and 12 cores and measure the speedup
GROMACS

Characterization: CPU intensive, little I/O

Description

GROMACS: GROningen MAchine for Chemical Simulations

GROMACS is a versatile package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. It is primarily designed for biochemical molecules like proteins, lipids and nucleic acids that have a lot of complicated bonded interactions, but since GROMACS is extremely fast at calculating the nonbonded interactions (that usually dominate simulations) many groups are also using it for research on non-biological systems, e.g. polymers [*].

Example

This example will be ran in an interactive session, with batch-mode executions being proposed later on as exercises.

Gaia

# Connect to Gaia (Linux/OS X):
(yourmachine)$> ssh access-gaia.uni.lu

# Request 1 full node in an interactive job:
(access-gaia)$> oarsub -I -l nodes=1,walltime=00:30:00
Iris

# Connect to Iris (Linux/OS X):
(yourmachine)$> ssh access-iris.uni.lu

# Request half a node in an interactive job:
(access-iris)$> si -t 0-0:30:0 -N 1 -c 1 --ntasks-per-node=14
# Check the GROMACS versions installed on the clusters:
(node)$> module avail 2>&1 | grep -i gromacs
There used to be two versions of GROMACS available on Gaia, a hybrid and a mt version

the hybrid version is OpenMP and MPI-enabled, all binaries have a '_mpi' suffix
the mt version is only OpenMP-enabled, as such it can take advantage of only one node's cores (however it may be faster on single-node executions than the hybrid version)
Currently only the following version of GROMACS is available:

bio/GROMACS/2016.3-intel-2017a-hybrid
We will perform our tests with the hybrid version:

# Load the MPI-enabled Gromacs, without CUDA support:
(node)$> module load bio/GROMACS

# Check that it has been loaded, along with its dependencies:
(node)$> module list

# Check the capabilities of the mdrun binary, note its suffix:
(node)$> gmx_mpi -version 2>/dev/null

# Go to the test directory
(node)$> cd ~/bioinfo-tutorial/gromacs

# Set the number of OpenMP threads to 1
(node)$> export OMP_NUM_THREADS=1
Gaia

# Perform a position restrained Molecular Dynamics run
(node)$> mpirun -np 12 -hostfile $OAR_NODEFILE -envlist OMP_NUM_THREADS,PATH,LD_LIBRARY_PATH gmx_mpi mdrun -v -s pr -e pr -o pr -c after_pr -g prlog > test.out 2>&1
Iris

# Perform a position restrained Molecular Dynamics run
(node)$> srun -n 12 gmx_mpi mdrun -v -s pr -e pr -o pr -c after_pr -g prlog > test.out 2>&1
We notice here that we are running gmx_mpi in parallel with mpirun/srun on 12/14 cores, and we explicitly export the OMP_NUM_THREADS variable to any remote node such that only one thread per MPI process will be created.

Question: What will happen if we do not set the number of OpenMP threads to 1?

GROMACS has many parallelization options and several parameters can be tuned to give you better performance depending on your workflow, see the references in the last section of this tutorial.

The used input corresponds to the Ribonuclease S-peptide example, which has been changed to perform 50k steps in the Molecular Dynamics run with position restraints on the peptide.

Proposed exercises

Several exercises are proposed for GROMACS:

create a launcher for GROMACS using the commands shown in the previous section
launch jobs using 1 node: 1, 2, 4, 8, 10 and 12 cores and measure the speedup obtained
check what happens when executing mdrun with 16 and 24 cores
launch a job using one full node that has GPU cards and run the GPU-enabled GROMACS to see if a speedup is obtained
Bowtie2/TopHat

Characterization: data intensive, RAM intensive

Description

Bowtie2: Fast and sensitive read alignment

Bowtie 2 is an ultrafast and memory-efficient tool for aligning sequencing reads to long reference sequences. It is particularly good at aligning reads of about 50 up to 100s or 1,000s of characters, and particularly good at aligning to relatively long (e.g. mammalian) genomes [*].

TopHat : A spliced read mapper for RNA-Seq

TopHat is a program that aligns RNA-Seq reads to a genome in order to identify exon-exon splice junctions. It is built on the ultrafast short read mapping program Bowtie [*].

Example

This example will show you how to use the latest version of TopHat in conjunction with the latest Bowtie2, by using the versions prebuilt for Linux by the developers.

Gaia

# Connect to Gaia (Linux/OS X):
(yourmachine)$> ssh access-gaia.uni.lu

# Request 1 full node in an interactive job:
(gaia-frontend)$> oarsub -I -l nodes=1,walltime=00:30:00
Iris

# Connect to Iris (Linux/OS X):
(yourmachine)$> ssh access-iris.uni.lu

# Request half a node in an interactive job:
(access-iris)$> si -t 0-0:30:0 -N 1 -c 14 --ntasks-per-node=1
# Create a folder for the new software and go to it
(node)$> mkdir ~/bioinfo-tutorial/newsoft
(node)$> cd ~/bioinfo-tutorial/newsoft

# Download latest Bowtie2 and Tophat, plus the SAM tools dependency:
(node)$> wget https://downloads.sourceforge.net/project/bowtie-bio/bowtie2/2.3.4.1/bowtie2-2.3.4.1-linux-x86_64.zip
(node)$> wget http://ccb.jhu.edu/software/tophat/downloads/tophat-2.1.1.Linux_x86_64.tar.gz
(node)$> wget https://github.com/samtools/samtools/releases/download/1.8/samtools-1.8.tar.bz2

# Unpack the three archives
(node)$> unzip bowtie2-2.3.4.1-linux-x86_64.zip
(node)$> tar xzvf tophat-2.1.1.Linux_x86_64.tar.gz
(node)$> tar xjvf samtools-1.8.tar.bz2

# SAMtools requires compilation:
(node)$> module load tools/bzip2/1.0.6-intel-2017a
(node)$> cd samtools-1.8 && ./configure && make && cd ..

# Create a file containing the paths to the binaries, to be sourced when needed
(node)$> echo "export PATH=$HOME/bioinfo-tutorial/newsoft/bowtie2-2.3.4.1-linux-x86_64:\$PATH" > newsoft
(node)$> echo "export PATH=$HOME/bioinfo-tutorial/newsoft/tophat-2.1.1.Linux_x86_64:\$PATH" >> newsoft
(node)$> echo "export PATH=$HOME/bioinfo-tutorial/newsoft/samtools-1.8:\$PATH" >> newsoft
(node)$> source newsoft

# You can now check that both main applications can be run:
(node)$> bowtie2 --version
(node)$> tophat2 --version
Now we will make a quick TopHat test, using the provided sample files:

# Go to the test directory, unpack the sample dataset and go to it
(node)$> cd ~/bioinfo-tutorial/tophat
(node)$> tar xzvf test_data.tar.gz
(node)$> cd test_data


# Launch TopHat, with Bowtie2 in serial mode
(node)$> tophat -r 20 test_ref reads_1.fq reads_2.fq

# Launch TopHat, with Bowtie2 in parallel mode
(node)$> tophat -p 12 -r 20 test_ref reads_1.fq reads_2.fq
We can see that for this fast execution, increasing the number of threads does not improve the calculation time due to the relatively high overhead of thread creation. Note that TopHat / Bowtie are not MPI applications and as such can take advantage of at most one compute node.

Next, we will make a longer test, where it will be interesting to monitor the TopHat pipeline (with htop for example) to see the transitions between the serial and parallel stages (left as an exercise).

# Load the file which will export $TOPHATTEST2 in the environment
(node)$> source ~/bioinfo-tutorial/tophat/test2_path

# Launch TopHat, with Bowtie2 in parallel mode
(node)$> tophat2 -p 12 -g 1 -r 200 --mate-std-dev 30 -o ./  $TOPHATTEST2/chr10.hs $TOPHATTEST2/SRR027888.SRR027890_chr10_1.fastq $TOPHATTEST2/SRR027888.SRR027890_chr10_2.fastq
The input data for the first test corresponds to the TopHat test set, while the second test is an example of aligning reads to the chromosome 10 of the human genome as given here.

Proposed exercises

The following exercises are proposed for TopHat/Bowtie2:

create a launcher for TopHat using the commands shown in the previous section
launch jobs with 1, 2, 4, 8 and 10 cores on one node, using the second test files, and measure the speedup obtained
mpiBLAST

Characterization: data intensive, little RAM overhead, native parallelization

Description

mpiBLAST: Open-Source Parallel BLAST

mpiBLAST is a freely available, open-source, parallel implementation of NCBI BLAST. By efficiently utilizing distributed computational resources through database fragmentation, query segmentation, intelligent scheduling, and parallel I/O, mpiBLAST improves NCBI BLAST performance by several orders of magnitude while scaling to hundreds of processors [*].

Example

This example will be ran in an interactive session, with batch-mode executions being proposed later on as exercises.

Gaia

# Connect to Gaia (Linux/OS X):
(yourmachine)$> ssh access-gaia.uni.lu

# Request 1 full node in an interactive job:
(access-gaia)$> oarsub -I -l nodes=1,walltime=00:30:00

# Load the bioinfo software set
(node)$> module use $RESIF_ROOTINSTALL/bioinfo/modules/all
Iris

# Connect to Iris (Linux/OS X):
(yourmachine)$> ssh access-iris.uni.lu

# Request half a node in an interactive job:
(access-iris)$> s -t 0-0:30:0 -N 1 -c 1 --ntasks-per-node=14

# Load the bioinfo software set
(node)$> module use /opt/apps/resif/data/stable/bioinfo/modules/all
# Check the mpiBLAST versions installed on the clusters:
(node)$> module avail 2>&1 | grep -i mpiblast

# Load the default mpiBLAST version:
(node)$> module load bio/mpiBLAST

# Check that it has been loaded, along with its dependencies:
(node)$> module list

# The mpiBLAST binaries should now be in your path
(node)$> mpiformatdb --version
(node)$> mpiblast --version
mpiBLAST requires access to NCBI substitution matrices and pre-formatted BLAST databases. For the purposes of this tutorial, a FASTA (NR) database has been formatted and split into 12 fragments, enabling the parallel alignment of a query against the database.

A .ncbirc file containing the paths to the necessary data files can be downloaded from here and placed in your $HOME directory (make sure to backup an existing $HOME/.ncbirc before overwriting it with the one in this tutorial).

Question: Knowing that the databases can take tens of gigabytes, what is an appropriate storage location for them on the clusters?

We will run a test using mpiBLAST. Note that mpiBLAST requires running with at least 3 processes, 2 dedicated for scheduling tasks and coordinating file output, with the additional processes performing the search.

Gaia

# Go to the test directory and execute mpiBLAST with one core for search
(node)$> cd ~/bioinfo-tutorial/mpiblast
(node)$> mpirun -np 3 mpiblast -p blastp -d nr -i test.fa -o test.out

# Note the speedup when using 12 cores
(node)$> mpirun -np 12 mpiblast -p blastp -d nr -i test.fa -o test.out
Iris

# Go to the test directory and execute mpiBLAST with one core for search
(node)$> cd ~/bioinfo-tutorial/mpiblast
(node)$> srun -n 3 mpiblast -p blastp -d nr -i test.fa -o test.out

# Note the speedup when using 14 cores
(node)$> srun -n 14 mpiblast -p blastp -d nr -i test.fa -o test.out
Proposed exercises

The following exercises are proposed for mpiBLAST:

create a launcher for mpiBLAST, making sure to export the required environment to the remote nodes
launch jobs with 8, 14 and 24 cores across two nodes and measure the speedup obtained
Useful references

Gromacs parallelization
Gromacs GPU acceleration
Gromacs USA workshop
Tutorial on GROMACS parallelization schemes

Bioinformatics workflows with snakemake and conda

 Copyright (c) 2019 UL HPC Team  <hpc-sysadmins@uni.lu>
Author: Sarah Peter



In this tutorial you will learn how to run a ChIP-seq analysis with the snakemake workflow engine on the cluster.

Disclaimer: In order to keep this tutorial simple, we use default parameters for the different tools as much as possible. However, for a real analysis you should always adapt the parameters to your dataset. Also, be aware that the results of some steps might be skewed, because we only work on data from one chromosome.

Note: To make it clear where you should execute a certain command, the prompt is prefixed with the location, i.e.

(access)$> for commands on the cluster access nodes
(node)$> for commands on a cluster node inside a job
(laptop)$> for commands locally on your machine
The actual command comes only after this prefix.

Table of contents

Setup the environment
Create snakemake workflow
Mapping
Peak calling
Generate bigWig files for visualisation
Summary rule
Cluster configuration for snakemake
Adjust mapping step to run on multiple threads
Configure job parameters with cluster.yaml
Run snakemake with cluster configuration
Inspect results in IGV
(Optional) Immediately submit all jobs
(Optional) Revert the changes to your environment
Useful stuff
References
Acknowledgements
Setup the environment

For this tutorial we will use the conda package manager to install the required tools.

Conda is an open source package management system and environment management system that runs on Windows, macOS and Linux. Conda quickly installs, runs and updates packages and their dependencies. Conda easily creates, saves, loads and switches between environments on your local computer. It was created for Python programs, but it can package and distribute software for any language.

Conda as a package manager helps you find and install packages. If you need a package that requires a different version of Python, you do not need to switch to a different environment manager, because conda is also an environment manager. With just a few commands, you can set up a totally separate environment to run that different version of Python, while continuing to run your usual version of Python in your normal environment.

— Conda manual

It can encapsulate software and packages in environments, so you can have multiple different versions of a software installed at the same time and avoid incompatibilities between different tools. It also has functionality to easily port and replicate environments, which is important to ensure reproducibility of analyses.

Attention when dealing with sensitive data: Everyone can very easily contribute installation recipies to the bioconda channel, without verified identity or double-checking from the community. Therefore it's possible to insert malicious software. If you use bioconda when processing sensitive data, you should check the recipes to verify that they install software from the official sources.

We will use conda on two levels in this tutorial. First, we use a conda environment to install and run snakemake. Second, inside the snakemake workflow we will define separate conda environments for each step.

1. Connect to the cluster

2. Start an interactive job

(access)$> si
3. Install conda

(node)$> mkdir -p $SCRATCH/downloads
(node)$> cd $SCRATCH/downloads
(node)$> wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
(node)$> chmod u+x Miniconda3-latest-Linux-x86_64.sh
(node)$> ./Miniconda3-latest-Linux-x86_64.sh
You need to specify your installation destination, e.g. /home/users/<your_username>/tools/miniconda3. You must use the full path and cannot use $HOME/tools/miniconda3. Answer yes to initialize Miniconda3.

The installation will modify your .bashrc to make conda directly available after each login. To activate the changes now, run

(node)$> source ~/.bashrc
Update conda to the latest version:

(node)$> conda update conda
4. Create a new conda environment and activate it

(node)$> conda create -n bioinfo_tutorial
(node)$> conda activate bioinfo_tutorial
After validation of the creation step and once activated, you can see that your prompt will now be prefixed with (bioinfo_tutorial) to show which environment is active. For the rest of the tutorial make sure that you always have this environment active.

5. Make sure Python does not pick up packages in your home directory

First we do a backup of the .bashrc before we modify it:

(bioinfo_tutorial) (node)$> cp ~/.bashrc ~/.bashrc-$(date +%Y%m%d).bak
Then we add export PYTHONNOUSERSITE=True :

(bioinfo_tutorial) (node)$> cat << EOF >> ~/.bashrc

# Stop Python from picking up packages in $HOME/.local
export PYTHONNOUSERSITE=True
EOF
For the later parts of this tutorial to work, we need to make this setting permanent by adding it to .bashrc. However, make sure to delete those lines after the tutorial, so your manually installed python packages are found again.

6. Install snakemake

(bioinfo_tutorial) (node)$> conda install -c bioconda -c conda-forge snakemake-minimal
Create snakemake workflow

The Snakemake workflow management system is a tool to create reproducible and scalable data analyses. Workflows are described via a human readable, Python based language. They can be seamlessly scaled to server, cluster, grid and cloud environments, without the need to modify the workflow definition. Finally, Snakemake workflows can entail a description of required software, which will be automatically deployed to any execution environment.

— Snakemake manual

Snakemake is a very useful tool if you need to combine multiple steps using different software into a coherent workflow. It comes with many features desired for running workflows, like

ensuring all input and result files are present
restarting at a failed step
rerunning (parts of a) pipeline when (some of) the input changed
support for wildcards to apply a step to a set of files
automatically parallelising where possible
software management
collecting benchmark data
modularisation
creating launcher scripts and submitting jobs to the cluster
creating a visualisation of the workflow steps (see below)
In this tutorial we will analyse ChIP-seq data from the paper Gérard D, Schmidt F, Ginolhac A, Schmitz M, Halder R, Ebert P, Schulz MH, Sauter T, Sinkkonen L. Temporal enhancer profiling of parallel lineages identifies AHR and GLIS1 as regulators of mesenchymal multipotency. Nucleic Acids Research, Volume 47, Issue 3, 20 February 2019, Pages 1141–1163, https://doi.org/10.1093/nar/gky1240 published by our colleagues at LSRU.

We will set up the following workflow:

DAG

To speed up computing time we use source files that only contain sequencing reads that map to chromosome 12. The files for input (control) and H3K4me3 (ChIP) are available on the cluster in the directory /work/projects/ulhpc-tutorials/bio/snakemake/chip-seq and the corresponding reference in /work/projects/ulhpc-tutorials/bio/snakemake/reference.

We have also already prepared the conda environments for each step in the workflow in /work/projects/ulhpc-tutorials/bio/snakemake/envs.

Create a working directory and link the necessary data:

(node)$> cd $SCRATCH
(node)$> mkdir bioinfo_tutorial
(node)$> cd bioinfo_tutorial
(node)$> ln -s /work/projects/ulhpc-tutorials/bio/snakemake/chip-seq .
(node)$> ln -s /work/projects/ulhpc-tutorials/bio/snakemake/reference .
(node)$> ln -s /work/projects/ulhpc-tutorials/bio/snakemake/envs .
Mapping

In Snakemake, workflows are specified as Snakefiles. Inspired by GNU Make, a Snakefile contains rules that denote how to create output files from input files. Dependencies between rules are handled implicitly, by matching filenames of input files against output files. Thereby wildcards can be used to write general rules.

— Snakemake manual - Writing Workflows

Most importantly, a rule can consist of a name (the name is optional and can be left out, creating an anonymous rule), input files, output files, and a shell command to generate the output from the input, i.e.

rule NAME:
    input: "path/to/inputfile", "path/to/other/inputfile"
    output: "path/to/outputfile", "path/to/another/outputfile"
    shell: "somecommand {input} {output}"
— Snakemake manual - Rules

For a detailed explanation of rules, have a look at the official Snakemake tutorial.

A basic rule for mapping a fastq file with bowtie2 could look like this:

rule mapping:
  input: "chip-seq/H3K4-TC1-ST2-D0.12.fastq.gz"
  output: "bowtie2/H3K4-TC1-ST2-D0.12.bam"
  shell:
    """
    bowtie2 -x reference/Mus_musculus.GRCm38.dna_sm.chromosome.12 -U {input} | \  
    samtools sort - > {output}
    samtools index {output}
    """
Since we have two fastq files to map, we should generalise the rule with wildcards:

rule mapping:
  input: "chip-seq/{sample}.fastq.gz"
  output: "bowtie2/{sample}.bam"
  shell:
    """
    bowtie2 -x reference/Mus_musculus.GRCm38.dna_sm.chromosome.12 -U {input} | \  
    samtools sort - > {output}
    samtools index {output}
    """
Now we need to tell snakemake to use a conda environment with bowtie2 and samtools inside to run this rule. For this purpose there is a specific conda directive that can be added to the rule. It accepts a yaml file that defines the conda environment.

conda: "envs/bowtie2.yaml"
You can easily export existing conda environments to a yaml file with conda env export or write the yaml from scratch. For this step the yaml file looks like this:

name: bowtie2
channels:
  - bioconda
dependencies:
  - bowtie2
  - samtools
We also use the params directive to define the path to the reference and declutter the command-line call, as well as the log directive to define a path to permanently store the output of the execution. This is especially useful in this step to store the bowtie2 mapping statistics, which are just written to the command-line (stderr) otherwise.

To track resource usage we add the benchmark directive, which will write performance measures to a tsv file.

Create a file called Snakefile in the current directory and open it in your favourite editor, e.g.

(node)$> nano Snakefile
Add the final rule for the mapping:

rule mapping:
  input: "chip-seq/{sample}.fastq.gz"
  output: "bowtie2/{sample}.bam"
  params:
    idx = "reference/Mus_musculus.GRCm38.dna_sm.chromosome.12"
  log: "logs/bowtie2_{sample}.log"
  benchmark: "benchmarks/mapping/{sample}.tsv"
  conda: "envs/bowtie2.yaml"
  shell:
    """
    bowtie2 -x {params.idx} -U {input} 2> {log} | \
    samtools sort - > {output}
    samtools index {output}
    """
You can test the rule by specifying one of the potential outputs. We first do a dry-run with with option -n.

(node)$> snakemake -npr --use-conda bowtie2/INPUT-TC1-ST2-D0.12.bam
If everything is fine we can run the rule to create the file bowtie2/INPUT-TC1-ST2-D0.12.bam:

(node)$> snakemake -pr --use-conda bowtie2/INPUT-TC1-ST2-D0.12.bam
The first run will take a bit longer, because snakemake creates the conda environment. In subsequent runs it will just activate the existing environment. However, it will recognise if the yaml files changes and then recreate the environment.

Check the mapping statistics and the benchmark report:

(node)$> cat logs/bowtie2_INPUT-TC1-ST2-D0.12.log
400193 reads; of these:
  400193 (100.00%) were unpaired; of these:
    1669 (0.42%) aligned 0 times
    379290 (94.78%) aligned exactly 1 time
    19234 (4.81%) aligned >1 times
99.58% overall alignment rate

(node)$> cat benchmarks/mapping/INPUT-TC1-ST2-D0.12.tsv
s       h:m:s   max_rss max_vms max_uss max_pss io_in io_out mean_load
19.1737 0:00:19 262.14  1404.55 258.79  258.94  0.00  0.00   0.00
After this step your working directory should contain the following files (using the tree command):

.
|-- benchmarks
|   `-- mapping
|       `-- INPUT-TC1-ST2-D0.12.tsv
|-- bowtie2
|   |-- INPUT-TC1-ST2-D0.12.bam
|   `-- INPUT-TC1-ST2-D0.12.bam.bai
|-- chip-seq -> /work/projects/ulhpc-tutorials/bio/snakemake/chip-seq
|-- envs -> /work/projects/ulhpc-tutorials/bio/snakemake/envs
|-- logs
|   `-- bowtie2_INPUT-TC1-ST2-D0.12.log
|-- reference -> /work/projects/ulhpc-tutorials/bio/snakemake/reference/
|-- Snakefile
Peak calling

The next step in the workflow is to call peaks with MACS2. This tells us where there is enrichment of the ChIP versus the input (control).

You should always choose the peak caller based on how you expect your enriched regions to look like, e.g. narrow or broad peaks.

Besides the list of peaks in BED format, MACS2 also produces coverage tracks.

Add the following rule to your Snakefile:

rule peak_calling:
  input:
    control = "bowtie2/INPUT-{sample}.bam",
    chip = "bowtie2/H3K4-{sample}.bam"
  output:
    peaks = "output/{sample}_peaks.narrowPeak",
    control_bdg = "macs2/{sample}_control_lambda.bdg",
    chip_bdg = "macs2/{sample}_treat_pileup.bdg"
  conda: "envs/macs2.yaml"
  shell:
    """
    macs2 callpeak -t {input.chip} -c {input.control} -f BAM -g mm -n {wildcards.sample} -B -q 0.01 --outdir macs2

    cp macs2/{wildcards.sample}_peaks.narrowPeak {output.peaks}
    """
The conda environment envs/macs2.yaml for this step is:

name: macs2
channels:
  - bioconda
dependencies:
  - macs2
Let's run this step with:

(node)$> snakemake -pr --use-conda output/TC1-ST2-D0.12_peaks.narrowPeak
Note that snakemake will not run the mapping step for bowtie2/INPUT-TC1-ST2-D0.12.bam again. It only runs rules for which the output is not present or the input has changed.

After this step your working directory should contain the following files:

.
|-- benchmarks
|   `-- mapping
|       |-- H3K4-TC1-ST2-D0.12.tsv
|       `-- INPUT-TC1-ST2-D0.12.tsv
|-- bowtie2
|   |-- H3K4-TC1-ST2-D0.12.bam
|   |-- H3K4-TC1-ST2-D0.12.bam.bai
|   |-- INPUT-TC1-ST2-D0.12.bam
|   `-- INPUT-TC1-ST2-D0.12.bam.bai
|-- chip-seq -> /work/projects/ulhpc-tutorials/bio/snakemake/chip-seq
|-- envs -> /work/projects/ulhpc-tutorials/bio/snakemake/envs
|-- logs
|   |-- bowtie2_H3K4-TC1-ST2-D0.12.log
|   `-- bowtie2_INPUT-TC1-ST2-D0.12.log
|-- macs2
|   |-- TC1-ST2-D0.12_control_lambda.bdg
|   |-- TC1-ST2-D0.12_model.r
|   |-- TC1-ST2-D0.12_peaks.narrowPeak
|   |-- TC1-ST2-D0.12_peaks.xls
|   |-- TC1-ST2-D0.12_summits.bed
|   `-- TC1-ST2-D0.12_treat_pileup.bdg
|-- output
|   `-- TC1-ST2-D0.12_peaks.narrowPeak
|-- reference -> /work/projects/ulhpc-tutorials/bio/snakemake/reference/
|-- Snakefile
Generate bigWig files for visualisation

For easier visualisation and faster transfer, we convert the two coverage tracks from the MACS2 output to bigWig format.

Add the following rule to your Snakefile:

rule bigwig:
  input: "macs2/{sample}.bdg"
  output: "output/{sample}.bigwig"
  params:
    idx = "reference/Mus_musculus.GRCm38.dna_sm.chromosome.12.fa.fai"
  conda: "envs/ucsc.yaml"
  shell:
    """
    bedGraphToBigWig {input} {params.idx} {output}
    """
The conda environment envs/ucsc.yaml for this step is:

name: ucsc
channels:
  - bioconda
dependencies:
  - ucsc-bedgraphtobigwig
Let's test this step with:

(node)$> snakemake -pr --use-conda output/TC1-ST2-D0.12_treat_pileup.bigwig
This time snakemake will only run the "bigwig" rule for the one file we specified.

After this step the output directory should contain the following files:

TC1-ST2-D0.12_peaks.narrowPeak
TC1-ST2-D0.12_treat_pileup.bigwig
Summary rule

To avoid always having to specify which output file we want on the command-line, we add one rule with just inputs that defines the result files we want to have in the end. Since by default snakemake executes the first rule in the snakefile, we add this rule as the first one to the top and then we don't need to specify anything additional on the command-line.

First, at the very top of the Snakefile, define a variable for the name of the sample:

SAMPLE = "TC1-ST2-D0.12"
This makes it easier to change the Snakefile and apply it to other datasets. Snakemake is based on Python so we can use Python code inside the Snakefile. We will use f-Strings to include the variable in the file names.

Add this rule at the top of the Snakefile after the line above:

rule all:
  input: f"output/{SAMPLE}_peaks.narrowPeak", f"output/{SAMPLE}_control_lambda.bigwig", f"output/{SAMPLE}_treat_pileup.bigwig"
Finally run the workflow again, this time without a specific target file:

(node)$> snakemake --use-conda -pr
After this step the output directory should contain the following files:

TC1-ST2-D0.12_control_lambda.bigwig
TC1-ST2-D0.12_peaks.narrowPeak
TC1-ST2-D0.12_treat_pileup.bigwig
Snakemake can visualise the dependency graph of the workflow with the following command:

(node)$> snakemake --dag | dot -Tpdf > dag.pdf
DAG

Cluster configuration for snakemake

Until now the workflow just runs on a single CPU on a single machine, which is not very efficient when we have much more resources available. To speed up the computation you should check in the documentation of the software you use how it can scale. For bioinformatics tools the most common option is multithreading.

In this workflow only bowtie2 has the option to run on multiple threads.

Adjust mapping step to run on multiple threads

We add the thread directive to the snakemake rule for the mapping step, to tell snakemake that this step can use multiple threads.

The specified threads have to be seen as a maximum. When Snakemake is executed with fewer cores, the number of threads will be adjusted, i.e. threads = min(threads, cores) with cores being the number of cores specified at the command line (option -j).

— Snakemake manual - Threads

So the value for threads should be the maximum that is reasonable for the respective software. For many software the speed-up plateaus at a certain number of threads or even starts to decrease again. For a regular bowtie2 run 16 is a good maximum, but for this tutorial we will only go up to 4 because we have a small dataset.

In the mapping rule in your Snakefile add the following line after the conda directive:

  threads: 4
We also need to add the option -p {threads} to the bowtie2 command-line call, to make it actually use those threads:

    bowtie2 -p {threads} -x {params.idx} -U {input} 2> {log} | \
such that the complete mapping rule now is the following:

rule mapping:
  input: "chip-seq/{sample}.fastq.gz"
  output: "bowtie2/{sample}.bam"
  params:
    idx = "reference/Mus_musculus.GRCm38.dna_sm.chromosome.12"
  log: "logs/bowtie2_{sample}.log"
  benchmark: "benchmarks/mapping/{sample}.tsv"
  conda: "envs/bowtie2.yaml"
  threads: 4
  shell:
    """
    bowtie2 -p {threads} -x {params.idx} -U {input} 2> {log} | \
    samtools sort - > {output}
    samtools index {output}
    """
If we want to rerun the workflow to compare different options, we need to delete the output files, otherwise snakemake will not run the steps again. Fortunately snakemake has a dedicated option for this:

(node)$> snakemake all --delete-all-output
Quit your current job and start a new one with more cores to test the multithreading:

(node)$> exit
(access)$> srun --cpu-bind=none -p interactive -t 0-0:15:0 -N 1 -c 6 --ntasks-per-node=1
(node)$> conda activate bioinfo_tutorial
(node)$> cd $SCRATCH/bioinfo_tutorial
Now we also need to tell snakemake that it has multiple cores available and can run steps multithreaded or run multiple tasks in parallel. This is done with the  -j option followed by the number of available cores (e.g. the number of cores you have reserved if you run it interactively).

(node)$> snakemake -j 4 -pr --use-conda bowtie2/INPUT-TC1-ST2-D0.12.bam
You should see in the output that the command-line call of bowtie2 now shows -p 4.

Check again the benchmark report:

(node)$> cat benchmarks/mapping/INPUT-TC1-ST2-D0.12.tsv
s      h:m:s   max_rss max_vms max_uss max_pss io_in io_out mean_load
6.7687 0:00:06 295.01  1728.68 291.64  291.79  0.00  16.00  0.00
Notice that the runtime has decreased, but I/O has increased.

Exercise: Try several options for -j up to the number of cores you reserved (6) and check the bowtie2 command and the values in the benchmark. Don't forget the clean-up between the tries.

Configure job parameters with cluster.yaml

Instead of reserving an interactive job and running snakemake inside that job, we want to use snakemake's cluster functionality to make it submit jobs to Slurm. For this we create a configuration file named cluster.yaml to define the values for the different sbatch options.

Options under the __default__ header apply to all rules, but it's possible to override them selectively with rule-specific options.

Create the file cluster.yaml in the same directory as the Snakefile with the following content:

__default__:
  time: "0-00:01:00"
  partition: "batch"
  nodes: 1
  ntasks: 1
  ncpus: 1
  job-name: "{rule}"
  output: "slurm-%j-%x.out"
  error: "slurm-%j-%x.err"
mapping:
  ncpus: 4
The only settings you may need to change per rule are:

time, to adjust to the runtime of the rule,
ncpus, to adjust to the number of threads specified in the rule,
maybe partition, to go to bigmem nodes, for example,
as long as the software you use only does multithreading or doesn't scale at all. With multithreading you cannot run across multiple nodes, so nodes needs to be 1 , and snakemake always just runs one task per job, so ntasks also stays at 1.

Attention: Be aware that ncpus should match the threads directive in the respective rule. If ncpus is less than threads snakemake will reserve only  ncpus cores, but run the rule on the number of threads specified with threads . When running on the Iris cluster, the value for both should not exceed 28, the number of cores on a regular node.

Run snakemake with cluster configuration

Make sure you quit your job and run the following from the access node.

Now we need to map the variables defined in cluster.yaml to the command-line parameters of sbatch. Check the documentation on the HPC website for details about the parameters.

The meaning of the option -j changes when running in cluster mode to denote the maximum number of simultaneous jobs.

(node)$> exit
(access)$> cd $SCRATCH/bioinfo_tutorial
(access)$> conda activate bioinfo_tutorial
(access)$> snakemake all --delete-all-output

(access)$> SLURM_ARGS="-p {cluster.partition} -N {cluster.nodes} -n {cluster.ntasks} -c {cluster.ncpus} -t {cluster.time} -J {cluster.job-name} -o {cluster.output} -e {cluster.error}"

(access)$> snakemake -j 10 -pr --use-conda --cluster-config cluster.yaml --cluster "sbatch $SLURM_ARGS"
Let's have a look at the jobs that were submitted:

# only job allocations
(access)$> sacct -X --name="mapping","peak_calling","bigwig" --format JobID%15,JobName%15,AllocCPUS,Submit,Start,End,Elapsed

# including all steps
(access)$> sacct --name="mapping","peak_calling","bigwig" --format JobID%15,JobName%15,NTasks,AllocCPUS,Submit,Start,End,Elapsed,MaxVMSize
Check the submit and end time to see which jobs were running at the same time and when snakemake waited for jobs to finish.

After this step you should see a bunch of slurm-XXXXXX-<rule name>.out and slurm-XXXXXX-<rule name>.err files in your working directory, which contain the (error) logs of the different snakemake rules.

Inspect results in IGV

Now that we have completed the workflow, let's have a look at the results.

For visualisation, download IGV, or use any other genome browser of your choice.

To copy the results from the cluster to your laptop, run the following in a local terminal (Linux and MacOS) or a MobaXterm local session (Windows) and replace <your_username> with your ULHPC user login. For alternative ways to transfer files, see the documentation on the HPC website. Pay attention in which directory you are, so you can find the files again.

(laptop)$> mkdir bioinfo_tutorial
(laptop)$> cd bioinfo_tutorial

# check where you are
(laptop)$> pwd

# transfer the output directory
(laptop)$> rsync -avz --rsh='ssh -p 8022' <your_username>@access-iris.uni.lu:/scratch/users/<your_username>/bioinfo_tutorial/output .
Start IGV and select mouse mm10 as genome in the drop-down menu in the upper left. Go to "File" -> "Load from File…" and select all three files that you have copied from the cluster.

In the search box enter for example "Ahr" to check the signal around one of the genes highlighted in the paper.

Pay attention to the scale on which the two coverage tracks are shown. You can right-click on the track name on the left and select "Autoscale" to adjust the range.

When you hover over the blocks in the TC1-ST2-D0.12_peaks.narrowPeak track, you can see additional information about the called peaks, e.g. p-value. The peaks should be at the transcription start sites, because that is what H3K4me3 marks. Pay attention to the arrows in the "Refseq genes" track to see in which direction transcription goes.

IGV

(Optional) Immediately submit all jobs

Snakemake has an option to immediately submit all jobs to the cluster and tell the scheduler about the dependencies so they run in the right order. It submits the jobs one-by-one, collecting the job ID of each from the Slurm output, and then forwards those job IDs as dependencies to the follow-up jobs.

Unfortunately snakemake doesn't parse the job submission message from Slurm cleanly, so the dependency lists look like  'Submitted', 'batch', 'job', '374519', 'Submitted', 'batch', 'job', '374520' instead of being just a list of the job IDs. Therefore, we need a wrapper script to get the dependencies right.

Create a python script called immediate_submit.py with the following content:

#!/usr/bin/env python3
import os
import sys

from snakemake.utils import read_job_properties

# last command-line argument is the job script
jobscript = sys.argv[-1]

# all other command-line arguments are the dependencies
dependencies = set(sys.argv[1:-1])

# parse the job script for the job properties that are encoded by snakemake within
job_properties = read_job_properties(jobscript)

# collect all command-line options in an array
cmdline = ["sbatch"]

# set all the slurm submit options as before
slurm_args = " -p {partition} -N {nodes} -n {ntasks} -c {ncpus} -t {time} -J {job-name} -o {output} -e {error} ".format(**job_properties["cluster"])

cmdline.append(slurm_args)

if dependencies:
    cmdline.append("--dependency")
    # only keep numbers in dependencies list
    dependencies = [ x for x in dependencies if x.isdigit() ]
    cmdline.append("afterok:" + ",".join(dependencies))

cmdline.append(jobscript)

os.system(" ".join(cmdline))
Besides the dependencies this script now also takes care of all the other Slurm options, so you don't need to define SLURM_ARGS anymore in the shell.

Make the script executable:

(access)$> chmod +x immediate_submit.py
Run snakemake with the following command and replace <your_username> with your ULHPC user login:

(access)$> snakemake all --delete-all-output
(access)$> snakemake --cluster-config cluster.yaml -j 50 -pr --use-conda --immediate-submit --notemp --cluster "/scratch/users/<your_username>/bioinfo_tutorial/immediate_submit.py {dependencies}"
With squeue -u <your_username> you can check the status of the submitted jobs and see when they all have finished.

(Optional) Revert the changes to your environment

Unset PYTHONNOUSERSITE

Remove PYTHONNOUSERSITE=True, so python finds the packages in your $HOME/.local again:

Open ~/.bashrc in your favourite editor (e.g. nano or vim).
Scroll to the very end of the file.
Remove the line containing export PYTHONNOUSERSITE=True.
Remove conda

If you want to stop conda from always being active:

(access)$> conda init --reverse
In case you want to get rid of conda completely, you can now also delete the directory where you installed it (default is $HOME/miniconda3).

Useful stuff

To avoid too much overhead in the number of jobs submitted to Slurm, use thegroup directive to group rules that can run together in a single job.
If your workflow runs for longer than just a few minutes, run snakemake insidescreen or prefix it with nohup. This prevents the workflow from stopping when your SSH session get's disconnected.
If PYTHONNOUSERSITE is set, Python won’t add the user site-packages directory to sys.path. If it's not set, Python will pick up packages from the user site-packages before packages from conda environments. This can lead to errors if package versions are incompatible and you cannot be sure anymore which version of a software/package you are using.
References

Köster, Johannes and Rahmann, Sven. “Snakemake - A scalable bioinformatics workflow engine”. Bioinformatics 2012.
Gérard D, Schmidt F, Ginolhac A, Schmitz M, Halder R, Ebert P, Schulz MH, Sauter T, Sinkkonen L. Temporal enhancer profiling of parallel lineages identifies AHR and GLIS1 as regulators of mesenchymal multipotency. Nucleic Acids Research, Volume 47, Issue 3, 20 February 2019, Pages 1141–1163, https://doi.org/10.1093/nar/gky1240
Langmead B, Salzberg S. Fast gapped-read alignment with Bowtie 2. Nature Methods. 2012, 9:357-359.
Zhang et al. Model-based Analysis of ChIP-Seq (MACS). Genome Biol (2008) vol. 9 (9) pp. R137
Acknowledgements

Many thanks to Aurélien Ginolhac, Cedric Laczny, Nikola de Lange and Roland Krause for their help in developing this tutorial.

Galaxy Introduction Exercise: From Peaks to Genes

/!\ IMPORTANT NOTE: This is an old version of the "From peaks to genes" tutorial now provided by the Galaxy Training Network. For a much nicer version and many other Galaxy tutorials, check out the Galaxy Training webpage.



For a version of this tutorial with the results of important steps embedded and direct links to workflows, go to the Galaxy server, select "Shared Data" and then "Pages" from the top menu and have a look at Galaxy Introduction.

Scenario

We stumbled upon a paper (Li et al., Cell Stem Cell 2012) that contains the analysis of possible target genes of an interesting protein. The targets were obtained by ChIP-seq and the raw data is available through GEO. The list of genes however is neither in the supplement of the paper nor part of the GEO submission. The closest thing we can find is a list of the regions where the signal is significantly enriched (peaks). The goal of this exercise is to turn this list of genomic regions into a list of possible target genes.

(Disclaimer: We are not affiliated with the authors of the paper and we don't make a statement about the relevance or quality of the paper. It is just a fitting example and nothing else.)

Step 1: Upload peaks

Download the list of peaks (the file "GSE37268_mof3.out.hpeak.txt.gz") from GEO (click here to get to the GEO entry) to your computer. Use the upload button to upload the file to Galaxy and select "mm9" as the genome. Galaxy will automatically unpack the file.

This file is not in any standard format and just by looking at it, we cannot find out what the numbers in the different columns mean. In the paper they mention that they used the peak caller HPeak. By looking at the HPeak manual we can find out that the columns contain the following information:

chromosome name*
start coordinate
end coordinate
length
location within the peak that has the highest hypothetical DNA fragment coverage (summit)
not relevant
not relevant
(*Note that the first column only contains the chromosome number, and X and Y are replaced by 20 and 21 for easier numerical manipulations.)

Step 2: Get genes from UCSC

We also need a list of genes in mouse, which we can obtain from UCSC. Galaxy has the UCSC table browser integrated as a tool, so we don't need to download the data to our computers.

Tool: Get Data -> UCSC main table browser
Select clade "Mammal", genome "Mouse", assembly "mm9"
Select group "Genes and Gene Prediction Tracks", track "RefSeq Genes"
Select table "refGene"
Select region "genome"
Select output format "BED"
Click button "get output"
Click button "Send query to Galaxy"
Step 3: Adjust chromosome naming

Have a look at both input files (either in the little preview window in the history or click on the eye icon to see one in the main frame) and find out what are the differences in the chromosome naming.

Apply the following workflow to GSE37268_mof3.out.hpeak.txt: Workflow 'Add "chr" at beginning of each line'.

After importing you can in the future use it by scrolling to the bottom of the tool panel, click on "All workflows" and then on the workflow name.

From carefully reading the HPeak manual, we should remember that it puts "20" and "21" instead of "X" and "Y". So now the chromosome names all start properly with "chr", but we still have "chr20" and "chr21" instead of "chrX" and "chrY".

Tool: Text Manipulation -> Replace text in a specific column
Input: result of workflow (Text reformatting on data X)
In colum: Column 1
Find pattern: chr20
Replace with: chrX
Do the same for "chr21" and "chrY", make sure you use the result of the first replacement as input (use rerun button and change input and search/replace)
Make sure the format of the output file is "interval", otherwise change it by clicking the pencil icon (do not convert to new format, but change data type).

Step 4: Visualize peaks

To visualize the peaks it's best to convert them to BED format first, because most viewers cannot deal with interval (because interval format just exists in Galaxy).

Click on the pencil icon of the latest dataset
Under the header "Convert to new format" select "Convert Genomic Intervals to BED"
Click "Convert"
Look at the new dataset. Some columns with generic names have been added and others were removed to comply to BED format rules.
This generated a new dataset in BED format which we'll use for visualization. We will however continue to work with the interval dataset.
Display in IGB:

Go to the IGB website
Download and install the Integrated Genome Browser on your computer
Start IGB and in the right panel select species "Mus musculus" and genome version "M_musculus_Jul_2007"
Go back to Galaxy
Click on the link "View" after "display with IGB" (expanded history view of BED dataset)
Type in your HPC credentials again, to allow IGB to access the data (you might also need to allow some connections and/or accept certificates)
Back in IGB, click "Load Data" next to the scroll bar on top to get to see the new track
Step 5: Add promoter region to gene records

Tool: Operate on Genomic Intervals -> Get flanks
Input dataset: RefSeq genes from UCSC (UCSC Main on Mouse: refGene (genome))
Options: Region: "Around Start", Location: "Upstream", Offset: 10000, Length: 12000
Inspect the resulting BED file and through comparing with the input find out what this operation actually did. Just look at the contents and compare the rows in the input to the rows in the output to find out how the start and end positions changed. Rename the dataset (by clicking on the pencil icon) to reflect your findings.

Step 6: Find overlaps

Tool: Operate on Genomic Intervals -> Intersect
Return: Overlapping Intervals
of: result of step 5 (Get flanks on data X)
that intersect: result of step 3 (second Replace text)
The order of the inputs is important! We want to end up with a list of genes, so the corresponding dataset needs to be the first input.

Step 7: Count genes on different chromosomes

To get a better overview of the genes we obtained, we want to look at their distribution across the different chromosomes.

Tool: Statistics -> Count occurrences of each record
Input: result from step 6 (Intersect on data X and data X)
Select column 1 (c1) with the chromosome names
Step 8: Draw barchart

Tool: Bar chart (use tool search to find it)
Input: result of step 7
Use X Tick labels: Yes, column 2
Numerical column: c1
Plot title is up to you
Label for Y axis: number of genes
Galaxy has a second option to visualise tabular data, with built-in dynamic visualisations:

Expand the dataset view and click on the visualization icon
Choose "Charts"
Enter a chart title, e.g. "Genes on different chromsomes"
Select "Bar diagrams" -> "Regular"
On the top, click on "Add Data"
Enter a label, e.g. "count"
Values for x-axis: Column: 2 [str]
Values for y-axis: Column: 1 [int]
On the very top, click "Draw"
Step 9: Name your history

In the history column click on "Unnamed history" at the top to rename it.

Step 10: Make a workflow out of steps 6 to 8

Click on the history options and select "Extract workflow"
On the top click on "Uncheck all", then specifically check "Treat as input dataset" on GSE37268_mof3.out.hpeak.txt and UCSC Main on Mouse: refGene (genome), as well as "Include" on the Intersect, Count and Bar chart
Click "Create Workflow"
To make sure our workflow is correct we look at it in the editor and make some small adjustments.

Top menu: Workflow
Click on the name of your new workflow and select "Edit"
The individual steps are displayed as boxes and their outputs and inputs are connected through lines. When you click on a box you see the tool options on the right. Besides the tools you should see two additional boxes titled "Input dataset". These represent the data we want to feed into our workflow. Although we have our two inputs in the workflow they are missing their connection to the first tool (Intersect), because we didn't carry over the intermediate steps. Connect each input dataset to the Intersect tool by dragging the arrow pointing outwards on the right of its box (which denotes an output) to an arrow on the left of the Intersect box pointing inwards (which denotes an input). Connect each input dataset with a different input of Intersect.

You should also change the names of the input datasets to remember that the first one contains genes and the second one peaks. Don't forget to save it in the end by clicking on "Options" (top right) and selecting "Save".

Step 11: Share workflow

Share your new workflow with the person to your left.

Top menu: Workflow
Click on your workflow's name and select "Share or publish"
Click "Share with a user"
Enter the username of the person to your left
Hit "Share"
Wait for the person on your right to do the same
Reload the workflows by clicking again on "Workflow" in the top menu
Under the header "Workflows shared with you by others" you should now see your right neighbour's workflow
Click on its name and select "View"
Compare with your workflow
Step 12: Cleaning up

Download your workflow:

Top menu: Workflow
Click on your workflow's name and select "Download or Export"
Click on "Download workflow to file so that it can be saved or imported into another Galaxy server"
Save the workflow file on your computer
Clean up history:
Delete all datasets that are neither initial input nor final results. Everything that can be easily recreated or is just part of an intermediate step can go. What I would keep are the extended genes, the intersect result and the bar chart (for a real analysis I would recommend to also download all final results). Deleted datasets can be undeleted for some time (see history options) and will only be ultimately removed from the server if they aren't used somewhere else or by somebody else and stay deleted for several weeks.

You can create new histories in the history options with "Create New".

To delete old histories:

History options: Saved histories
Check the history you want to delete
Click "Delete Permanently" on the bottom if you need to free up space or just "Delete"

Scalable Science - Computational Physics, Chemistry & Engineering applications

 Copyright (c) 2015-2019 UL HPC Team  <hpc-sysadmins@uni.lu>


The objective of this session is to exemplify the execution of several common, parallel, Computational Physics, Chemistry & Engineering software on the UL HPC platform.

Targeted applications include:

OpenFOAM: CFD package for solving complex fluid flows involving chemical reactions, turbulence and heat transfer
NAMD: parallel molecular dynamics code designed for high-performance simulation of large biomolecular systems
ASE: Atomistic Simulation Environment (Python-based) with the aim of setting up, steering, and analyzing atomistic simulations
ABINIT: materials science package implementing DFT, DFPT, MBPT and TDDFT
Quantum Espresso: integrated suite of tools for electronic-structure calculations and materials modeling at the nanoscale
The tutorial will cover:

Basics for parallel execution under the SLURM scheduler
different MPI suites available on UL HPC
running simple test cases in parallel
running QuantumEspresso in parallel over a single node and over multiple nodes
running OpenFOAM in parallel over a single node and over multiple nodes
running ABINIT in parallel over a single node and over multiple nodes
the interesting case of the ASE toolkit
Prerequisites

As part of this tutorial several input files will be required and you will need to download them before following the instructions in the next sections:

Or simply clone the full tutorials repository and make a link to this tutorial

    (access-iris)$> git clone https://github.com/ULHPC/tutorials.git
    (access-iris)$> ln -s tutorials/advanced/MultiPhysics/ ~/multiphysics-tutorial
Basics

Note: you can check out either the instructions for the OAR scheduler (gaia and chaos clusters) or SLURM (for iris).

Preliminaries with SLURM for parallel execution

First of all, we will submit on the iris cluster an interactive job with 2 tasks on each of 2 compute nodes for 1 hour.

   (iris-frontend)$> si -N 2 --ntasks-per-node 2
   (node)$>
The SLURM scheduler provides several environment variables once we are inside a job, check them out with

   (node)$> env | grep SLURM_
We are interested especially in the environment variable which lists the compute nodes reserved for the job -- SLURM_NODELIST. Let's check its content:

   (node)$> echo $SLURM_NODELIST
To get the total number of cores available in the job, we can use the wordcount wc utility, in line counting mode:

   (node)$> srun hostname | wc -l
To get the number of cores available on the current compute node:

   (node)$> echo $SLURM_CPUS_ON_NODE
Some questions to think about: - How many cores are available for the job? - What's the difference between -N (nodes), -n (tasks) and -c (cores per task) when launching a job? - What allocations can we get if we specify -n but not -N?

MPI suites available on the Iris cluster

Now, let's check for the environment modules (available through Lmod) which match MPI (Message Passing Interface) the libraries that provide inter-process communication over a network:

   (node)$> module avail mpi/


   ------------------------ /opt/apps/resif/data/stable/default/modules/all ------------------------
      mpi/OpenMPI/2.1.3-GCC-6.4.0-2.28    mpi/impi/2018.1.163-iccifort-2018.1.163-GCC-6.4.0-2.28    toolchain/gompi/2018a    toolchain/iimpi/2018a

   Use "module spider" to find all possible modules.
   Use "module keyword key1 key2 ..." to search for all possible modules matching any of the "keys".
Perform the same search for the toolchains:

   (node)$> module avail toolchain/
Toolchains represent sets of compilers together with libraries commonly required to build software, such as MPI, BLAS/LAPACK (linear algebra) and FFT (Fast Fourier Transforms). For more details, see the EasyBuild page on toolchains.

For our initial tests we will use the foss toolchain which includes GCC, OpenMPI, OpenBLAS/LAPACK, ScaLAPACK(/BLACS) and FFTW:

   (node)$> module load toolchain/foss
   (node)$> module list
The main alternative to this toolchain is intel (toolchain/intel) that includes the Intel tools icc (C/C++ Compiler), ifort (Fortran compiler), impi (IntelMPI) and imkl (Math Kernel Library).

Both toolchains provide an MPI implementation, that are set up to integrate with the SLURM scheduler. One particularity of the tight integration between the MPI libraries and SLURM is that applications can be directly started in parallel with the srun command, instead of the (traditional) mpirun or mpiexec. Note that srun takes different parameters than either OpenMPI or IntelMPI's mpirun.

As of June 2019 we are testing a new set of global software with updated versions for our major applications, libraries and their dependencies. To try them out, you'll need first to switch the software set from the production one loaded by default to the development (experimental/testing) one:

Test now:

  (node)$> module load swenv/default-env/devel
  (node)$> module avail toolchain/
To go back to the production software set: (node)$> module load swenv/default-env/latest

Simple test case on iris

Now, we will compile and run a simple hellompi MPI application. Save the following source code in /tmp/hellompi.c :

   #include <mpi.h>
   #include <stdio.h>
   #include <stdlib.h>
   int main(int argc, char** argv) {
     MPI_Init(NULL, NULL);
     int world_size;
     MPI_Comm_size(MPI_COMM_WORLD, &world_size);
     int world_rank;
     MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);
     char processor_name[MPI_MAX_PROCESSOR_NAME];
     int name_len;
     MPI_Get_processor_name(processor_name, &name_len);
     printf("Hello world from %s, rank %d out of %d CPUs\n", processor_name, world_rank, world_size);
     MPI_Finalize();
   }
Load a toolchain, which will bring in a C compiler and MPI implementation and compile the above code:

   (node)$> cd /tmp
   (node)$> mpicc -o hellompi hellompi.c
Run it:

   (node)$> srun ./hellompi
Why didn't it work? Remember we stored this application on a compute node's /tmp directory, which is local to each node, not shared. Thus the application couldn't be found (more on this later) on the remote nodes.

Let's move it to the $HOME directory which is common across the cluster, and try again:

   (node)$> mv /tmp/hellompi ~/multiphysics-tutorial
   (node)$> cd ~/multiphysics-tutorial
   (node)$> srun ./hellompi
Now we will run it in different ways and see what happens:

   (node)$> srun -n 1 hellompi
   (node)$> srun -n 2 hellompi
   (node)$> srun -n 3 hellompi
   (node)$> srun -n 4 hellompi
   (node)$> srun hellompi
Note that SLURM's srun knows the environment of your job and this will drive parallel execution, if you do not override it explicitly!

Simple batch launchers for parallel code

Below follow example launchers that you may use for your MPI, MPI+OpenMP, or CUDA code:

MPI only application
#!/bin/bash -l
#SBATCH -J ParallelJob
#SBATCH -n 128
#SBATCH -c 1
#SBATCH --time=0-01:00:00
#SBATCH -p batch

module load toolchain/intel
srun /path/to/your/intel-toolchain-compiled-application
OpenMPI only application
#!/bin/bash -l
#SBATCH -J ThreadedJob
#SBATCH -N 1
#SBATCH --ntasks-per-node=1
#SBATCH -c 28
#SBATCH --time=0-01:00:00
#SBATCH -p batch

export OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK}
srun /path/to/your/threaded.app
Multi-node hybrid application MPI+OpenMP
#!/bin/bash -l
#SBATCH -J HybridParallelJob
#SBATCH -N 10
#SBATCH --ntasks-per-node=1
#SBATCH -c 28
#SBATCH --time=0-01:00:00
#SBATCH -p batch

module load toolchain/intel
export OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK}
srun -n $SLURM_NTASKS /path/to/your/parallel-hybrid-app
Multi-node multi-GPU MPI application
#!/bin/bash -l
#SBATCH -J GPUJob
#SBATCH -N 4
#SBATCH --ntasks-per-socket=4
#SBATCH -c 7
#SBATCH --gres=gpu:4
#SBATCH --time=0-01:00:00
#SBATCH -p gpu

module load toolchain/intel
module load system/CUDA

export OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK}
srun /path/to/your/gpu-app
QuantumESPRESSO

Check for the available versions of QuantumESPRESSO (QE in short), as of June 2019 this shows on the Iris cluster:

   (node)$> module avail quantumespresso
   -------------------- /opt/apps/resif/data/stable/default/modules/all --------------------
      chem/QuantumESPRESSO/6.1-intel-2018a-maxter500                        phys/Yambo/4.2.2-intel-2018a-QuantumESPRESSO-6.2.1-qexml
      chem/QuantumESPRESSO/6.1-intel-2018a                                  phys/Yambo/4.2.2-intel-2018a-QuantumESPRESSO-6.2.1-qexsd_hdf5
      chem/QuantumESPRESSO/6.2.1-intel-2018a                         (D)    phys/Yambo/4.2.2-intel-2018a-QuantumESPRESSO-6.2.1-qexsd
      phys/Yambo/r15234-intel-2018a-QuantumESPRESSO-6.2.1-qexml             phys/Yambo/4.2.2-intel-2018a-QuantumESPRESSO-6.2.1
      phys/Yambo/r15234-intel-2018a-QuantumESPRESSO-6.2.1-qexsd_hdf5        phys/Yambo/4.2.4-intel-2018a-QuantumESPRESSO-6.1
      phys/Yambo/r15234-intel-2018a-QuantumESPRESSO-6.2.1-qexsd             phys/Yambo/4.3.1-r132-intel-2018aQuantumESPRESSO-6.1          (D)
See that various versions are available, and also other applications (Yambo) that are linked to specific versions of QE are found.

One thing we note is that all versions of QE are built with support for both MPI and OpenMP. In this combination QE can give better performance than the pure MPI versions, by running only one MPI process per node (instead of one MPI process for each core in the job) that creates (OpenMP) threads which run in parallel locally and communicate over shared memory.

Load the latest QE version available in the production software environment:

   (node)$> module load chem/QuantumESPRESSO
We will use the PWscf (Plane-Wave Self-Consistent Field) package of QE for our tests. Run it in sequential mode, it will wait for your input. You should see a "Parallel version (MPI), running on 1 processors" message, and can stop it with CTRL-C:

   (node)$> pw.x
   (node)$> srun -n 1 pw.x
Now try the parallel run over all the nodes/cores in the job:

   (node)$> srun pw.x
Before stopping it, check that pw.x processes are created on the remote node. You will need to:

open a second connection to the cluster, or a second window if you're using screen or tmux
check which nodes the job is using, with squeue -j $JOBID
connect to the job on the second node from the set of nodes from the step above with sjoin $JOBID $NODENAME (e.g. sjoin 456123 iris-123)
use htop to show the processes, filter the shown list to see only your user with u and then selecting your username
Note that this check procedure can be invaluable when you are running an application for the first time, or with new options. Generally, some things to look for are:

that processes are created on the remote node, instead of all of them on the head node (which leads to huge slowdowns)
the percentage of CPU usage those processes have, for CPU-intensive work, the values in the CPU% column should be close to 100%
if the values are constantly close to 50%, or 25% (or even less) it may mean that more parallel processes were started than should have on that node (e.g. if all processes are running on the head node) and that they are constantly competing for the same cores, which makes execution very slow
the number of threads created by each process
here the number of OpenMP threads, controlled through the OMP_NUM_THREADS environment variable or Intel MKL threads (MKL_NUM_THREADS) may need to be tuned
Now we will run pw.x to perform electronic structure calculations in the presence of a finite homogeneous electric field, and we will use sample input (PW example10) to calculate high-frequency dielectric constant of bulk Silicon. For reference, many examples are given in the installation directory of QE, see $EBROOTQUANTUMESPRESSO/espresso-$EBVERSIONQUANTUMESPRESSO/PW/examples.

   (node)$> cd ~/multiphysics-tutorial
   (node)$> cd inputs/qe
   (node)$> srun -n 1 pw.x < si.scf.efield2.in
We will see the calculation progress, this serial execution (we forced srun to only use a single task) should take around 2 minutes.

Next, we will clean up the directory holding output files, and re-run the example in parallel:

   (node)$> rm -rf out
   (node)$> srun pw.x < si.scf.efield2.in > si.scf.efield2.out
When the execution ends, we can take a look at the last 10 lines of output and check the execution time:

   (node)$> tail si.scf.efield2.out
You can now try to run the same example but testing some different things:

using all cores in a single node, with one core per MPI process (28 tasks with 1 core per task)
explicitly setting the number of OpenMP threads and running 1 MPI process and 28 threads (1 task with 28 cores per task)
running across several nodes and e.g. compare execution on the broadwell nodes vs skylake based nodes (use sbatch -C broadwell to limit your jobs to nodes with this feature, idem for skylake)
Finally, we clean the environment by running module purge:

   (node)$> module purge
   (node)$> module list
References

QE: user's manual
QE: understanding parallelism
QE: running on parallel machines
QE: parallelization levels
OpenFOAM

Check for the available versions of OpenFOAM on Iris:

   (node)$> module avail openfoam
We will use the cae/OpenFOAM/v1712-intel-2018a version:

    (node)$> module load cae/OpenFOAM/v1712-intel-2018a
We load OpenFOAM's startup file:

   (node)$> source $FOAM_BASH
Now we will run the reactingParcelFoam solver of OpenFOAM on an example showing the spray-film cooling of hot boxes (lagrangian/reactingParcelFilmFoam/hotBoxes). For reference, many examples are given in the installation directory of OpenFOAM, see $FOAM_TUTORIALS.

Before the main execution, some pre-processing steps:

   (node)$> cd ~/multiphysics-tutorial/inputs/openfoam
   (node)$> cp -rf 0.org 0
   (node)$> blockMesh
   (node)$> topoSet
   (node)$> subsetMesh c0 -patch wallFilm -overwrite
   (node)$> ./patchifyObstacles > log.patchifyObstacles 2>&1
   (node)$> extrudeToRegionMesh -overwrite
   (node)$> changeDictionary
   (node)$> rm -rf system/wallFilmRegion
   (node)$> cp -r system/wallFilmRegion.org system/wallFilmRegion
   (node)$> find ./0 -maxdepth 1 -type f -exec sed -i "s/wallFilm/\"(region0_to.*)\"/g" {} \;
   (node)$> paraFoam -touch
   (node)$> paraFoam -touch -region wallFilmRegion
   (node)$> decomposePar -region wallFilmRegion
   (node)$> decomposePar
Solver execution, note the environment variables we need to export:

   (node)$> srun --export MPI_BUFFER_SIZE,WM_PROJECT_DIR reactingParcelFilmFoam -parallel
Note that the solver execution will take a long time - you can interrupt and/or test in a larger job, which would require:

editing the decomposeParDict file to change the numberOfSubdomains directive in order to match the new number of processes
then rerun the decomposePar commands as above
Parenthesis: how can you achieve the best (fastest) execution time? Some questions to think about:

is just increasing the number of cores optimal? (why not?)
how can you increase processing speed on Iris?
would you get an additional speedup if you compile OpenFOAM on the most recent architecture Iris nodes to take advantage of the newer instruction set available in their CPUs? (yes!)
After the main execution, post-processing steps:

   (node)$> reconstructPar -region wallFilmRegion
   (node)$> reconstructPar
You can now try to copy and run additional examples from OpenFOAM, note:

the ones which include an Allrun-parallel file can be run in parallel
you can run the Allrun.pre script to prepare the execution
you have to run yourself further pre-execution instructions from the Allrun-parallel script
instead of runParallel $application 4 you will have to run mpirun with the correct parameters and the particular application name yourself
last post-processing steps from Allrun-parallel have to be run manually
Finally, we clean the environment:

   (node)$> module purge
   (node)$> module list
References

OpenFOAM: user's guide
OpenFOAM: running applications in parallel
ABINIT

Check for the available versions of ABINIT and load the latest:

   (node)$> module load abinit
   (node)$> module load chem/ABINIT
We will use one of ABINIT's parallel test cases to exemplify parallel execution. For reference, many examples are given in the installation directory of ABINIT, see $EBROOTABINIT/share/abinit-test.

   (node)$> cd ~/multiphysics-tutorial/inputs/abinit
   (node)$> srun abinit < si_kpt_band_fft.files
After some initial processing and messages, we will see:

    finddistrproc.F90:394:WARNING
    Your input dataset does not let Abinit find an appropriate process distribution with nproc=    4
    Try to comment all the np* vars and set paral_kgb=    -4 to have advices on process distribution.

    abinit : WARNING -
     The product of npkpt, npfft, npband and npspinor is bigger than the number of processors.
     The user-defined values of npkpt, npfft, npband or npspinor will be modified,
     in order to bring this product below nproc .
     At present, only a very simple algorithm is used ...

    abinit : WARNING -
     Set npfft to 1

    initmpi_grid.F90:108:WARNING
      The number of band*FFT*kpt*spinor processors, npband*npfft*npkpt*npspinor should be
     equal to the total number of processors, nproc.
     However, npband   =    2           npfft    =    1           npkpt    =    1           npspinor =    1       and nproc    =    4
As shown above, ABINIT itself can give details into how to tune input parameters for the dataset used.

Edit the input file si_kpt_band_fft.in as per ABINIT's instructions, then re-run ABINIT. The following message will be shown, with a list of parameters that you will need to edit in si_kpt_band_fft.

   "Computing all possible proc distrib for this input with nproc less than      4"
Next, ensure you can now run ABINIT on this example to completion.

Parenthesis: will a parallel application always allow execution on any number of cores? Some questions to think about:

are there cases where an input problem cannot be split in some particular ways? (yes!)
are all ways to split a problem optimal for solving it as fast as possible? (no!)
is it possible to split a problem such that the solver has unbalanced cases and works much slower? (yes)
is there a generic way to tune the problem in order to be solved as fast as possible? (no, it's domain & application specific!)
Finally, we clean the environment:

   (node)$> module purge
   (node)$> module list
References

ABINIT: user's guide
ABINIT: tutorials
NAMD

NAMD, recipient of a 2002 Gordon Bell Award and a 2012 Sidney Fernbach Award, is a parallel molecular dynamics code designed for high-performance simulation of large biomolecular systems. Based on Charm++ parallel objects, NAMD scales to hundreds of cores for typical simulations and beyond 500,000 cores for the largest simulations.

The latest NAMD 2.13 is available on the iris cluster as of June 2019 in the development, and on Debian 8 nodes of gaia as of August 2017, let's check for it:

    (node)$> module avail namd
    (node)$> module load swenv/default-env/devel
    (node)$> module avail namd

    --------------------------- /opt/apps/resif/data/devel/default/modules/all ---------------------------
       chem/NAMD/2.13-foss-2019a-mpi
We will use one of the benchmark inputs of NAMD to test it, specifically the reference STMV (virus) benchmark (1,066,628 atoms, periodic, PME).

    (node)$> cd ~/multiphysics-tutorial/inputs/namd
    (node)$> tar xf stmv.tar.gz
    (node)$> cd stmv
    (node)$> module load chem/NAMD
Now, we will need to set the outputName parameter within the input file to the path that we want:

    (node)$> sed -i 's/^outputName.*$/outputName    generated-data/g' stmv.namd
Next we will perform the parallel execution of NAMD, showing its runtime output both on console and storing it to file using tee:

    (node)$> srun namd2 stmv.namd | tee out
ASE

ASE is a Python library for working with atoms *.

ASE can interface with many external codes as calculators: Asap, GPAW, Hotbit, ABINIT, CP2K, CASTEP, DFTB+, ELK, EXCITING, FHI-aims, FLEUR, GAUSSIAN, Gromacs, Jacapo, LAMMPS, MOPAC, NWChem, SIESTA, TURBOMOLE and VASP. More details on the official webpage.

Let us run the official short example structure optimization of hydrogen molecule on the Iris cluster. Note that parallel executions of the external codes require specific environment variables to be set up, e.g. for NWChem it's ASE_NWCHEM_COMMAND which needs to include the srun parallel job launcher of Iris, which integrates with the MPI suites.

  (node)$> module avail NWChem ASE
  (node)$> module load chem/ASE/3.17.0-intel-2019a-Python-2.7.15 chem/NWChem/6.8.revision47-intel-2019a-Python-2.7.15
  (node)$> export ASE_NWCHEM_COMMAND='srun nwchem PREFIX.nw > PREFIX.out'
  (node)$> python
         >>> from ase import Atoms
         >>> from ase.optimize import BFGS
         >>> from ase.calculators.nwchem import NWChem
         >>> from ase.io import write
         >>> h2 = Atoms('H2', positions=[[0, 0, 0], [0, 0, 0.7]])
         >>> h2.calc = NWChem(xc='PBE')
         >>> opt = BFGS(h2)
         >>> opt.run(fmax=0.02)
               Step     Time          Energy         fmax
         BFGS:    0 11:55:55      -31.435218        2.2691
         BFGS:    1 11:55:55      -31.490762        0.3740
         BFGS:    2 11:55:56      -31.492780        0.0630
         BFGS:    3 11:55:57      -31.492837        0.0023
         True
         >>> write('H2.xyz', h2)
         >>> h2.get_potential_energy()
         -31.49283665375563
Note that the (very) important part here was to let ASE know how it can run NWChem in parallel, by explicitly setting the environment variable ASE_NWCHEM_COMMAND to the parallel execution commands for NWChem.

References

ASE: tutorials
ASE: calculators
Now practice!

The main objective of this session was to get you accustomed to running scalable applications in parallel.

Remember that the benefit of running in a HPC/supercomputing environment comes only if your application can take advantage of parallel processing.

Now it's up to you to run your own test-cases, and discover how to optimize your executions on the UL HPC platform.

Big Data Applications (batch, stream, hybrid)

 Copyright (c) 2013-2021 S. Varrette and UL HPC Team  <hpc-team@uni.lu>


The objective of this tutorial is to demonstrate how to build and run on top of the UL HPC platform a couple of reference analytics engine for large-scale Big Data processing, i.e. Hadoop, Flink or Apache Spark.

Pre-requisites

Ensure you are able to connect to the UL HPC clusters. In particular, recall that the module command is not available on the access frontends. For all tests and compilation, you MUST work on a computing node

(laptop)$ ssh aion-cluster  # or iris-cluster
Now you'll need to pull the latest changes in your working copy of the ULHPC/tutorials you should have cloned in ~/git/github.com/ULHPC/tutorials (see "preliminaries" tutorial)

(access)$ cd ~/git/github.com/ULHPC/tutorials
(access)$ git pull
Now configure a dedicated directory ~/tutorials/bigdata for this session

# return to your home
(access)$> mkdir -p ~/tutorials/bigdata
(access)$> cd ~/tutorials/bigdata
# create a symbolic link to the reference material
(access)$> ln -s ~/git/github.com/ULHPC/tutorials/bigdata ref.d
# Prepare a couple of symbolic links that will be useful for the training
(access)$> ln -s ref.d/scripts .     # Don't forget trailing '.' means 'here'
(access)$> ln -s ref.d/settings .    # idem
(access)$> ln -s ref.d/src .         # idem
Advanced users (eventually yet strongly recommended), create a Tmux session (see Tmux cheat sheet and tutorial) or GNU Screen session you can recover later. See also "Getting Started" tutorial .

SOCKS 5 Proxy plugin (optional but VERY useful)

Many Big Data Analytics framework (including for the Jupyter Notenooks, the Dask dashboard etc.) involves a web interface (at the level of the master and/or the workers) you probably want to access in a relative transparent way.

Relying on SSH tunnels forwarding is of course one way opf proceeding, yet that's not the most convenient. A more user-friendly approach consists in rely on a SOCKS proxy, which is basically an SSH tunnel in which specific applications forward their traffic down the tunnel to the server, and then on the server end, the proxy forwards the traffic out to the general Internet. Unlike a VPN, a SOCKS proxy has to be configured on an app by app basis on the client machine, but can be set up without any specialty client agents.

These steps were also described in the Preliminaries tutorial.

Setting Up the Tunnel

To initiate such a SOCKS proxy using SSH (listening on localhost:1080 for instance), you simply need to use the -D 1080 command line option when connecting to the cluster:

(laptop)$> ssh -D 1080 -C iris-cluster
-D: Tells SSH that we want a SOCKS tunnel on the specified port number (you can choose a number between 1025-65536)
-C: Compresses the data before sending it
Configuring Firefox to Use the Tunnel: see Preliminaries tutorial

We will see later on (in the section dedicated to Spark) how to effectively use this configuration.

Getting Started with Hadoop

Hadoop (2.10.0) is provided to you as a module:

module av Hadoop
module load tools/Hadoop
When doing that, the Hadoop distribution is installed in $EBROOTHADOOP (this is set by Easybuild for any loaded software.)

The below instructions are based on the official tutorial.

Hadoop in Single mode

By default, Hadoop is configured to run in a non-distributed mode, as a single Java process. This is useful for debugging.

Let's test it

mkdir -p runs/hadoop/single/input
cd runs/hadoop/single
# Prepare input data
mkdir input
cp ${EBROOTHADOOP}/etc/hadoop/*.xml input
# Map-reduce grep <pattern> -- result is produced in output/
hadoop jar ${EBROOTHADOOP}/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.10.0.jar grep input output 'dfs[a-z.]+'
[...]
        File System Counters
                FILE: Number of bytes read=1292102
                FILE: Number of bytes written=3190426
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
        Map-Reduce Framework
                Map input records=1
                Map output records=1
                Map output bytes=17
                Map output materialized bytes=25
                Input split bytes=168
                Combine input records=0
                Combine output records=0
                Reduce input groups=1
                Reduce shuffle bytes=25
                Reduce input records=1
                Reduce output records=1
                Spilled Records=2
                Shuffled Maps =1
                Failed Shuffles=0
                Merged Map outputs=1
                GC time elapsed (ms)=5
                Total committed heap usage (bytes)=1019740160
        Shuffle Errors
                BAD_ID=0
                CONNECTION=0
                IO_ERROR=0
                WRONG_LENGTH=0
                WRONG_MAP=0
                WRONG_REDUCE=0
        File Input Format Counters
                Bytes Read=123
        File Output Format Counters
                Bytes Written=23
# Check the results on the local filesystem
$> cat output/*
1       dfsadmin
You can also view the output files on the distributed filesystem:

hdfs dfs -cat output/*
Pseudo-Distributed Operation

Hadoop can also be run on a single-node in a pseudo-distributed mode where each Hadoop daemon runs in a separate Java process. Follow the official tutorial to ensure you are running in Single Node Cluster

Once this is done, follow the official Wordcount instructions

 # Interactive job on 2 nodes:
 si -N 2 --ntasks-per-node 1 -c 16 -t 2:00:00
 ```

```bash
cd ~/tutorials/bigdata
# Pseudo-Distributed operation
mkdir -p run/shared/hadoop

Full cluster setup

Follow the official instructions of the Cluster Setup.

Once this is done, Repeat the execution of the official Wordcount example.

Apache Flink

Apache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. Flink has been designed to run in all common cluster environments, perform computations at in-memory speed and at any scale.



Flink is available as a module:

$ module load devel/Flink
Follow the official Flink Hands-on training It should be fine in standalone mode, yet to run Flink in a fully distributed fashion on top of a static (but possibly heterogeneous) cluster requires more efforts. For instance, you won't be able to start directly the start-cluster.sh script as the log settings (among other) need to be defined and inherit from the Slurm reservation. This complex setup is illustrated with another very popular Big Data analytics framework: Spark.

Big Data Analytics with Spark

The objective of this section is to compile and run on Apache Spark on top of the UL HPC platform.

Apache Spark is a large-scale data processing engine that performs in-memory computing. Spark offers bindings in Java, Scala, Python and R for building parallel applications. high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs. It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, MLlib for machine learning, GraphX for graph processing, and Spark Streaming.

As for Hadoop, we are first going to build Spark using Easybuild before performing some basic examples. More precisely, in this part, we will review the basic usage of Spark in two cases:

a single conffiguration where the classical interactive wrappers (pyspark, scala and R wrappers) will be reviewed.
a Standalone cluster configuration - a simple cluster manager included with Spark that makes it easy to set up a cluster), where we will run the Pi estimation.
Building a more recent version of Spark with Easybuild

Spark is present as a module on the ULHPC platform yet it is a relatively old version (2.4.3). So we are first going to install a newer version ([3.1.1(https://spark.apache.org/releases/spark-release-3-1-1.html)) using EasyBuild. For this reason, you should first check the "Using and Building (custom) software with EasyBuild on the UL HPC platform" tutorial. As mentioned at that occasion, when you're looking for a more recent version of a given software (Spark here) than the one provided, you will typically search for the most recent version of Spark provided by Easybuild with eb -S <pattern>

As it might be tricky to guess the most appropriate version, the script scripts/suggest-easyconfigs -v <version> <pattern> is provided

Let's do that with Spark

### Have an interactive job for the build
(access)$> cd ~/tutorials/bigdata
(access)$> si -c4
# properly configure Easybuild prefix and local build environment
$ cat settings/default.sh
$ source settings/default.sh
$ eb --version
$ echo $EASYBUILD_PREFIX
Now let's check the available easyconfigs for Spark:

$ eb -S Spark
# search for an exact match
$ ./scripts/suggest-easyconfigs -v ${RESIF_VERSION_PROD} Spark
=> Searching Easyconfigs matching pattern 'Spark'
Spark-1.3.0.eb
Spark-1.4.1.eb
Spark-1.5.0.eb
Spark-1.6.0.eb
Spark-1.6.1.eb
Spark-2.0.0.eb
Spark-2.0.2.eb
Spark-2.2.0-Hadoop-2.6-Java-1.8.0_144.eb
Spark-2.2.0-Hadoop-2.6-Java-1.8.0_152.eb
Spark-2.2.0-intel-2017b-Hadoop-2.6-Java-1.8.0_152-Python-3.6.3.eb
Spark-2.3.0-Hadoop-2.7-Java-1.8.0_162.eb
Spark-2.4.0-Hadoop-2.7-Java-1.8.eb
Spark-2.4.0-foss-2018b-Python-2.7.15.eb
Spark-2.4.0-intel-2018b-Hadoop-2.7-Java-1.8-Python-3.6.6.eb
Spark-2.4.0-intel-2018b-Python-2.7.15.eb
Spark-2.4.0-intel-2018b-Python-3.6.6.eb
Spark-2.4.5-intel-2019b-Python-3.7.4-Java-1.8.eb
Spark-3.0.0-foss-2018b-Python-2.7.15.eb
Spark-3.0.0-intel-2018b-Python-2.7.15.eb
Spark-3.1.1-fosscuda-2020b.eb
Spark-3.1.1-foss-2020a-Python-3.8.2.eb
Total: 21 entries

... potential exact match for 2020b toolchain
Spark-3.1.1-fosscuda-2020b.eb
 --> suggesting 'Spark-3.1.1-fosscuda-2020b.eb'
As can be seen, a GPU enabled version is proposed but won't be appropriate on Aion compute node. In that case, you'll likely want to create and adapt an existing easyconfig -- see official tutorial. While out of scope in this session, here is how you would typically proceed:

Copy the easyconfig file locally:
eb --copy-ec Spark-3.1.1-fosscuda-2020b.eb Spark-3.1.1-foss-2020b.eb
(eventually) Rename the file to match the target version
Check on the website for the most up-to-date version of the software released
Adapt the filename of the copied easyconfig to match the target version / toolchain
Edit the content of the easyconfig
You'll typically have to adapt the version of the dependencies (use again scripts/suggest-easyconfigs -s  dep1 dep2 [...]) and the checksum(s) of the source/patch files to match the static versions set for the target toolchain, enforce https urls etc.
You may have to repeat that process for the dependencies. And if you succeed, kindly do not forget to submitting your easyconfig as pull requests (--new-pr) to the Easybuild community.

To save some time, the appropriate easyconfigs file Spark-3.1.1-foss-2020b-Python-3.8.6.eb (and its dependency Apache Arrow) that you can use to build locally this application on top of the UL HPC Software set according to the recommended guidelines.

# If not done yet, properly configure Easybuild prefix and local build environment
$ source settings/default.sh
$ echo $EASYBUILD_PREFIX      # Check the format which must be:
#    <home>/.local/easybuild/<cluster>/<version>/epyc
Now you can build Spark from the provided easyconfigs -- the -r/--robot option control the robot search path for Easybuild (where to search for easyconfigs):

# Dry-run: check the matched dependencies
$ eb src/Spark-3.1.1-foss-2020b-Python-3.8.6.eb -D -r src:   # <-- don't forget the trailing ':'
# only Arrow and Spark should noyt be checked
# Launch the build
$ eb src/Spark-3.1.1-foss-2020b-Python-3.8.6.eb -r src:
Installation will last ~8 minutes using a full Aion node (-c 128). In general it is preferable to make builds within a screen session.

Once the build is completed, recall that it was installed under your homedir under ~/.local/easybuild/<cluster>/<version>/epyc when the default EASYBUILD_PREFIX target (for the sake of generality) to ~/.local/easybuild/. So if you want to access the installed module within another job, you'll need to load the settings settings/default.sh (to correct the values of the variables EASYBUILD_PREFIX and LOCAL_MODULES and invoke mu:

$ source settings/default.sh
$ mu    # shorcut for module use $LOCAL_MODULES
$ module av Spark   # Must display the build version (3.1.1)
Interactive usage

Exit your reservation to reload one with the --exclusive flag to allocate an exclusive node -- it's better for big data analytics to dedicated full nodes (properly set). Let's load the installed module:

(laptop)$ ssh aion-cluster
(access)$ si -c 128 --exclusive -t 2:00:00
$ source settings/default.sh   # See above remark
$ mu    # not required
$ module load devel/Spark/3.1.1
As in the GNU Parallel tutorial, let's create a list of images from the OpenImages V4 data set. A copy of this data set is available on the ULHPC facility, under /work/projects/bigdata_sets/OpenImages_V4/. Let's create a CSV file which contains a random selection of 1000 training files within this dataset (prefixed by a line number). You may want to do it as follows (copy the full command):

#                                                       training set     select first 10K  random sort  take only top 10   prefix by line number      print to stdout AND in file
#                                                         ^^^^^^           ^^^^^^^^^^^^^   ^^^^^^^^     ^^^^^^^^^^^^^      ^^^^^^^^^^^^^^^^^^^^^^^^   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
$ find /work/projects/bigdata_sets/OpenImages_V4/train/ -print | head -n 10000 | sort -R   |  head -n 1000       | awk '{ print ++i","$0 }' | tee openimages_v4_filelist.csv
1,/work/projects/bigdata_sets/OpenImages_V4/train/6196380ea79283e0.jpg
2,/work/projects/bigdata_sets/OpenImages_V4/train/7f23f40740731c03.jpg
3,/work/projects/bigdata_sets/OpenImages_V4/train/dbfc1b37f45b3957.jpg
4,/work/projects/bigdata_sets/OpenImages_V4/train/f66087cdf8e172cd.jpg
5,/work/projects/bigdata_sets/OpenImages_V4/train/5efed414dd8b23d0.jpg
6,/work/projects/bigdata_sets/OpenImages_V4/train/1be054cb3021f6aa.jpg
7,/work/projects/bigdata_sets/OpenImages_V4/train/61446dee2ee9eb27.jpg
8,/work/projects/bigdata_sets/OpenImages_V4/train/dba2da75d899c3e7.jpg
9,/work/projects/bigdata_sets/OpenImages_V4/train/7ea06f092abc005e.jpg
10,/work/projects/bigdata_sets/OpenImages_V4/train/2db694eba4d4bb04.jpg
Download also another data files from Uber:

curl -o src/uber.csv https://gitlab.com/rahasak-labs/dot/-/raw/master/src/main/resources/uber.csv
Pyspark

PySpark is the Spark Python API and exposes Spark Contexts to the Python programming environment.

$> pyspark
pyspark
Python 3.8.6 (default, Sep  3 2021, 01:03:58)
[GCC 10.2.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.1.1
      /_/

Using Python version 3.8.6 (default, Sep  3 2021 01:03:58)
Spark context Web UI available at http://aion-84.aion-cluster.uni.lux:4040
Spark context available as 'sc' (master = local[*], app id = local-1637268453800).
SparkSession available as 'spark'.
>>>
See this tutorial for playing with pyspark.

In particular, play with the build-in filter(), map(), and reduce() functions which are all common in functional programming.

>>> txt = sc.textFile('file:////home/users/svarrette/tutorials/bigdata/openimages_v4_filelist.csv')
>>> print(txt.count())
1000
>>> txt2 = sc.textFile('file:////home/users/svarrette/tutorials/bigdata/src/uber.csv')
>>> print(txt2.count())
652436
>>> python_lines = txt.filter(lambda line: 'python' in line.lower())
>>> print(python_lines.count())
6
>>> big_list = range(10000)
>>> rdd = sc.parallelize(big_list, 2)
>>> odds = rdd.filter(lambda x: x % 2 != 0)
>>> odds.take(5)
[1, 3, 5, 7, 9]
Scala Spark Shell

Spark includes a modified version of the Scala shell that can be used interactively. Instead of running pyspark above, run the spark-shell command:

$> spark-shell
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Spark context Web UI available at http://aion-1.aion-cluster.uni.lux:4040
Spark context available as 'sc' (master = local[*], app id = local-1637272004201).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.1.1
      /_/

Using Scala version 2.12.10 (OpenJDK 64-Bit Server VM, Java 11.0.2)
Type in expressions to have them evaluated.
Type :help for more information.

scala>
R Spark Shell

The Spark R API is still experimental. Only a subset of the R API is available -- See the SparkR Documentation. Since this tutorial does not cover R, we are not going to use it.

Running Spark in standalone cluster

Reference Documentation
Spark applications run as independent sets of processes on a cluster, coordinated by the SparkContext object in your main program (called the driver program).

Specifically, to run on a cluster, the SparkContext can connect to several types of cluster managers (either Spark’s own standalone cluster manager, Mesos or YARN), which allocate resources across applications. Once connected, Spark acquires executors on nodes in the cluster, which are processes that run computations and store data for your application. Next, it sends your application code (defined by JAR or Python files passed to SparkContext) to the executors. Finally, SparkContext sends tasks to the executors to run.



There are several useful things to note about this architecture:

Each application gets its own executor processes, which stay up for the duration of the whole application and run tasks in multiple threads. This has the benefit of isolating applications from each other, on both the scheduling side (each driver schedules its own tasks) and executor side (tasks from different applications run in different JVMs). However, it also means that data cannot be shared across different Spark applications (instances of SparkContext) without writing it to an external storage system.
Spark is agnostic to the underlying cluster manager. As long as it can acquire executor processes, and these communicate with each other, it is relatively easy to run it even on a cluster manager that also supports other applications (e.g. Mesos/YARN).
The driver program must listen for and accept incoming connections from its executors throughout its lifetime (e.g., see spark.driver.port in the network config section). As such, the driver program must be network addressable from the worker nodes.
Because the driver schedules tasks on the cluster, it should be run close to the worker nodes, preferably on the same local area network. If you'd like to send requests to the cluster remotely, it's better to open an RPC to the driver and have it submit operations from nearby than to run a driver far away from the worker nodes.
Cluster Manager

Spark currently supports three cluster managers:

Standalone – a simple cluster manager included with Spark that makes it easy to set up a cluster.
Apache Mesos – a general cluster manager that can also run Hadoop MapReduce and service applications.
Hadoop YARN – the resource manager in Hadoop 2.
In this session, we will deploy a standalone cluster, which consists of performing the following workflow (with the objective to prepare a launcher script):

create a master and the workers. Check the web interface of the master.
submit a spark application to the cluster using the spark-submit script
Let the application run and collect the result
stop the cluster at the end.
To facilitate these steps, Spark comes with a couple of scripts you can use to launch or stop your cluster, based on Hadoop's deploy scripts, and available in $EBROOTSPARK/sbin:

Script	Description
sbin/start-master.sh	Starts a master instance on the machine the script is executed on.
sbin/start-slaves.sh	Starts a slave instance on each machine specified in the conf/slaves file.
sbin/start-slave.sh	Starts a slave instance on the machine the script is executed on.
sbin/start-all.sh	Starts both a master and a number of slaves as described above.
sbin/stop-master.sh	Stops the master that was started via the bin/start-master.sh script.
sbin/stop-slaves.sh	Stops all slave instances on the machines specified in the conf/slaves file.
sbin/stop-all.sh	Stops both the master and the slaves as described above.
Yet the ULHPC team has designed a dedicated launcher script ./scripts/launcher.Spark.sh that exploits these script to quickly deploy and in a flexible way a Spark cluster over the resources allocated by slurm.

Quit your previous job - eventually detach from your screen session Ensure that you have connected by SSH to the cluster by opening an SOCKS proxy:

(laptop)$> ssh -D 1080 -C aion-cluster
Then make a new reservation across multiple full nodes:

# If not yet done, go to the appropriate directory
$ cd ~/tutorials/bigdata
# Play with -N to scale as you wish (or not) - below allocation is optimizing Aion compute nodes
#              on iris: use '-N <N> --ntasks-per-node 2 -c 14'
# You'll likely need to reserve less nodes to satisfy all demands ;(
$ salloc -N 2 --ntasks-per-node 8 -c 16 --exclusive # --reservation=hpcschool
$ source settings/default.sh
$ module load devel/Spark
# Deploy an interactive Spark cluster **ACROSS** all reserved nodes
$ ./scripts/launcher.Spark.sh -i
SLURM_JOBID  = 64441
SLURM_JOB_NODELIST = aion-[0003-0004]
SLURM_NNODES = 2
SLURM_NTASK  = 16
Submission directory = /mnt/irisgpfs/users/svarrette/tutorials/bigdata
starting org.apache.spark.deploy.master.Master, logging to /home/users/svarrette/.spark/logs/spark-64441-org.apache.spark.deploy.master.Master-1-aion-0001.out
==========================================
============== Spark Master ==============
==========================================
url: spark://aion-0003:7077
Web UI: http://aion-0003:8082

===========================================
============ 16 Spark Workers ==============
===========================================
export SPARK_HOME=$EBROOTSPARK
export MASTER_URL=spark://aion-0003:7077
export SPARK_DAEMON_MEMORY=4096m
export SPARK_WORKER_CORES=16
export SPARK_WORKER_MEMORY=61440m
export SPARK_EXECUTOR_MEMORY=61440m

 - create slave launcher script '/home/users/svarrette/.spark/worker/spark-start-slaves-64441.sh'
==========================================
        *** Interactive mode ***
==========================================
Ex of submission command:
    module load devel/Spark
    export SPARK_HOME=$EBROOTSPARK
    spark-submit \
        --master spark://$(scontrol show hostname $SLURM_NODELIST | head -n 1):7077 \
        --conf spark.driver.memory=${SPARK_DAEMON_MEMORY} \
        --conf spark.executor.memory=${SPARK_EXECUTOR_MEMORY} \
        --conf spark.python.worker.memory=${SPARK_WORKER_MEMORY} \
        $SPARK_HOME/examples/src/main/python/pi.py 1000
As we are in interactive mode (-i option of the launcher script), copy/paste the export commands mentioned by the command to have them defined in your shell -- DO NOT COPY the above output but the one obtained on your side when launching the script.

You can transparently access the Web UI (master web portal, on http://<IP>:8082) using a SOCKS 5 Proxy Approach. Recall that this is possible as soon you have initiated an SSH connection with -D 1080 flag option to open on the local port 1080:

(laptop)$> ssh -D 1080 -C aion-cluster
Now, enable the ULHPC proxy setting from Foxy Proxy extension (Firefox recommended) and access transparently the Web UI of the master process by entering the provided URL http://aion-<N>:8082 -- if you haven't enabled the remote DNS resolution, you will need to enter the url http://172.21.XX.YY:8082/ (adapt the IP).

It is worth to note that:

The memory in use exceed the capacity of a single node, demonstrated if needed the scalability of the proposed setup
The number of workers (and each of their memory) is automatically defined by the way you have request your jobs (-N 2 --ntasks-per-node 8 in this case).
Each worker is multithreaded and execute on 16 cores, except one which has 1 less core (thread) available (15) than the others -- note that this value is also automatically inherited by the slurm reservation (-c 16 in this case).
1 core is indeed reserved for the master process.
As suggested, you can submit a Spark jobs to your freshly deployed cluster with spark-submit:

spark-submit \
        --master spark://$(scontrol show hostname $SLURM_NODELIST | head -n 1):7077 \
        --conf spark.driver.memory=${SPARK_DAEMON_MEMORY} \
        --conf spark.executor.memory=${SPARK_EXECUTOR_MEMORY} \
        --conf spark.python.worker.memory=${SPARK_WORKER_MEMORY} \
        $SPARK_HOME/examples/src/main/python/pi.py 1000
And check the effect on the master portal. At the end, you should have a report of the Completed application as in the below screenshot.



When you have finished, don't forget to close your tunnel and disable FoxyProxy on your browser.

Passive jobs examples:

$> sbatch ./launcher.Spark.sh
[...]
Once finished, you can check the result of the default application submitted (in result_${SLURM_JOB_NAME}-${SLURM_JOB_ID}.out).

$> cat result_${SLURM_JOB_NAME}-${SLURM_JOB_ID}.out
Pi is roughly 3.141420
In case of problems, you can check the logs of the daemons in ~/.spark/logs/

Further Reading

You can find on the Internet many resources for expanding your HPC experience with Spark. Here are some links you might find useful to go further:

Using Spark with GPFS on the ACCRE Cluster
ARIS notes on Spark
Deployment of Spark and HDFS with Singularity

This tutorial will build a Singularity container with Apache Spark, Hadoop HDFS and Java. We will deploy a Big Data cluster running Singularity through Slurm over Iris or Aion on CPU-only nodes.

Step 1: Required software

Create a virtual machine with Ubuntu 18.04 and having Docker and Singularity installed.
This project will leverage the following scripts: scripts/Dockerfile and its scripts/docker-entrypoint.sh
Step 2: Create the docker container

Clean and create the Spark+Hadoop+Java Docker container that will be later used by Singularity
sudo docker system prune -a
sudo docker build . --tag sparkhdfs
The Dockerfile contains steps to install Apache Spark, Hadoop and JDK11:
# Start from a base image
FROM ubuntu:18.04 AS builder

# Avoid prompts with tzdata
ENV DEBIAN_FRONTEND=noninteractive

# Set the working directory in the container
WORKDIR /usr/local

# Update Ubuntu Software repository
RUN apt-get update
RUN apt-get install -y curl unzip zip

# Install wget
RUN apt-get install -y wget

# Download Apache Hadoop
RUN wget https://downloads.apache.org/hadoop/core/hadoop-3.3.5/hadoop-3.3.5.tar.gz
RUN tar xvf hadoop-3.3.5.tar.gz 
RUN mv hadoop-3.3.5 hadoop

# Download Apache Spark
RUN wget https://dlcdn.apache.org/spark/spark-3.4.0/spark-3.4.0-bin-hadoop3.tgz
RUN tar xvf spark-3.4.0-bin-hadoop3.tgz
RUN mv spark-3.4.0-bin-hadoop3 spark

# Final stage
FROM ubuntu:18.04

COPY --from=builder \
/usr/local/hadoop /opt/hadoop

# Set environment variables for Hadoop
ENV HADOOP_HOME=/opt/hadoop
ENV PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

COPY --from=builder \
/usr/local/spark /opt/spark

# Set environment variables for Spark
ENV SPARK_HOME=/opt/spark
ENV PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin

# Install ssh and JDK11
RUN apt-get update && apt-get install -y openssh-server ca-certificates-java openjdk-11-jdk
ENV JAVA_HOME="/usr/lib/jvm/java-11-openjdk-amd64"

RUN ssh-keygen -t rsa -f /root/.ssh/id_rsa -q -P ""
RUN cat /root/.ssh/id_rsa.pub >> /root/.ssh/authorized_keys
RUN chmod 0600 /root/.ssh/authorized_keys
RUN echo "PermitRootLogin yes" >> /etc/ssh/sshd_config && \
    echo "PubkeyAuthentication yes" >> /etc/ssh/sshd_config && \
    echo "StrictHostKeyChecking no" >> /etc/ssh/ssh_config

# Copy the docker-entrypoint.sh script into the Docker image
COPY ./docker-entrypoint.sh /

# Set the entrypoint script to run when the container starts
ENTRYPOINT ["/docker-entrypoint.sh"]

# Expose the necessary ports
EXPOSE 50070 8080 7078 22 9000 8020
### DONE
The docker-entrypoint.sh will later be used when running the Singularity container through Slurm.
#!/bin/bash
set -e

case "$1" in
    sh|bash)
        set -- "$@"
        exec "$@"
    ;;
    sparkMaster)
        shift
        echo "Running Spark Master on `hostname` ${SLURM_PROCID}"
        /opt/spark/sbin/start-master.sh "$@" 1>$HOME/sparkMaster.out 2>&1 &
        status=$?
        if [ $status -ne 0 ]; then
            echo "Failed to start Spark Master: $status"
            exit $status
        fi

        exec tail -f $(ls -Art $HOME/sparkMaster.out | tail -n 1)
    ;;
    sparkWorker)
        shift
        /opt/spark/sbin/start-worker.sh "$@" 1>$HOME/sworker-${SLURM_PROCID}.out 2>&1 &
        status=$?
        if [ $status -ne 0 ]; then
            echo "Failed to start Spark worker: $status"
            exit $status
        fi

        exec tail -f $(ls -Art $HOME/sworker-${SLURM_PROCID}.out | tail -n 1)
    ;;
    sparkHDFSNamenode)
        shift
        echo "Running HDFS Namenode on `hostname` ${SLURM_PROCID}"
        /opt/hadoop/bin/hdfs namenode -format
        echo "Done format"
        /opt/hadoop/bin/hdfs --daemon start namenode "$@" 1>$HOME/hdfsNamenode.out 2>&1 &

        status=$?
        if [ $status -ne 0 ]; then
            echo "Failed to start HDFS Namenode: $status"
            exit $status
        fi

        exec tail -f $(ls -Art $HOME/hdfsNamenode.out | tail -n 1)
    ;;
    sparkHDFSDatanode)
        shift
        echo "Running HDFS datanode on `hostname` ${SLURM_PROCID}"
        /opt/hadoop/bin/hdfs --daemon start datanode "$@" 1>$HOME/hdfsDatanode-${SLURM_PROCID}.out 2>&1 &

        status=$?
        if [ $status -ne 0 ]; then
            echo "Failed to start HDFS datanode: $status"
            exit $status
        fi

        exec tail -f $(ls -Art $HOME/hdfsDatanode-${SLURM_PROCID}.out | tail -n 1)
    ;;
esac

Step 3: Create the singularity container

Either directly create the sparkhdfs.sif Singularity container, or use a sandbox to eventually modify/add before exporting to sif format. The sandbox is useful to further customize the Singularity container (e.g., modifying its docker-entry-point.sh).
#directly create a singularity container
sudo singularity build sparkhdfs.sif docker-daemon://sparkhdfs:latest
#create the sandbox directory from existing docker sparkhdfs container, then create the sparkhdfs.sif
sudo singularity build --sandbox sparkhdfs docker-daemon://sparkhdfs:latest
sudo singularity build sparkhdfs.sif sparkhdfs/
Step 4: Create a script to deploy Spark and HDFS

The following script runSparkHDFS.sh runs singularity sparkhdfs.sif container for deploying the Spark standalone cluster (one Master and two workers) and the Hadoop HDFS Namenode and Datanodes.
You should further customize this script (e.g., time, resources etc.)
This script assumes that under your $HOME directory there are two subdirectories installed for spark and hadoop configuration files.
#!/bin/bash -l
#SBATCH -J SparkHDFS
#SBATCH -N 3 # Nodes
#SBATCH -n 3 # Tasks
#SBATCH --ntasks-per-node=1
#SBATCH --mem=16GB
#SBATCH -c 16 # Cores assigned to each task
#SBATCH --time=0-00:59:00
#SBATCH -p batch
#SBATCH --qos=normal
#SBATCH --mail-user=first.lastname@uni.lu
#SBATCH --mail-type=BEGIN,END

module load tools/Singularity

hostName="`hostname`"
echo "hostname=$hostName"

#save it for future job refs
myhostname="`hostname`"
rm coordinatorNode
touch  coordinatorNode
cat > coordinatorNode << EOF
$myhostname
EOF

#create Spark configs
SPARK_CONF=${HOME}/spark/conf/spark-defaults.conf
cat > ${SPARK_CONF} << EOF

# Master settings
spark.master spark://$hostName:7078

# Memory settings
spark.driver.memory 2g
spark.executor.memory 12g

# Cores settings
spark.executor.cores 8
spark.cores.max 16

# Network settings
spark.driver.host $hostName

# Other settings
spark.logConf true

EOF

SPARK_ENVSH=${HOME}/spark/conf/spark-env.sh
cat > ${SPARK_ENVSH} << EOF
#!/usr/bin/env bash

SPARK_MASTER_HOST="$hostName"
SPARK_MASTER_PORT="7078"
SPARK_HOME="/opt/spark"
HADOOP_HOME="/opt/hadoop"

EOF

SPARK_L4J=${HOME}/spark/conf/log4j.properties
cat > ${SPARK_L4J} << EOF
# Set everything to be logged to the console
log4j.rootCategory=DEBUG, console
log4j.appender.console=org.apache.log4j.ConsoleAppender
log4j.appender.console.target=System.err
log4j.appender.console.layout=org.apache.log4j.PatternLayout
log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n

# Settings to quiet third party logs that are too verbose
log4j.logger.org.spark_project.jetty=ERROR
log4j.logger.org.spark_project.jetty.util.component.AbstractLifeCycle=ERROR
log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO
log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO
EOF

### create HDFS config
HDFS_SITE=${HOME}/hadoop/etc/hadoop/hdfs-site.xml
cat > ${HDFS_SITE} << EOF
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
  <property>
    <name>dfs.replication</name>
    <value>1</value>
  </property>

  <property>
    <name>dfs.namenode.name.dir</name>
    <value>/tmp/hadoop/hdfs/name</value>
  </property>

  <property>
   <name>dfs.datanode.data.dir</name>
   <value>/tmp/hadoop/hdfs/data</value>
  </property>

</configuration>

EOF

HDFS_CORESITE=${HOME}/hadoop/etc/hadoop/core-site.xml
cat > ${HDFS_CORESITE} << EOF
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
  <property>
  <name>fs.defaultFS</name>
  <value>hdfs://$hostName:9000</value>
  </property>

</configuration>

EOF

###

# Create a launcher script for SparkMaster and hdfsNamenode
#Once started, the Spark master will print out a spark://HOST:PORT to be used for submitting jobs

SPARKM_LAUNCHER=${HOME}/spark-start-master-${SLURM_JOBID}.sh
echo " - create SparkMaster and hdfsNamenode launcher script '${SPARKM_LAUNCHER}'"
cat << 'EOF' > ${SPARKM_LAUNCHER}
#!/bin/bash

echo "I am ${SLURM_PROCID} running on:"
hostname

#we are going to share an instance for Spark master and HDFS namenode
singularity instance start --bind $HOME/hadoop/logs:/opt/hadoop/logs,$HOME/hadoop/etc/hadoop:/opt/hadoop/etc/hadoop,$HOME/spark/conf:/opt/spark/conf,$HOME/spark/logs:/opt/spark/logs,$HOME/spark/work:/opt/spark/work \
 sparkhdfs.sif shinst

singularity run --bind $HOME/hadoop/logs:/opt/hadoop/logs,$HOME/hadoop/etc/hadoop:/opt/hadoop/etc/hadoop instance://shinst \
  sparkHDFSNamenode 2>&1 &

singularity run --bind $HOME/spark/conf:/opt/spark/conf,$HOME/spark/logs:/opt/spark/logs,$HOME/spark/work:/opt/spark/work instance://shinst \
  sparkMaster


#the following example works for running without instance only the Spark Master
#singularity run --bind $HOME/spark/conf:/opt/spark/conf,$HOME/spark/logs:/opt/spark/logs,$HOME/spark/work:/opt/spark/work sparkhdfs.sif \
# sparkMaster

EOF
chmod +x ${SPARKM_LAUNCHER}

srun --exclusive -N 1 -n 1 -c 16 --ntasks-per-node=1 -l -o $HOME/SparkMaster-`hostname`.out \
 ${SPARKM_LAUNCHER} &

export SPARKMASTER="spark://$hostName:7078"

echo "Starting Spark workers and HDFS datanodes"

SPARK_LAUNCHER=${HOME}/spark-start-workers-${SLURM_JOBID}.sh
echo " - create Spark workers and HDFS datanodes launcher script '${SPARK_LAUNCHER}'"
cat << 'EOF' > ${SPARK_LAUNCHER}
#!/bin/bash

echo "I am ${SLURM_PROCID} running on:"
hostname

#we are going to share an instance for Spark workers and HDFS datanodes
singularity instance start --bind $HOME/hadoop/logs:/opt/hadoop/logs,$HOME/hadoop/etc/hadoop:/opt/hadoop/etc/hadoop,$HOME/spark/conf:/opt/spark/conf,$HOME/spark/logs:/opt/spark/logs,$HOME/spark/work:/opt/spark/work \
 sparkhdfs.sif shinst

singularity run --bind $HOME/hadoop/logs:/opt/hadoop/logs,$HOME/hadoop/etc/hadoop:/opt/hadoop/etc/hadoop instance://shinst \
  sparkHDFSDatanode 2>&1 &

singularity run --bind $HOME/spark/conf:/opt/spark/conf,$HOME/spark/logs:/opt/spark/logs,$HOME/spark/work:/opt/spark/work instance://shinst \
  sparkWorker $SPARKMASTER -c 8 -m 12G


#the following without instance only Spark worker
#singularity run --bind $HOME/spark/conf:/opt/spark/conf,$HOME/spark/logs:/opt/spark/logs,$HOME/spark/work:/opt/spark/work sparkhdfs.sif \
# sparkWorker $SPARKMASTER -c 8 -m 8G 

EOF
chmod +x ${SPARK_LAUNCHER}

srun --exclusive -N 2 -n 2 -c 16 --ntasks-per-node=1 -l -o $HOME/SparkWorkers-`hostname`.out \
 ${SPARK_LAUNCHER} &

pid=$!
sleep 3600s
wait $pid

echo $HOME

echo "Ready Stopping SparkHDFS instances"

Now you can deploy Spark and HDFS with one command. Before that, under your $HOME directory we have to install Spark and Hadoop config directories.
#Login to Iris/Aion
ssh aion-cluster
#Make sure your $HOME directory contains the required scripts and configuration files
# You have cloned the tutorials on your laptop
# From bigdata directory e.g. /Users/ocm/bdhpc/tutorials/bigdata
#Replace omarcu with your username and from your laptop rsync as follows:
bigdata % rsync --rsh='ssh -p 8022' -avzu scripts/sparkhdfs/ aion-cluster:/home/users/omarcu/
Your home directory looks as following:
#Login to Iris/Aion
ssh aion-cluster
#Make sure your $HOME directory contains the required scripts and configuration files
 create mode 100644 /home/users/omarcu/Dockerfile
 create mode 100755 /home/users/omarcu/clean.sh
 create mode 100755 /home/users/omarcu/docker-entrypoint.sh
 create mode 100644 /home/users/omarcu/hadoop/etc/hadoop/capacity-scheduler.xml
 create mode 100644 /home/users/omarcu/hadoop/etc/hadoop/configuration.xsl
 create mode 100644 /home/users/omarcu/hadoop/etc/hadoop/container-executor.cfg
 create mode 100644 /home/users/omarcu/hadoop/etc/hadoop/hadoop-env.cmd
 create mode 100644 /home/users/omarcu/hadoop/etc/hadoop/hadoop-env.sh
 create mode 100644 /home/users/omarcu/hadoop/etc/hadoop/hadoop-metrics2.properties
 create mode 100644 /home/users/omarcu/hadoop/etc/hadoop/hadoop-policy.xml
 create mode 100644 /home/users/omarcu/hadoop/etc/hadoop/hadoop-user-functions.sh.example
 create mode 100644 /home/users/omarcu/hadoop/etc/hadoop/hdfs-rbf-site.xml
 create mode 100644 /home/users/omarcu/hadoop/etc/hadoop/httpfs-env.sh
 create mode 100644 /home/users/omarcu/hadoop/etc/hadoop/httpfs-log4j.properties
 create mode 100644 /home/users/omarcu/hadoop/etc/hadoop/httpfs-site.xml
 create mode 100644 /home/users/omarcu/hadoop/etc/hadoop/kms-acls.xml
 create mode 100644 /home/users/omarcu/hadoop/etc/hadoop/kms-env.sh
 create mode 100644 /home/users/omarcu/hadoop/etc/hadoop/kms-log4j.properties
 create mode 100644 /home/users/omarcu/hadoop/etc/hadoop/kms-site.xml
 create mode 100644 /home/users/omarcu/hadoop/etc/hadoop/log4j.properties
 create mode 100644 /home/users/omarcu/hadoop/etc/hadoop/mapred-env.cmd
 create mode 100644 /home/users/omarcu/hadoop/etc/hadoop/mapred-env.sh
 create mode 100644 /home/users/omarcu/hadoop/etc/hadoop/mapred-queues.xml.template
 create mode 100644 /home/users/omarcu/hadoop/etc/hadoop/mapred-site.xml
 create mode 100644 /home/users/omarcu/hadoop/etc/hadoop/shellprofile.d/example.sh
 create mode 100644 /home/users/omarcu/hadoop/etc/hadoop/ssl-client.xml.example
 create mode 100644 /home/users/omarcu/hadoop/etc/hadoop/ssl-server.xml.example
 create mode 100644 /home/users/omarcu/hadoop/etc/hadoop/user_ec_policies.xml.template
 create mode 100644 /home/users/omarcu/hadoop/etc/hadoop/workers
 create mode 100644 /home/users/omarcu/hadoop/etc/hadoop/yarn-env.cmd
 create mode 100644 /home/users/omarcu/hadoop/etc/hadoop/yarn-env.sh
 create mode 100644 /home/users/omarcu/hadoop/etc/hadoop/yarn-site.xml
 create mode 100644 /home/users/omarcu/hadoop/etc/hadoop/yarnservice-log4j.properties
 create mode 100755 /home/users/omarcu/runSparkHDFS.sh
 create mode 100644 /home/users/omarcu/spark/spark-terasort-1.2-SNAPSHOT-jar-with-dependencies.jar
Step 5: How the output looks

0 [omarcu@access1 ~]$ sbatch runSparkHDFS.sh
Submitted batch job 771126
0 [omarcu@access1 ~]$ sq
# squeue -u omarcu
   JOBID PARTIT       QOS                 NAME       USER NODE  CPUS ST         TIME    TIME_LEFT PRIORITY NODELIST(REASON)
  771126  batch    normal            SparkHDFS     omarcu    3    48  R         0:02        58:58    10398 aion-[0129,0131-0132]

0 [omarcu@access1 ~]$ ls
SparkWorkers-aion-0129.out  
coordinatorNode  
hdfsDatanode-0.out  
hdfsNamenode.out  
runSparkMaster.sh  
slurm-771126.out  
spark-start-master-771126.sh   
sparkMaster.out  
sworker-0.out
SparkMaster-aion-0129.out  
clean.sh                    
hadoop           
hdfsDatanode-1.out  
runSparkHDFS.sh    
spark             
spark-start-workers-771126.sh  
sparkhdfs.sif    
sworker-1.out

[omarcu@access1 ~]$ cat slurm-771126.out 
hostname=aion-0129
 - create SparkMaster and hdfsNamenode launcher script '/home/users/omarcu/spark-start-master-771126.sh'
Starting Spark workers and HDFS datanodes
 - create Spark workers and HDFS datanodes launcher script '/home/users/omarcu/spark-start-workers-771126.sh'

0 [omarcu@access1 ~]$ cat spark-start-master-771126.sh
#!/bin/bash

echo "I am ${SLURM_PROCID} running on:"
hostname

#we are going to share an instance for Spark master and HDFS namenode
singularity instance start --bind $HOME/hadoop/logs:/opt/hadoop/logs,$HOME/hadoop/etc/hadoop:/opt/hadoop/etc/hadoop,$HOME/spark/conf:/opt/spark/conf,$HOME/spark/logs:/opt/spark/logs,$HOME/spark/work:/opt/spark/work \
 sparkhdfs.sif shinst

singularity run --bind $HOME/hadoop/logs:/opt/hadoop/logs,$HOME/hadoop/etc/hadoop:/opt/hadoop/etc/hadoop instance://shinst \
  sparkHDFSNamenode 2>&1 &

singularity run --bind $HOME/spark/conf:/opt/spark/conf,$HOME/spark/logs:/opt/spark/logs,$HOME/spark/work:/opt/spark/work instance://shinst \
  sparkMaster

0 [omarcu@access1 ~]$ cat spark-start-workers-771126.sh
#!/bin/bash

echo "I am ${SLURM_PROCID} running on:"
hostname

#we are going to share an instance for Spark workers and HDFS datanodes
singularity instance start --bind $HOME/hadoop/logs:/opt/hadoop/logs,$HOME/hadoop/etc/hadoop:/opt/hadoop/etc/hadoop,$HOME/spark/conf:/opt/spark/conf,$HOME/spark/logs:/opt/spark/logs,$HOME/spark/work:/opt/spark/work \
 sparkhdfs.sif shinst

singularity run --bind $HOME/hadoop/logs:/opt/hadoop/logs,$HOME/hadoop/etc/hadoop:/opt/hadoop/etc/hadoop instance://shinst \
  sparkHDFSDatanode 2>&1 &

singularity run --bind $HOME/spark/conf:/opt/spark/conf,$HOME/spark/logs:/opt/spark/logs,$HOME/spark/work:/opt/spark/work instance://shinst \
  sparkWorker $SPARKMASTER -c 8 -m 12G

0 [omarcu@access1 ~]$ less spark/logs/spark-omarcu-org.apache.spark.deploy.master.Master-1-aion-0129.out
Spark Command: /usr/lib/jvm/java-11-openjdk-amd64/bin/java -cp /opt/spark/conf/:/opt/spark/jars/* -Xmx1g org.apache.spark.deploy.master.Master --host aion-0129 --port 7078 --webui-port 8080
========================================
Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
23/05/31 12:01:44 INFO Master: Started daemon with process name: 74@aion-0129
23/05/31 12:01:44 INFO SignalUtils: Registering signal handler for TERM
23/05/31 12:01:44 INFO SignalUtils: Registering signal handler for HUP
23/05/31 12:01:44 INFO SignalUtils: Registering signal handler for INT
23/05/31 12:01:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/05/31 12:01:45 INFO SecurityManager: Changing view acls to: omarcu
23/05/31 12:01:45 INFO SecurityManager: Changing modify acls to: omarcu
23/05/31 12:01:45 INFO SecurityManager: Changing view acls groups to: 
23/05/31 12:01:45 INFO SecurityManager: Changing modify acls groups to: 
23/05/31 12:01:45 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: omarcu; groups with view permissions: EMPTY; users with modify permissions: omarcu; groups with modify permissions: EMPTY
23/05/31 12:01:45 INFO Utils: Successfully started service 'sparkMaster' on port 7078.
23/05/31 12:01:45 INFO Master: Starting Spark master at spark://aion-0129:7078
23/05/31 12:01:45 INFO Master: Running Spark version 3.4.0
23/05/31 12:01:45 INFO JettyUtils: Start Jetty 0.0.0.0:8080 for MasterUI
23/05/31 12:01:45 INFO Utils: Successfully started service 'MasterUI' on port 8080.
23/05/31 12:01:45 INFO MasterWebUI: Bound MasterWebUI to 0.0.0.0, and started at http://aion-0129:8080
23/05/31 12:01:45 INFO Master: I have been elected leader! New state: ALIVE
23/05/31 12:01:46 INFO Master: Registering worker 172.21.12.48:34843 with 8 cores, 12.0 GiB RAM
23/05/31 12:01:51 INFO Master: Registering worker 172.21.12.47:34815 with 8 cores, 12.0 GiB RAM

Spark is running at http://aion-0129:8080 with two workers.
Running HDFS Namenode on aion-0129/172.21.12.45:9000 with two datanodes.
Step 6: Running manually the Terasort application

We are going to use aion-0129 singularity shared instance to add a file to Hadoop HDFS and run our Spark application.
Take a shell to the singularity instance.
0 [omarcu@aion-0129 ~](771126 N/T/CN)$ module load tools/Singularity
0 [omarcu@aion-0129 ~](771126 N/T/CN)$ singularity instance list
INSTANCE NAME    PID        IP    IMAGE
shinst           1406958          /home/users/omarcu/sparkhdfs.sif
0 [omarcu@aion-0129 ~](771126 N/T/CN)$ singularity shell instance://shinst
Singularity> bash
Run Spark shell and quit (testing).
omarcu@aion-0129:~$ /opt/spark/bin/spark-shell --master spark://aion-0129:7078
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
23/05/31 12:15:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Spark context Web UI available at http://aion-0129:4040
Spark context available as 'sc' (master = spark://aion-0129:7078, app id = app-20230531121527-0000).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.4.0
      /_/

Using Scala version 2.12.17 (OpenJDK 64-Bit Server VM, Java 11.0.19)
Type in expressions to have them evaluated.
Type :help for more information.

scala> :q

Next, we submit our jobs - see details about our application at https://github.com/ehiggs/spark-terasort.
omarcu@aion-0129:~$ cd /opt/spark/
omarcu@aion-0129:/opt/spark$ bin/spark-submit --master spark://aion-0129:7078 --class com.github.ehiggs.spark.terasort.TeraGen /home/users/omarcu/spark/spark-terasort-1.2-SNAPSHOT-jar-with-dependencies.jar 10g hdfs://aion-0129:9000/terasort_in
23/05/31 12:18:45 INFO SparkContext: Running Spark version 3.4.0
23/05/31 12:18:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/05/31 12:18:45 INFO ResourceUtils: ==============================================================
23/05/31 12:18:45 INFO ResourceUtils: No custom resources configured for spark.driver.
23/05/31 12:18:45 INFO ResourceUtils: ==============================================================
23/05/31 12:18:45 INFO SparkContext: Submitted application: TeraGen (10GB)
23/05/31 12:18:45 INFO SparkContext: Spark configuration:
spark.app.name=TeraGen (10GB)
spark.app.startTime=1685535525515
spark.app.submitTime=1685535525437
spark.cores.max=16
spark.driver.extraJavaOptions=-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false
spark.driver.host=aion-0129
spark.driver.memory=2g
spark.executor.cores=8
spark.executor.extraJavaOptions=-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false
spark.executor.memory=12g
spark.jars=file:/home/users/omarcu/spark/spark-terasort-1.2-SNAPSHOT-jar-with-dependencies.jar
spark.logConf=true
spark.master=spark://aion-0129:7078
spark.submit.deployMode=client
spark.submit.pyFiles=
23/05/31 12:18:45 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 8, script: , vendor: , memory -> name: memory, amount: 12288, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/05/31 12:18:45 INFO ResourceProfile: Limiting resource is cpus at 8 tasks per executor
23/05/31 12:18:45 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/05/31 12:18:45 INFO SecurityManager: Changing view acls to: omarcu
23/05/31 12:18:45 INFO SecurityManager: Changing modify acls to: omarcu
23/05/31 12:18:45 INFO SecurityManager: Changing view acls groups to: 
23/05/31 12:18:45 INFO SecurityManager: Changing modify acls groups to: 
23/05/31 12:18:45 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: omarcu; groups with view permissions: EMPTY; users with modify permissions: omarcu; groups with modify permissions: EMPTY
23/05/31 12:18:45 INFO Utils: Successfully started service 'sparkDriver' on port 43365.
23/05/31 12:18:45 INFO SparkEnv: Registering MapOutputTracker
23/05/31 12:18:46 INFO SparkEnv: Registering BlockManagerMaster
23/05/31 12:18:46 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/05/31 12:18:46 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/05/31 12:18:46 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/05/31 12:18:46 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f8ba2d41-2a8e-4119-8341-9cd78a1dba15
23/05/31 12:18:46 INFO MemoryStore: MemoryStore started with capacity 1048.8 MiB
23/05/31 12:18:46 INFO SparkEnv: Registering OutputCommitCoordinator
23/05/31 12:18:46 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
23/05/31 12:18:46 INFO Utils: Successfully started service 'SparkUI' on port 4040.
23/05/31 12:18:46 INFO SparkContext: Added JAR file:/home/users/omarcu/spark/spark-terasort-1.2-SNAPSHOT-jar-with-dependencies.jar at spark://aion-0129:43365/jars/spark-terasort-1.2-SNAPSHOT-jar-with-dependencies.jar with timestamp 1685535525515
23/05/31 12:18:46 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://aion-0129:7078...
23/05/31 12:18:46 INFO TransportClientFactory: Successfully created connection to aion-0129/172.21.12.45:7078 after 26 ms (0 ms spent in bootstraps)
23/05/31 12:18:46 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20230531121846-0001
23/05/31 12:18:46 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20230531121846-0001/0 on worker-20230531120144-172.21.12.47-34815 (172.21.12.47:34815) with 8 core(s)
23/05/31 12:18:46 INFO StandaloneSchedulerBackend: Granted executor ID app-20230531121846-0001/0 on hostPort 172.21.12.47:34815 with 8 core(s), 12.0 GiB RAM
23/05/31 12:18:46 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20230531121846-0001/1 on worker-20230531120146-172.21.12.48-34843 (172.21.12.48:34843) with 8 core(s)
23/05/31 12:18:46 INFO StandaloneSchedulerBackend: Granted executor ID app-20230531121846-0001/1 on hostPort 172.21.12.48:34843 with 8 core(s), 12.0 GiB RAM
23/05/31 12:18:46 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44799.
23/05/31 12:18:46 INFO NettyBlockTransferService: Server created on aion-0129:44799
23/05/31 12:18:46 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/05/31 12:18:46 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, aion-0129, 44799, None)
23/05/31 12:18:46 INFO BlockManagerMasterEndpoint: Registering block manager aion-0129:44799 with 1048.8 MiB RAM, BlockManagerId(driver, aion-0129, 44799, None)
23/05/31 12:18:46 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, aion-0129, 44799, None)
23/05/31 12:18:46 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, aion-0129, 44799, None)
23/05/31 12:18:46 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20230531121846-0001/0 is now RUNNING
23/05/31 12:18:46 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20230531121846-0001/1 is now RUNNING
23/05/31 12:18:46 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
===========================================================================
===========================================================================
Input size: 10GB
Total number of records: 100000000
Number of output partitions: 2
Number of records/output partition: 50000000
===========================================================================
===========================================================================
23/05/31 12:18:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/05/31 12:18:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/05/31 12:18:47 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:83
23/05/31 12:18:47 INFO DAGScheduler: Got job 0 (runJob at SparkHadoopWriter.scala:83) with 2 output partitions
23/05/31 12:18:47 INFO DAGScheduler: Final stage: ResultStage 0 (runJob at SparkHadoopWriter.scala:83)
23/05/31 12:18:47 INFO DAGScheduler: Parents of final stage: List()
23/05/31 12:18:47 INFO DAGScheduler: Missing parents: List()
23/05/31 12:18:47 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at mapPartitionsWithIndex at TeraGen.scala:66), which has no missing parents
23/05/31 12:18:47 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 101.3 KiB, free 1048.7 MiB)
23/05/31 12:18:47 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 36.2 KiB, free 1048.7 MiB)
23/05/31 12:18:47 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on aion-0129:44799 (size: 36.2 KiB, free: 1048.8 MiB)
23/05/31 12:18:47 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1535
23/05/31 12:18:47 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at mapPartitionsWithIndex at TeraGen.scala:66) (first 15 tasks are for partitions Vector(0, 1))
23/05/31 12:18:47 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks resource profile 0
23/05/31 12:18:48 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.21.12.47:34826) with ID 0,  ResourceProfileId 0
23/05/31 12:18:48 INFO BlockManagerMasterEndpoint: Registering block manager 172.21.12.47:40937 with 7.0 GiB RAM, BlockManagerId(0, 172.21.12.47, 40937, None)
23/05/31 12:18:48 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.21.12.47, executor 0, partition 0, PROCESS_LOCAL, 7492 bytes) 
23/05/31 12:18:48 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (172.21.12.47, executor 0, partition 1, PROCESS_LOCAL, 7492 bytes) 
23/05/31 12:18:48 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.21.12.48:57728) with ID 1,  ResourceProfileId 0
23/05/31 12:18:48 INFO BlockManagerMasterEndpoint: Registering block manager 172.21.12.48:36261 with 7.0 GiB RAM, BlockManagerId(1, 172.21.12.48, 36261, None)
23/05/31 12:18:48 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.21.12.47:40937 (size: 36.2 KiB, free: 7.0 GiB)
23/05/31 12:20:15 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 86967 ms on 172.21.12.47 (executor 0) (1/2)
23/05/31 12:20:15 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 87277 ms on 172.21.12.47 (executor 0) (2/2)
23/05/31 12:20:15 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
23/05/31 12:20:15 INFO DAGScheduler: ResultStage 0 (runJob at SparkHadoopWriter.scala:83) finished in 88.278 s
23/05/31 12:20:15 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
23/05/31 12:20:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
23/05/31 12:20:15 INFO DAGScheduler: Job 0 finished: runJob at SparkHadoopWriter.scala:83, took 88.340332 s
23/05/31 12:20:15 INFO SparkHadoopWriter: Start to commit write Job job_202305311218463169431769019098759_0001.
23/05/31 12:20:15 INFO SparkHadoopWriter: Write Job job_202305311218463169431769019098759_0001 committed. Elapsed time: 83 ms.
23/05/31 12:20:15 INFO SparkContext: Starting job: count at TeraGen.scala:94
23/05/31 12:20:15 INFO DAGScheduler: Got job 1 (count at TeraGen.scala:94) with 2 output partitions
23/05/31 12:20:15 INFO DAGScheduler: Final stage: ResultStage 1 (count at TeraGen.scala:94)
23/05/31 12:20:15 INFO DAGScheduler: Parents of final stage: List()
23/05/31 12:20:15 INFO DAGScheduler: Missing parents: List()
23/05/31 12:20:15 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[1] at mapPartitionsWithIndex at TeraGen.scala:66), which has no missing parents
23/05/31 12:20:15 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 4.0 KiB, free 1048.7 MiB)
23/05/31 12:20:15 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.3 KiB, free 1048.7 MiB)
23/05/31 12:20:15 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on aion-0129:44799 (size: 2.3 KiB, free: 1048.8 MiB)
23/05/31 12:20:15 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535
23/05/31 12:20:15 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[1] at mapPartitionsWithIndex at TeraGen.scala:66) (first 15 tasks are for partitions Vector(0, 1))
23/05/31 12:20:15 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks resource profile 0
23/05/31 12:20:15 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2) (172.21.12.48, executor 1, partition 0, PROCESS_LOCAL, 7492 bytes) 
23/05/31 12:20:15 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3) (172.21.12.47, executor 0, partition 1, PROCESS_LOCAL, 7492 bytes) 
23/05/31 12:20:15 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.21.12.47:40937 (size: 2.3 KiB, free: 7.0 GiB)
23/05/31 12:20:16 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.21.12.48:36261 (size: 2.3 KiB, free: 7.0 GiB)
23/05/31 12:20:30 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 14657 ms on 172.21.12.48 (executor 1) (1/2)
23/05/31 12:20:30 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 14962 ms on 172.21.12.47 (executor 0) (2/2)
23/05/31 12:20:30 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
23/05/31 12:20:30 INFO DAGScheduler: ResultStage 1 (count at TeraGen.scala:94) finished in 14.972 s
23/05/31 12:20:30 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
23/05/31 12:20:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
23/05/31 12:20:30 INFO DAGScheduler: Job 1 finished: count at TeraGen.scala:94, took 14.977215 s
Number of records written: 100000000
23/05/31 12:20:30 INFO SparkContext: Invoking stop() from shutdown hook
23/05/31 12:20:30 INFO SparkContext: SparkContext is stopping with exitCode 0.
23/05/31 12:20:30 INFO SparkUI: Stopped Spark web UI at http://aion-0129:4040
23/05/31 12:20:30 INFO StandaloneSchedulerBackend: Shutting down all executors
23/05/31 12:20:30 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down
23/05/31 12:20:30 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/05/31 12:20:30 INFO MemoryStore: MemoryStore cleared
23/05/31 12:20:30 INFO BlockManager: BlockManager stopped
23/05/31 12:20:30 INFO BlockManagerMaster: BlockManagerMaster stopped
23/05/31 12:20:30 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/05/31 12:20:31 INFO SparkContext: Successfully stopped SparkContext
23/05/31 12:20:31 INFO ShutdownHookManager: Shutdown hook called
23/05/31 12:20:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-eca95b40-741a-4b10-860f-3a16cd2d850a
23/05/31 12:20:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-fdbb3552-28c4-4abd-bc9c-eabf3a1435d3
omarcu@aion-0129:/opt/spark$ bin/spark-submit --master spark://aion-0129:7078 --class com.github.ehiggs.spark.terasort.TeraSort /home/users/omarcu/spark/spark-terasort-1.2-SNAPSHOT-jar-with-dependencies.jar hdfs://aion-0129:9000/terasort_in hdfs://aion-0129:9000/terasort_out
23/05/31 12:22:06 INFO SparkContext: Running Spark version 3.4.0
23/05/31 12:22:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/05/31 12:22:06 INFO ResourceUtils: ==============================================================
23/05/31 12:22:06 INFO ResourceUtils: No custom resources configured for spark.driver.
23/05/31 12:22:06 INFO ResourceUtils: ==============================================================
23/05/31 12:22:06 INFO SparkContext: Submitted application: TeraSort
23/05/31 12:22:06 INFO SparkContext: Spark configuration:
spark.app.name=TeraSort
spark.app.startTime=1685535726750
spark.app.submitTime=1685535726689
spark.cores.max=16
spark.driver.extraJavaOptions=-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false
spark.driver.host=aion-0129
spark.driver.memory=2g
spark.executor.cores=8
spark.executor.extraJavaOptions=-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false
spark.executor.memory=12g
spark.jars=file:/home/users/omarcu/spark/spark-terasort-1.2-SNAPSHOT-jar-with-dependencies.jar
spark.logConf=true
spark.master=spark://aion-0129:7078
spark.serializer=org.apache.spark.serializer.KryoSerializer
spark.submit.deployMode=client
spark.submit.pyFiles=
23/05/31 12:22:06 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 8, script: , vendor: , memory -> name: memory, amount: 12288, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/05/31 12:22:06 INFO ResourceProfile: Limiting resource is cpus at 8 tasks per executor
23/05/31 12:22:06 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/05/31 12:22:07 INFO SecurityManager: Changing view acls to: omarcu
23/05/31 12:22:07 INFO SecurityManager: Changing modify acls to: omarcu
23/05/31 12:22:07 INFO SecurityManager: Changing view acls groups to: 
23/05/31 12:22:07 INFO SecurityManager: Changing modify acls groups to: 
23/05/31 12:22:07 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: omarcu; groups with view permissions: EMPTY; users with modify permissions: omarcu; groups with modify permissions: EMPTY
23/05/31 12:22:07 INFO Utils: Successfully started service 'sparkDriver' on port 42767.
23/05/31 12:22:07 INFO SparkEnv: Registering MapOutputTracker
23/05/31 12:22:07 INFO SparkEnv: Registering BlockManagerMaster
23/05/31 12:22:07 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/05/31 12:22:07 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/05/31 12:22:07 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/05/31 12:22:07 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-3be7f28e-d8a7-461a-9075-8dd0fd172ec8
23/05/31 12:22:07 INFO MemoryStore: MemoryStore started with capacity 1048.8 MiB
23/05/31 12:22:07 INFO SparkEnv: Registering OutputCommitCoordinator
23/05/31 12:22:07 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
23/05/31 12:22:07 INFO Utils: Successfully started service 'SparkUI' on port 4040.
23/05/31 12:22:07 INFO SparkContext: Added JAR file:/home/users/omarcu/spark/spark-terasort-1.2-SNAPSHOT-jar-with-dependencies.jar at spark://aion-0129:42767/jars/spark-terasort-1.2-SNAPSHOT-jar-with-dependencies.jar with timestamp 1685535726750
23/05/31 12:22:07 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://aion-0129:7078...
23/05/31 12:22:07 INFO TransportClientFactory: Successfully created connection to aion-0129/172.21.12.45:7078 after 30 ms (0 ms spent in bootstraps)
23/05/31 12:22:08 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20230531122208-0003
23/05/31 12:22:08 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20230531122208-0003/0 on worker-20230531120144-172.21.12.47-34815 (172.21.12.47:34815) with 8 core(s)
23/05/31 12:22:08 INFO StandaloneSchedulerBackend: Granted executor ID app-20230531122208-0003/0 on hostPort 172.21.12.47:34815 with 8 core(s), 12.0 GiB RAM
23/05/31 12:22:08 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20230531122208-0003/1 on worker-20230531120146-172.21.12.48-34843 (172.21.12.48:34843) with 8 core(s)
23/05/31 12:22:08 INFO StandaloneSchedulerBackend: Granted executor ID app-20230531122208-0003/1 on hostPort 172.21.12.48:34843 with 8 core(s), 12.0 GiB RAM
23/05/31 12:22:08 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35531.
23/05/31 12:22:08 INFO NettyBlockTransferService: Server created on aion-0129:35531
23/05/31 12:22:08 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/05/31 12:22:08 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, aion-0129, 35531, None)
23/05/31 12:22:08 INFO BlockManagerMasterEndpoint: Registering block manager aion-0129:35531 with 1048.8 MiB RAM, BlockManagerId(driver, aion-0129, 35531, None)
23/05/31 12:22:08 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20230531122208-0003/0 is now RUNNING
23/05/31 12:22:08 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20230531122208-0003/1 is now RUNNING
23/05/31 12:22:08 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, aion-0129, 35531, None)
23/05/31 12:22:08 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, aion-0129, 35531, None)
23/05/31 12:22:08 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
23/05/31 12:22:09 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 192.6 KiB, free 1048.6 MiB)
23/05/31 12:22:09 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.7 KiB, free 1048.6 MiB)
23/05/31 12:22:09 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on aion-0129:35531 (size: 32.7 KiB, free: 1048.8 MiB)
23/05/31 12:22:09 INFO SparkContext: Created broadcast 0 from newAPIHadoopFile at TeraSort.scala:60
23/05/31 12:22:09 INFO FileInputFormat: Total input files to process : 2
23/05/31 12:22:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/05/31 12:22:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/05/31 12:22:09 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:83
23/05/31 12:22:09 INFO DAGScheduler: Registering RDD 0 (newAPIHadoopFile at TeraSort.scala:60) as input to shuffle 0
23/05/31 12:22:09 INFO DAGScheduler: Got job 0 (runJob at SparkHadoopWriter.scala:83) with 76 output partitions
23/05/31 12:22:09 INFO DAGScheduler: Final stage: ResultStage 1 (runJob at SparkHadoopWriter.scala:83)
23/05/31 12:22:09 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
23/05/31 12:22:09 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
23/05/31 12:22:09 INFO DAGScheduler: Submitting ShuffleMapStage 0 (hdfs://aion-0129:9000/terasort_in NewHadoopRDD[0] at newAPIHadoopFile at TeraSort.scala:60), which has no missing parents
23/05/31 12:22:09 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 4.6 KiB, free 1048.6 MiB)
23/05/31 12:22:09 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.9 KiB, free 1048.6 MiB)
23/05/31 12:22:09 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on aion-0129:35531 (size: 2.9 KiB, free: 1048.8 MiB)
23/05/31 12:22:09 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535
23/05/31 12:22:09 INFO DAGScheduler: Submitting 76 missing tasks from ShuffleMapStage 0 (hdfs://aion-0129:9000/terasort_in NewHadoopRDD[0] at newAPIHadoopFile at TeraSort.scala:60) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
23/05/31 12:22:09 INFO TaskSchedulerImpl: Adding task set 0.0 with 76 tasks resource profile 0
23/05/31 12:22:09 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.21.12.47:39364) with ID 0,  ResourceProfileId 0
23/05/31 12:22:09 INFO BlockManagerMasterEndpoint: Registering block manager 172.21.12.47:36545 with 7.0 GiB RAM, BlockManagerId(0, 172.21.12.47, 36545, None)
23/05/31 12:22:10 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.21.12.47, executor 0, partition 0, ANY, 7460 bytes) 
23/05/31 12:22:10 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (172.21.12.47, executor 0, partition 1, ANY, 7460 bytes) 
23/05/31 12:22:10 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2) (172.21.12.47, executor 0, partition 2, ANY, 7460 bytes) 
23/05/31 12:22:10 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3) (172.21.12.47, executor 0, partition 3, ANY, 7460 bytes) 
23/05/31 12:22:10 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4) (172.21.12.47, executor 0, partition 4, ANY, 7460 bytes) 
23/05/31 12:22:10 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5) (172.21.12.47, executor 0, partition 5, ANY, 7460 bytes) 
23/05/31 12:22:10 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6) (172.21.12.47, executor 0, partition 6, ANY, 7460 bytes) 
23/05/31 12:22:10 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7) (172.21.12.47, executor 0, partition 7, ANY, 7460 bytes) 
23/05/31 12:22:10 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.21.12.48:59970) with ID 1,  ResourceProfileId 0
23/05/31 12:22:10 INFO BlockManagerMasterEndpoint: Registering block manager 172.21.12.48:38673 with 7.0 GiB RAM, BlockManagerId(1, 172.21.12.48, 38673, None)
23/05/31 12:22:10 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.21.12.47:36545 (size: 2.9 KiB, free: 7.0 GiB)
23/05/31 12:22:10 INFO TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8) (172.21.12.48, executor 1, partition 8, ANY, 7460 bytes) 
23/05/31 12:22:10 INFO TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9) (172.21.12.48, executor 1, partition 9, ANY, 7460 bytes) 
23/05/31 12:22:10 INFO TaskSetManager: Starting task 10.0 in stage 0.0 (TID 10) (172.21.12.48, executor 1, partition 10, ANY, 7460 bytes) 
23/05/31 12:22:10 INFO TaskSetManager: Starting task 11.0 in stage 0.0 (TID 11) (172.21.12.48, executor 1, partition 11, ANY, 7460 bytes) 
23/05/31 12:22:10 INFO TaskSetManager: Starting task 12.0 in stage 0.0 (TID 12) (172.21.12.48, executor 1, partition 12, ANY, 7460 bytes) 
23/05/31 12:22:10 INFO TaskSetManager: Starting task 13.0 in stage 0.0 (TID 13) (172.21.12.48, executor 1, partition 13, ANY, 7460 bytes) 
23/05/31 12:22:10 INFO TaskSetManager: Starting task 14.0 in stage 0.0 (TID 14) (172.21.12.48, executor 1, partition 14, ANY, 7460 bytes) 
23/05/31 12:22:10 INFO TaskSetManager: Starting task 15.0 in stage 0.0 (TID 15) (172.21.12.48, executor 1, partition 15, ANY, 7460 bytes) 
23/05/31 12:22:10 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.21.12.48:38673 (size: 2.9 KiB, free: 7.0 GiB)
23/05/31 12:22:10 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.21.12.47:36545 (size: 32.7 KiB, free: 7.0 GiB)
23/05/31 12:22:11 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.21.12.48:38673 (size: 32.7 KiB, free: 7.0 GiB)
23/05/31 12:22:12 INFO TaskSetManager: Starting task 16.0 in stage 0.0 (TID 16) (172.21.12.47, executor 0, partition 16, ANY, 7460 bytes) 
23/05/31 12:22:12 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 2761 ms on 172.21.12.47 (executor 0) (1/76)
23/05/31 12:22:12 INFO TaskSetManager: Starting task 17.0 in stage 0.0 (TID 17) (172.21.12.47, executor 0, partition 17, ANY, 7460 bytes) 
23/05/31 12:22:12 INFO TaskSetManager: Starting task 18.0 in stage 0.0 (TID 18) (172.21.12.47, executor 0, partition 18, ANY, 7460 bytes) 
23/05/31 12:22:12 INFO TaskSetManager: Starting task 19.0 in stage 0.0 (TID 19) (172.21.12.47, executor 0, partition 19, ANY, 7460 bytes) 
23/05/31 12:22:12 INFO TaskSetManager: Starting task 20.0 in stage 0.0 (TID 20) (172.21.12.47, executor 0, partition 20, ANY, 7460 bytes) 
23/05/31 12:22:12 INFO TaskSetManager: Starting task 21.0 in stage 0.0 (TID 21) (172.21.12.47, executor 0, partition 21, ANY, 7460 bytes) 
23/05/31 12:22:12 INFO TaskSetManager: Starting task 22.0 in stage 0.0 (TID 22) (172.21.12.47, executor 0, partition 22, ANY, 7460 bytes) 
23/05/31 12:22:12 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 2804 ms on 172.21.12.47 (executor 0) (2/76)
23/05/31 12:22:12 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 2805 ms on 172.21.12.47 (executor 0) (3/76)
23/05/31 12:22:12 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 2803 ms on 172.21.12.47 (executor 0) (4/76)
23/05/31 12:22:12 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 2810 ms on 172.21.12.47 (executor 0) (5/76)
23/05/31 12:22:12 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2828 ms on 172.21.12.47 (executor 0) (6/76)
23/05/31 12:22:12 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 2809 ms on 172.21.12.47 (executor 0) (7/76)
23/05/31 12:22:13 INFO TaskSetManager: Starting task 23.0 in stage 0.0 (TID 23) (172.21.12.47, executor 0, partition 23, ANY, 7460 bytes) 
23/05/31 12:22:13 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 2892 ms on 172.21.12.47 (executor 0) (8/76)
23/05/31 12:22:13 INFO TaskSetManager: Starting task 24.0 in stage 0.0 (TID 24) (172.21.12.48, executor 1, partition 24, ANY, 7460 bytes) 
23/05/31 12:22:13 INFO TaskSetManager: Finished task 11.0 in stage 0.0 (TID 11) in 3216 ms on 172.21.12.48 (executor 1) (9/76)
23/05/31 12:22:13 INFO TaskSetManager: Starting task 25.0 in stage 0.0 (TID 25) (172.21.12.48, executor 1, partition 25, ANY, 7460 bytes) 
23/05/31 12:22:13 INFO TaskSetManager: Finished task 10.0 in stage 0.0 (TID 10) in 3337 ms on 172.21.12.48 (executor 1) (10/76)
23/05/31 12:22:13 INFO TaskSetManager: Starting task 26.0 in stage 0.0 (TID 26) (172.21.12.48, executor 1, partition 26, ANY, 7460 bytes) 
23/05/31 12:22:13 INFO TaskSetManager: Finished task 12.0 in stage 0.0 (TID 12) in 3497 ms on 172.21.12.48 (executor 1) (11/76)
23/05/31 12:22:13 INFO TaskSetManager: Starting task 27.0 in stage 0.0 (TID 27) (172.21.12.48, executor 1, partition 27, ANY, 7460 bytes) 
23/05/31 12:22:13 INFO TaskSetManager: Starting task 28.0 in stage 0.0 (TID 28) (172.21.12.48, executor 1, partition 28, ANY, 7460 bytes) 
23/05/31 12:22:13 INFO TaskSetManager: Finished task 15.0 in stage 0.0 (TID 15) in 3525 ms on 172.21.12.48 (executor 1) (12/76)
23/05/31 12:22:13 INFO TaskSetManager: Finished task 13.0 in stage 0.0 (TID 13) in 3527 ms on 172.21.12.48 (executor 1) (13/76)
23/05/31 12:22:13 INFO TaskSetManager: Starting task 29.0 in stage 0.0 (TID 29) (172.21.12.48, executor 1, partition 29, ANY, 7460 bytes) 
23/05/31 12:22:13 INFO TaskSetManager: Finished task 8.0 in stage 0.0 (TID 8) in 3555 ms on 172.21.12.48 (executor 1) (14/76)
23/05/31 12:22:13 INFO TaskSetManager: Starting task 30.0 in stage 0.0 (TID 30) (172.21.12.48, executor 1, partition 30, ANY, 7460 bytes) 
23/05/31 12:22:13 INFO TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 3566 ms on 172.21.12.48 (executor 1) (15/76)
23/05/31 12:22:13 INFO TaskSetManager: Starting task 31.0 in stage 0.0 (TID 31) (172.21.12.48, executor 1, partition 31, ANY, 7460 bytes) 
23/05/31 12:22:13 INFO TaskSetManager: Finished task 14.0 in stage 0.0 (TID 14) in 3571 ms on 172.21.12.48 (executor 1) (16/76)
23/05/31 12:22:14 INFO TaskSetManager: Starting task 32.0 in stage 0.0 (TID 32) (172.21.12.47, executor 0, partition 32, ANY, 7460 bytes) 
23/05/31 12:22:14 INFO TaskSetManager: Starting task 33.0 in stage 0.0 (TID 33) (172.21.12.47, executor 0, partition 33, ANY, 7460 bytes) 
23/05/31 12:22:14 INFO TaskSetManager: Finished task 19.0 in stage 0.0 (TID 19) in 1204 ms on 172.21.12.47 (executor 0) (17/76)
23/05/31 12:22:14 INFO TaskSetManager: Starting task 34.0 in stage 0.0 (TID 34) (172.21.12.47, executor 0, partition 34, ANY, 7460 bytes) 
23/05/31 12:22:14 INFO TaskSetManager: Finished task 22.0 in stage 0.0 (TID 22) in 1197 ms on 172.21.12.47 (executor 0) (18/76)
23/05/31 12:22:14 INFO TaskSetManager: Finished task 18.0 in stage 0.0 (TID 18) in 1209 ms on 172.21.12.47 (executor 0) (19/76)
23/05/31 12:22:14 INFO TaskSetManager: Starting task 35.0 in stage 0.0 (TID 35) (172.21.12.47, executor 0, partition 35, ANY, 7460 bytes) 
23/05/31 12:22:14 INFO TaskSetManager: Starting task 36.0 in stage 0.0 (TID 36) (172.21.12.47, executor 0, partition 36, ANY, 7460 bytes) 
23/05/31 12:22:14 INFO TaskSetManager: Finished task 23.0 in stage 0.0 (TID 23) in 1141 ms on 172.21.12.47 (executor 0) (20/76)
23/05/31 12:22:14 INFO TaskSetManager: Finished task 16.0 in stage 0.0 (TID 16) in 1287 ms on 172.21.12.47 (executor 0) (21/76)
23/05/31 12:22:14 INFO TaskSetManager: Starting task 37.0 in stage 0.0 (TID 37) (172.21.12.47, executor 0, partition 37, ANY, 7460 bytes) 
23/05/31 12:22:14 INFO TaskSetManager: Starting task 38.0 in stage 0.0 (TID 38) (172.21.12.47, executor 0, partition 38, ANY, 7460 bytes) 
23/05/31 12:22:14 INFO TaskSetManager: Finished task 17.0 in stage 0.0 (TID 17) in 1255 ms on 172.21.12.47 (executor 0) (22/76)
23/05/31 12:22:14 INFO TaskSetManager: Finished task 21.0 in stage 0.0 (TID 21) in 1246 ms on 172.21.12.47 (executor 0) (23/76)
23/05/31 12:22:14 INFO TaskSetManager: Starting task 39.0 in stage 0.0 (TID 39) (172.21.12.47, executor 0, partition 39, ANY, 7460 bytes) 
23/05/31 12:22:14 INFO TaskSetManager: Finished task 20.0 in stage 0.0 (TID 20) in 1261 ms on 172.21.12.47 (executor 0) (24/76)
23/05/31 12:22:14 INFO TaskSetManager: Starting task 40.0 in stage 0.0 (TID 40) (172.21.12.47, executor 0, partition 40, ANY, 7460 bytes) 
23/05/31 12:22:14 INFO TaskSetManager: Finished task 37.0 in stage 0.0 (TID 37) in 304 ms on 172.21.12.47 (executor 0) (25/76)
23/05/31 12:22:14 INFO TaskSetManager: Starting task 41.0 in stage 0.0 (TID 41) (172.21.12.48, executor 1, partition 41, ANY, 7460 bytes) 
23/05/31 12:22:14 INFO TaskSetManager: Finished task 24.0 in stage 0.0 (TID 24) in 1179 ms on 172.21.12.48 (executor 1) (26/76)
23/05/31 12:22:14 INFO TaskSetManager: Starting task 42.0 in stage 0.0 (TID 42) (172.21.12.48, executor 1, partition 42, ANY, 7460 bytes) 
23/05/31 12:22:14 INFO TaskSetManager: Finished task 25.0 in stage 0.0 (TID 25) in 1170 ms on 172.21.12.48 (executor 1) (27/76)
23/05/31 12:22:15 INFO TaskSetManager: Starting task 43.0 in stage 0.0 (TID 43) (172.21.12.48, executor 1, partition 43, ANY, 7460 bytes) 
23/05/31 12:22:15 INFO TaskSetManager: Finished task 26.0 in stage 0.0 (TID 26) in 1169 ms on 172.21.12.48 (executor 1) (28/76)
23/05/31 12:22:15 INFO TaskSetManager: Starting task 44.0 in stage 0.0 (TID 44) (172.21.12.48, executor 1, partition 44, ANY, 7460 bytes) 
23/05/31 12:22:15 INFO TaskSetManager: Finished task 27.0 in stage 0.0 (TID 27) in 1207 ms on 172.21.12.48 (executor 1) (29/76)
23/05/31 12:22:15 INFO TaskSetManager: Starting task 45.0 in stage 0.0 (TID 45) (172.21.12.48, executor 1, partition 45, ANY, 7460 bytes) 
23/05/31 12:22:15 INFO TaskSetManager: Finished task 29.0 in stage 0.0 (TID 29) in 1189 ms on 172.21.12.48 (executor 1) (30/76)
23/05/31 12:22:15 INFO TaskSetManager: Starting task 46.0 in stage 0.0 (TID 46) (172.21.12.48, executor 1, partition 46, ANY, 7460 bytes) 
23/05/31 12:22:15 INFO TaskSetManager: Finished task 28.0 in stage 0.0 (TID 28) in 1241 ms on 172.21.12.48 (executor 1) (31/76)
23/05/31 12:22:15 INFO TaskSetManager: Starting task 47.0 in stage 0.0 (TID 47) (172.21.12.48, executor 1, partition 47, ANY, 7460 bytes) 
23/05/31 12:22:15 INFO TaskSetManager: Finished task 30.0 in stage 0.0 (TID 30) in 1247 ms on 172.21.12.48 (executor 1) (32/76)
23/05/31 12:22:15 INFO TaskSetManager: Starting task 48.0 in stage 0.0 (TID 48) (172.21.12.47, executor 0, partition 48, ANY, 7460 bytes) 
23/05/31 12:22:15 INFO TaskSetManager: Starting task 49.0 in stage 0.0 (TID 49) (172.21.12.47, executor 0, partition 49, ANY, 7460 bytes) 
23/05/31 12:22:15 INFO TaskSetManager: Finished task 33.0 in stage 0.0 (TID 33) in 1109 ms on 172.21.12.47 (executor 0) (33/76)
23/05/31 12:22:15 INFO TaskSetManager: Finished task 34.0 in stage 0.0 (TID 34) in 1105 ms on 172.21.12.47 (executor 0) (34/76)
23/05/31 12:22:15 INFO TaskSetManager: Starting task 50.0 in stage 0.0 (TID 50) (172.21.12.47, executor 0, partition 50, ANY, 7460 bytes) 
23/05/31 12:22:15 INFO TaskSetManager: Finished task 35.0 in stage 0.0 (TID 35) in 1088 ms on 172.21.12.47 (executor 0) (35/76)
23/05/31 12:22:15 INFO TaskSetManager: Starting task 51.0 in stage 0.0 (TID 51) (172.21.12.47, executor 0, partition 51, ANY, 7460 bytes) 
23/05/31 12:22:15 INFO TaskSetManager: Starting task 52.0 in stage 0.0 (TID 52) (172.21.12.48, executor 1, partition 52, ANY, 7460 bytes) 
23/05/31 12:22:15 INFO TaskSetManager: Finished task 36.0 in stage 0.0 (TID 36) in 1090 ms on 172.21.12.47 (executor 0) (36/76)
23/05/31 12:22:15 INFO TaskSetManager: Finished task 31.0 in stage 0.0 (TID 31) in 1305 ms on 172.21.12.48 (executor 1) (37/76)
23/05/31 12:22:15 INFO TaskSetManager: Starting task 53.0 in stage 0.0 (TID 53) (172.21.12.47, executor 0, partition 53, ANY, 7460 bytes) 
23/05/31 12:22:15 INFO TaskSetManager: Finished task 38.0 in stage 0.0 (TID 38) in 1085 ms on 172.21.12.47 (executor 0) (38/76)
23/05/31 12:22:15 INFO TaskSetManager: Starting task 54.0 in stage 0.0 (TID 54) (172.21.12.47, executor 0, partition 54, ANY, 7460 bytes) 
23/05/31 12:22:15 INFO TaskSetManager: Finished task 32.0 in stage 0.0 (TID 32) in 1224 ms on 172.21.12.47 (executor 0) (39/76)
23/05/31 12:22:15 INFO TaskSetManager: Starting task 55.0 in stage 0.0 (TID 55) (172.21.12.47, executor 0, partition 55, ANY, 7460 bytes) 
23/05/31 12:22:15 INFO TaskSetManager: Finished task 39.0 in stage 0.0 (TID 39) in 1175 ms on 172.21.12.47 (executor 0) (40/76)
23/05/31 12:22:15 INFO TaskSetManager: Starting task 56.0 in stage 0.0 (TID 56) (172.21.12.47, executor 0, partition 56, ANY, 7460 bytes) 
23/05/31 12:22:15 INFO TaskSetManager: Finished task 40.0 in stage 0.0 (TID 40) in 1088 ms on 172.21.12.47 (executor 0) (41/76)
23/05/31 12:22:15 INFO TaskSetManager: Starting task 57.0 in stage 0.0 (TID 57) (172.21.12.48, executor 1, partition 57, ANY, 7460 bytes) 
23/05/31 12:22:15 INFO TaskSetManager: Finished task 41.0 in stage 0.0 (TID 41) in 1146 ms on 172.21.12.48 (executor 1) (42/76)
23/05/31 12:22:16 INFO TaskSetManager: Starting task 58.0 in stage 0.0 (TID 58) (172.21.12.48, executor 1, partition 58, ANY, 7460 bytes) 
23/05/31 12:22:16 INFO TaskSetManager: Finished task 42.0 in stage 0.0 (TID 42) in 1167 ms on 172.21.12.48 (executor 1) (43/76)
23/05/31 12:22:16 INFO TaskSetManager: Starting task 59.0 in stage 0.0 (TID 59) (172.21.12.48, executor 1, partition 59, ANY, 7460 bytes) 
23/05/31 12:22:16 INFO TaskSetManager: Finished task 43.0 in stage 0.0 (TID 43) in 1158 ms on 172.21.12.48 (executor 1) (44/76)
23/05/31 12:22:16 INFO TaskSetManager: Starting task 60.0 in stage 0.0 (TID 60) (172.21.12.48, executor 1, partition 60, ANY, 7460 bytes) 
23/05/31 12:22:16 INFO TaskSetManager: Finished task 45.0 in stage 0.0 (TID 45) in 1138 ms on 172.21.12.48 (executor 1) (45/76)
23/05/31 12:22:16 INFO TaskSetManager: Starting task 61.0 in stage 0.0 (TID 61) (172.21.12.47, executor 0, partition 61, ANY, 7460 bytes) 
23/05/31 12:22:16 INFO TaskSetManager: Finished task 49.0 in stage 0.0 (TID 49) in 1064 ms on 172.21.12.47 (executor 0) (46/76)
23/05/31 12:22:16 INFO TaskSetManager: Starting task 62.0 in stage 0.0 (TID 62) (172.21.12.47, executor 0, partition 62, ANY, 7460 bytes) 
23/05/31 12:22:16 INFO TaskSetManager: Finished task 51.0 in stage 0.0 (TID 51) in 1057 ms on 172.21.12.47 (executor 0) (47/76)
23/05/31 12:22:16 INFO TaskSetManager: Starting task 63.0 in stage 0.0 (TID 63) (172.21.12.48, executor 1, partition 63, ANY, 7460 bytes) 
23/05/31 12:22:16 INFO TaskSetManager: Finished task 44.0 in stage 0.0 (TID 44) in 1207 ms on 172.21.12.48 (executor 1) (48/76)
23/05/31 12:22:16 INFO TaskSetManager: Starting task 64.0 in stage 0.0 (TID 64) (172.21.12.47, executor 0, partition 64, ANY, 7460 bytes) 
23/05/31 12:22:16 INFO TaskSetManager: Finished task 53.0 in stage 0.0 (TID 53) in 1073 ms on 172.21.12.47 (executor 0) (49/76)
23/05/31 12:22:16 INFO TaskSetManager: Starting task 65.0 in stage 0.0 (TID 65) (172.21.12.48, executor 1, partition 65, ANY, 7460 bytes) 
23/05/31 12:22:16 INFO TaskSetManager: Starting task 66.0 in stage 0.0 (TID 66) (172.21.12.47, executor 0, partition 66, ANY, 7460 bytes) 
23/05/31 12:22:16 INFO TaskSetManager: Finished task 46.0 in stage 0.0 (TID 46) in 1212 ms on 172.21.12.48 (executor 1) (50/76)
23/05/31 12:22:16 INFO TaskSetManager: Finished task 50.0 in stage 0.0 (TID 50) in 1107 ms on 172.21.12.47 (executor 0) (51/76)
23/05/31 12:22:16 INFO TaskSetManager: Starting task 67.0 in stage 0.0 (TID 67) (172.21.12.48, executor 1, partition 67, ANY, 7460 bytes) 
23/05/31 12:22:16 INFO TaskSetManager: Finished task 52.0 in stage 0.0 (TID 52) in 1116 ms on 172.21.12.48 (executor 1) (52/76)
23/05/31 12:22:16 INFO TaskSetManager: Starting task 68.0 in stage 0.0 (TID 68) (172.21.12.47, executor 0, partition 68, ANY, 7460 bytes) 
23/05/31 12:22:16 INFO TaskSetManager: Finished task 48.0 in stage 0.0 (TID 48) in 1133 ms on 172.21.12.47 (executor 0) (53/76)
23/05/31 12:22:16 INFO TaskSetManager: Starting task 69.0 in stage 0.0 (TID 69) (172.21.12.47, executor 0, partition 69, ANY, 7460 bytes) 
23/05/31 12:22:16 INFO TaskSetManager: Starting task 70.0 in stage 0.0 (TID 70) (172.21.12.47, executor 0, partition 70, ANY, 7460 bytes) 
23/05/31 12:22:16 INFO TaskSetManager: Finished task 54.0 in stage 0.0 (TID 54) in 1033 ms on 172.21.12.47 (executor 0) (54/76)
23/05/31 12:22:16 INFO TaskSetManager: Starting task 71.0 in stage 0.0 (TID 71) (172.21.12.48, executor 1, partition 71, ANY, 7460 bytes) 
23/05/31 12:22:16 INFO TaskSetManager: Finished task 55.0 in stage 0.0 (TID 55) in 1018 ms on 172.21.12.47 (executor 0) (55/76)
23/05/31 12:22:16 INFO TaskSetManager: Finished task 47.0 in stage 0.0 (TID 47) in 1202 ms on 172.21.12.48 (executor 1) (56/76)
23/05/31 12:22:16 INFO TaskSetManager: Starting task 72.0 in stage 0.0 (TID 72) (172.21.12.47, executor 0, partition 72, ANY, 7460 bytes) 
23/05/31 12:22:16 INFO TaskSetManager: Finished task 56.0 in stage 0.0 (TID 56) in 1040 ms on 172.21.12.47 (executor 0) (57/76)
23/05/31 12:22:17 INFO TaskSetManager: Starting task 73.0 in stage 0.0 (TID 73) (172.21.12.48, executor 1, partition 73, ANY, 7460 bytes) 
23/05/31 12:22:17 INFO TaskSetManager: Finished task 57.0 in stage 0.0 (TID 57) in 1126 ms on 172.21.12.48 (executor 1) (58/76)
23/05/31 12:22:17 INFO TaskSetManager: Starting task 74.0 in stage 0.0 (TID 74) (172.21.12.48, executor 1, partition 74, ANY, 7460 bytes) 
23/05/31 12:22:17 INFO TaskSetManager: Finished task 58.0 in stage 0.0 (TID 58) in 1143 ms on 172.21.12.48 (executor 1) (59/76)
23/05/31 12:22:17 INFO TaskSetManager: Starting task 75.0 in stage 0.0 (TID 75) (172.21.12.48, executor 1, partition 75, ANY, 7460 bytes) 
23/05/31 12:22:17 INFO TaskSetManager: Finished task 59.0 in stage 0.0 (TID 59) in 1179 ms on 172.21.12.48 (executor 1) (60/76)
23/05/31 12:22:17 INFO TaskSetManager: Finished task 60.0 in stage 0.0 (TID 60) in 1133 ms on 172.21.12.48 (executor 1) (61/76)
23/05/31 12:22:17 INFO TaskSetManager: Finished task 62.0 in stage 0.0 (TID 62) in 1102 ms on 172.21.12.47 (executor 0) (62/76)
23/05/31 12:22:17 INFO TaskSetManager: Finished task 64.0 in stage 0.0 (TID 64) in 1096 ms on 172.21.12.47 (executor 0) (63/76)
23/05/31 12:22:17 INFO TaskSetManager: Finished task 66.0 in stage 0.0 (TID 66) in 1094 ms on 172.21.12.47 (executor 0) (64/76)
23/05/31 12:22:17 INFO TaskSetManager: Finished task 61.0 in stage 0.0 (TID 61) in 1154 ms on 172.21.12.47 (executor 0) (65/76)
23/05/31 12:22:17 INFO TaskSetManager: Finished task 68.0 in stage 0.0 (TID 68) in 1092 ms on 172.21.12.47 (executor 0) (66/76)
23/05/31 12:22:17 INFO TaskSetManager: Finished task 69.0 in stage 0.0 (TID 69) in 1089 ms on 172.21.12.47 (executor 0) (67/76)
23/05/31 12:22:17 INFO TaskSetManager: Finished task 70.0 in stage 0.0 (TID 70) in 1087 ms on 172.21.12.47 (executor 0) (68/76)
23/05/31 12:22:17 INFO TaskSetManager: Finished task 71.0 in stage 0.0 (TID 71) in 1212 ms on 172.21.12.48 (executor 1) (69/76)
23/05/31 12:22:17 INFO TaskSetManager: Finished task 63.0 in stage 0.0 (TID 63) in 1315 ms on 172.21.12.48 (executor 1) (70/76)
23/05/31 12:22:17 INFO TaskSetManager: Finished task 67.0 in stage 0.0 (TID 67) in 1274 ms on 172.21.12.48 (executor 1) (71/76)
23/05/31 12:22:17 INFO TaskSetManager: Finished task 65.0 in stage 0.0 (TID 65) in 1289 ms on 172.21.12.48 (executor 1) (72/76)
23/05/31 12:22:17 INFO TaskSetManager: Finished task 75.0 in stage 0.0 (TID 75) in 328 ms on 172.21.12.48 (executor 1) (73/76)
23/05/31 12:22:17 INFO TaskSetManager: Finished task 72.0 in stage 0.0 (TID 72) in 1121 ms on 172.21.12.47 (executor 0) (74/76)
23/05/31 12:22:18 INFO TaskSetManager: Finished task 73.0 in stage 0.0 (TID 73) in 1078 ms on 172.21.12.48 (executor 1) (75/76)
23/05/31 12:22:18 INFO TaskSetManager: Finished task 74.0 in stage 0.0 (TID 74) in 1073 ms on 172.21.12.48 (executor 1) (76/76)
23/05/31 12:22:18 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
23/05/31 12:22:18 INFO DAGScheduler: ShuffleMapStage 0 (newAPIHadoopFile at TeraSort.scala:60) finished in 8.396 s
23/05/31 12:22:18 INFO DAGScheduler: looking for newly runnable stages
23/05/31 12:22:18 INFO DAGScheduler: running: Set()
23/05/31 12:22:18 INFO DAGScheduler: waiting: Set(ResultStage 1)
23/05/31 12:22:18 INFO DAGScheduler: failed: Set()
23/05/31 12:22:18 INFO DAGScheduler: Submitting ResultStage 1 (ShuffledRDD[1] at repartitionAndSortWithinPartitions at TeraSort.scala:63), which has no missing parents
23/05/31 12:22:18 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 101.9 KiB, free 1048.5 MiB)
23/05/31 12:22:18 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 37.0 KiB, free 1048.4 MiB)
23/05/31 12:22:18 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on aion-0129:35531 (size: 37.0 KiB, free: 1048.7 MiB)
23/05/31 12:22:18 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1535
23/05/31 12:22:18 INFO DAGScheduler: Submitting 76 missing tasks from ResultStage 1 (ShuffledRDD[1] at repartitionAndSortWithinPartitions at TeraSort.scala:63) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
23/05/31 12:22:18 INFO TaskSchedulerImpl: Adding task set 1.0 with 76 tasks resource profile 0
23/05/31 12:22:18 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 76) (172.21.12.47, executor 0, partition 0, NODE_LOCAL, 7185 bytes) 
23/05/31 12:22:18 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 77) (172.21.12.48, executor 1, partition 1, NODE_LOCAL, 7185 bytes) 
23/05/31 12:22:18 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 78) (172.21.12.47, executor 0, partition 2, NODE_LOCAL, 7185 bytes) 
23/05/31 12:22:18 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 79) (172.21.12.48, executor 1, partition 3, NODE_LOCAL, 7185 bytes) 
23/05/31 12:22:18 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 80) (172.21.12.47, executor 0, partition 4, NODE_LOCAL, 7185 bytes) 
23/05/31 12:22:18 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 81) (172.21.12.48, executor 1, partition 5, NODE_LOCAL, 7185 bytes) 
23/05/31 12:22:18 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 82) (172.21.12.47, executor 0, partition 6, NODE_LOCAL, 7185 bytes) 
23/05/31 12:22:18 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 83) (172.21.12.48, executor 1, partition 7, NODE_LOCAL, 7185 bytes) 
23/05/31 12:22:18 INFO TaskSetManager: Starting task 8.0 in stage 1.0 (TID 84) (172.21.12.47, executor 0, partition 8, NODE_LOCAL, 7185 bytes) 
23/05/31 12:22:18 INFO TaskSetManager: Starting task 9.0 in stage 1.0 (TID 85) (172.21.12.48, executor 1, partition 9, NODE_LOCAL, 7185 bytes) 
23/05/31 12:22:18 INFO TaskSetManager: Starting task 10.0 in stage 1.0 (TID 86) (172.21.12.47, executor 0, partition 10, NODE_LOCAL, 7185 bytes) 
23/05/31 12:22:18 INFO TaskSetManager: Starting task 11.0 in stage 1.0 (TID 87) (172.21.12.48, executor 1, partition 11, NODE_LOCAL, 7185 bytes) 
23/05/31 12:22:18 INFO TaskSetManager: Starting task 12.0 in stage 1.0 (TID 88) (172.21.12.47, executor 0, partition 12, NODE_LOCAL, 7185 bytes) 
23/05/31 12:22:18 INFO TaskSetManager: Starting task 13.0 in stage 1.0 (TID 89) (172.21.12.48, executor 1, partition 13, NODE_LOCAL, 7185 bytes) 
23/05/31 12:22:18 INFO TaskSetManager: Starting task 14.0 in stage 1.0 (TID 90) (172.21.12.47, executor 0, partition 14, NODE_LOCAL, 7185 bytes) 
23/05/31 12:22:18 INFO TaskSetManager: Starting task 15.0 in stage 1.0 (TID 91) (172.21.12.48, executor 1, partition 15, NODE_LOCAL, 7185 bytes) 
23/05/31 12:22:18 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.21.12.47:36545 (size: 37.0 KiB, free: 7.0 GiB)
23/05/31 12:22:18 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.21.12.48:38673 (size: 37.0 KiB, free: 7.0 GiB)
23/05/31 12:22:18 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.21.12.47:39364
23/05/31 12:22:18 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.21.12.48:59970
23/05/31 12:22:28 INFO TaskSetManager: Starting task 16.0 in stage 1.0 (TID 92) (172.21.12.48, executor 1, partition 16, NODE_LOCAL, 7185 bytes) 
23/05/31 12:22:28 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 77) in 10270 ms on 172.21.12.48 (executor 1) (1/76)
23/05/31 12:22:28 INFO TaskSetManager: Starting task 17.0 in stage 1.0 (TID 93) (172.21.12.47, executor 0, partition 17, NODE_LOCAL, 7185 bytes) 
23/05/31 12:22:28 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 78) in 10466 ms on 172.21.12.47 (executor 0) (2/76)
23/05/31 12:22:29 INFO TaskSetManager: Starting task 18.0 in stage 1.0 (TID 94) (172.21.12.48, executor 1, partition 18, NODE_LOCAL, 7185 bytes) 
23/05/31 12:22:29 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 81) in 10796 ms on 172.21.12.48 (executor 1) (3/76)
23/05/31 12:22:29 INFO TaskSetManager: Starting task 19.0 in stage 1.0 (TID 95) (172.21.12.47, executor 0, partition 19, NODE_LOCAL, 7185 bytes) 
23/05/31 12:22:29 INFO TaskSetManager: Finished task 10.0 in stage 1.0 (TID 86) in 10885 ms on 172.21.12.47 (executor 0) (4/76)
23/05/31 12:22:29 INFO TaskSetManager: Starting task 20.0 in stage 1.0 (TID 96) (172.21.12.47, executor 0, partition 20, NODE_LOCAL, 7185 bytes) 
23/05/31 12:22:29 INFO TaskSetManager: Finished task 12.0 in stage 1.0 (TID 88) in 11247 ms on 172.21.12.47 (executor 0) (5/76)
23/05/31 12:22:29 INFO TaskSetManager: Starting task 21.0 in stage 1.0 (TID 97) (172.21.12.47, executor 0, partition 21, NODE_LOCAL, 7185 bytes) 
23/05/31 12:22:29 INFO TaskSetManager: Finished task 8.0 in stage 1.0 (TID 84) in 11265 ms on 172.21.12.47 (executor 0) (6/76)
23/05/31 12:22:30 INFO TaskSetManager: Starting task 22.0 in stage 1.0 (TID 98) (172.21.12.48, executor 1, partition 22, NODE_LOCAL, 7185 bytes) 
23/05/31 12:22:30 INFO TaskSetManager: Finished task 11.0 in stage 1.0 (TID 87) in 12338 ms on 172.21.12.48 (executor 1) (7/76)
23/05/31 12:22:31 INFO TaskSetManager: Starting task 23.0 in stage 1.0 (TID 99) (172.21.12.47, executor 0, partition 23, NODE_LOCAL, 7185 bytes) 
23/05/31 12:22:31 INFO TaskSetManager: Finished task 14.0 in stage 1.0 (TID 90) in 12919 ms on 172.21.12.47 (executor 0) (8/76)
23/05/31 12:22:31 INFO TaskSetManager: Starting task 24.0 in stage 1.0 (TID 100) (172.21.12.48, executor 1, partition 24, NODE_LOCAL, 7185 bytes) 
23/05/31 12:22:31 INFO TaskSetManager: Finished task 9.0 in stage 1.0 (TID 85) in 12995 ms on 172.21.12.48 (executor 1) (9/76)
23/05/31 12:22:31 INFO TaskSetManager: Starting task 25.0 in stage 1.0 (TID 101) (172.21.12.48, executor 1, partition 25, NODE_LOCAL, 7185 bytes) 
23/05/31 12:22:31 INFO TaskSetManager: Finished task 15.0 in stage 1.0 (TID 91) in 13562 ms on 172.21.12.48 (executor 1) (10/76)
23/05/31 12:22:32 INFO TaskSetManager: Starting task 26.0 in stage 1.0 (TID 102) (172.21.12.47, executor 0, partition 26, NODE_LOCAL, 7185 bytes) 
23/05/31 12:22:32 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 80) in 13712 ms on 172.21.12.47 (executor 0) (11/76)
23/05/31 12:22:32 INFO TaskSetManager: Starting task 27.0 in stage 1.0 (TID 103) (172.21.12.48, executor 1, partition 27, NODE_LOCAL, 7185 bytes) 
23/05/31 12:22:32 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 79) in 13919 ms on 172.21.12.48 (executor 1) (12/76)
23/05/31 12:22:32 INFO TaskSetManager: Starting task 28.0 in stage 1.0 (TID 104) (172.21.12.47, executor 0, partition 28, NODE_LOCAL, 7185 bytes) 
23/05/31 12:22:32 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 82) in 14168 ms on 172.21.12.47 (executor 0) (13/76)
23/05/31 12:22:33 INFO TaskSetManager: Starting task 29.0 in stage 1.0 (TID 105) (172.21.12.48, executor 1, partition 29, NODE_LOCAL, 7185 bytes) 
23/05/31 12:22:33 INFO TaskSetManager: Finished task 13.0 in stage 1.0 (TID 89) in 14815 ms on 172.21.12.48 (executor 1) (14/76)
23/05/31 12:22:33 INFO TaskSetManager: Starting task 30.0 in stage 1.0 (TID 106) (172.21.12.47, executor 0, partition 30, NODE_LOCAL, 7185 bytes) 
23/05/31 12:22:33 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 76) in 15018 ms on 172.21.12.47 (executor 0) (15/76)
23/05/31 12:22:33 INFO TaskSetManager: Starting task 31.0 in stage 1.0 (TID 107) (172.21.12.48, executor 1, partition 31, NODE_LOCAL, 7185 bytes) 
23/05/31 12:22:33 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 83) in 15152 ms on 172.21.12.48 (executor 1) (16/76)
23/05/31 12:22:35 INFO TaskSetManager: Starting task 32.0 in stage 1.0 (TID 108) (172.21.12.47, executor 0, partition 32, NODE_LOCAL, 7185 bytes) 
23/05/31 12:22:35 INFO TaskSetManager: Finished task 17.0 in stage 1.0 (TID 93) in 6572 ms on 172.21.12.47 (executor 0) (17/76)
23/05/31 12:22:36 INFO TaskSetManager: Starting task 33.0 in stage 1.0 (TID 109) (172.21.12.48, executor 1, partition 33, NODE_LOCAL, 7185 bytes) 
23/05/31 12:22:36 INFO TaskSetManager: Finished task 16.0 in stage 1.0 (TID 92) in 7956 ms on 172.21.12.48 (executor 1) (18/76)
23/05/31 12:22:38 INFO TaskSetManager: Starting task 34.0 in stage 1.0 (TID 110) (172.21.12.48, executor 1, partition 34, NODE_LOCAL, 7185 bytes) 
23/05/31 12:22:38 INFO TaskSetManager: Finished task 18.0 in stage 1.0 (TID 94) in 9174 ms on 172.21.12.48 (executor 1) (19/76)
23/05/31 12:22:38 INFO TaskSetManager: Starting task 35.0 in stage 1.0 (TID 111) (172.21.12.47, executor 0, partition 35, NODE_LOCAL, 7185 bytes) 
23/05/31 12:22:38 INFO TaskSetManager: Finished task 19.0 in stage 1.0 (TID 95) in 9401 ms on 172.21.12.47 (executor 0) (20/76)
23/05/31 12:22:38 INFO TaskSetManager: Starting task 36.0 in stage 1.0 (TID 112) (172.21.12.48, executor 1, partition 36, NODE_LOCAL, 7185 bytes) 
23/05/31 12:22:38 INFO TaskSetManager: Starting task 37.0 in stage 1.0 (TID 113) (172.21.12.47, executor 0, partition 37, NODE_LOCAL, 7185 bytes) 
23/05/31 12:22:38 INFO TaskSetManager: Finished task 20.0 in stage 1.0 (TID 96) in 9316 ms on 172.21.12.47 (executor 0) (21/76)
23/05/31 12:22:38 INFO TaskSetManager: Finished task 22.0 in stage 1.0 (TID 98) in 8226 ms on 172.21.12.48 (executor 1) (22/76)
23/05/31 12:22:41 INFO TaskSetManager: Starting task 38.0 in stage 1.0 (TID 114) (172.21.12.47, executor 0, partition 38, NODE_LOCAL, 7185 bytes) 
23/05/31 12:22:41 INFO TaskSetManager: Finished task 23.0 in stage 1.0 (TID 99) in 10249 ms on 172.21.12.47 (executor 0) (23/76)
23/05/31 12:22:41 INFO TaskSetManager: Starting task 39.0 in stage 1.0 (TID 115) (172.21.12.48, executor 1, partition 39, NODE_LOCAL, 7185 bytes) 
23/05/31 12:22:41 INFO TaskSetManager: Finished task 24.0 in stage 1.0 (TID 100) in 10495 ms on 172.21.12.48 (executor 1) (24/76)
23/05/31 12:22:42 INFO TaskSetManager: Starting task 40.0 in stage 1.0 (TID 116) (172.21.12.47, executor 0, partition 40, NODE_LOCAL, 7185 bytes) 
23/05/31 12:22:42 INFO TaskSetManager: Finished task 21.0 in stage 1.0 (TID 97) in 12563 ms on 172.21.12.47 (executor 0) (25/76)
23/05/31 12:22:43 INFO TaskSetManager: Starting task 41.0 in stage 1.0 (TID 117) (172.21.12.48, executor 1, partition 41, NODE_LOCAL, 7185 bytes) 
23/05/31 12:22:43 INFO TaskSetManager: Finished task 25.0 in stage 1.0 (TID 101) in 12101 ms on 172.21.12.48 (executor 1) (26/76)
23/05/31 12:22:44 INFO TaskSetManager: Starting task 42.0 in stage 1.0 (TID 118) (172.21.12.47, executor 0, partition 42, NODE_LOCAL, 7185 bytes) 
23/05/31 12:22:44 INFO TaskSetManager: Finished task 28.0 in stage 1.0 (TID 104) in 11841 ms on 172.21.12.47 (executor 0) (27/76)
23/05/31 12:22:44 INFO TaskSetManager: Starting task 43.0 in stage 1.0 (TID 119) (172.21.12.48, executor 1, partition 43, NODE_LOCAL, 7185 bytes) 
23/05/31 12:22:44 INFO TaskSetManager: Finished task 27.0 in stage 1.0 (TID 103) in 12349 ms on 172.21.12.48 (executor 1) (28/76)
23/05/31 12:22:44 INFO TaskSetManager: Starting task 44.0 in stage 1.0 (TID 120) (172.21.12.48, executor 1, partition 44, NODE_LOCAL, 7185 bytes) 
23/05/31 12:22:44 INFO TaskSetManager: Finished task 29.0 in stage 1.0 (TID 105) in 11468 ms on 172.21.12.48 (executor 1) (29/76)
23/05/31 12:22:45 INFO TaskSetManager: Starting task 45.0 in stage 1.0 (TID 121) (172.21.12.48, executor 1, partition 45, NODE_LOCAL, 7185 bytes) 
23/05/31 12:22:45 INFO TaskSetManager: Finished task 31.0 in stage 1.0 (TID 107) in 12109 ms on 172.21.12.48 (executor 1) (30/76)
23/05/31 12:22:46 INFO TaskSetManager: Starting task 46.0 in stage 1.0 (TID 122) (172.21.12.47, executor 0, partition 46, NODE_LOCAL, 7185 bytes) 
23/05/31 12:22:46 INFO TaskSetManager: Finished task 32.0 in stage 1.0 (TID 108) in 11378 ms on 172.21.12.47 (executor 0) (31/76)
23/05/31 12:22:46 INFO TaskSetManager: Starting task 47.0 in stage 1.0 (TID 123) (172.21.12.47, executor 0, partition 47, NODE_LOCAL, 7185 bytes) 
23/05/31 12:22:46 INFO TaskSetManager: Finished task 37.0 in stage 1.0 (TID 113) in 7866 ms on 172.21.12.47 (executor 0) (32/76)
23/05/31 12:22:46 INFO TaskSetManager: Starting task 48.0 in stage 1.0 (TID 124) (172.21.12.48, executor 1, partition 48, NODE_LOCAL, 7185 bytes) 
23/05/31 12:22:46 INFO TaskSetManager: Finished task 33.0 in stage 1.0 (TID 109) in 10474 ms on 172.21.12.48 (executor 1) (33/76)
23/05/31 12:22:48 INFO TaskSetManager: Starting task 49.0 in stage 1.0 (TID 125) (172.21.12.47, executor 0, partition 49, NODE_LOCAL, 7185 bytes) 
23/05/31 12:22:48 INFO TaskSetManager: Finished task 26.0 in stage 1.0 (TID 102) in 16324 ms on 172.21.12.47 (executor 0) (34/76)
23/05/31 12:22:49 INFO TaskSetManager: Starting task 50.0 in stage 1.0 (TID 126) (172.21.12.48, executor 1, partition 50, NODE_LOCAL, 7185 bytes) 
23/05/31 12:22:49 INFO TaskSetManager: Finished task 34.0 in stage 1.0 (TID 110) in 11476 ms on 172.21.12.48 (executor 1) (35/76)
23/05/31 12:22:50 INFO BlockManagerInfo: Removed broadcast_1_piece0 on aion-0129:35531 in memory (size: 2.9 KiB, free: 1048.7 MiB)
23/05/31 12:22:50 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.21.12.47:36545 in memory (size: 2.9 KiB, free: 7.0 GiB)
23/05/31 12:22:50 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.21.12.48:38673 in memory (size: 2.9 KiB, free: 7.0 GiB)
23/05/31 12:22:51 INFO TaskSetManager: Starting task 51.0 in stage 1.0 (TID 127) (172.21.12.47, executor 0, partition 51, NODE_LOCAL, 7185 bytes) 
23/05/31 12:22:51 INFO TaskSetManager: Finished task 35.0 in stage 1.0 (TID 111) in 13362 ms on 172.21.12.47 (executor 0) (36/76)
23/05/31 12:22:51 INFO TaskSetManager: Starting task 52.0 in stage 1.0 (TID 128) (172.21.12.47, executor 0, partition 52, NODE_LOCAL, 7185 bytes) 
23/05/31 12:22:51 INFO TaskSetManager: Finished task 40.0 in stage 1.0 (TID 116) in 9852 ms on 172.21.12.47 (executor 0) (37/76)
23/05/31 12:22:52 INFO TaskSetManager: Starting task 53.0 in stage 1.0 (TID 129) (172.21.12.47, executor 0, partition 53, NODE_LOCAL, 7185 bytes) 
23/05/31 12:22:52 INFO TaskSetManager: Finished task 30.0 in stage 1.0 (TID 106) in 19315 ms on 172.21.12.47 (executor 0) (38/76)
23/05/31 12:22:52 INFO TaskSetManager: Starting task 54.0 in stage 1.0 (TID 130) (172.21.12.47, executor 0, partition 54, NODE_LOCAL, 7185 bytes) 
23/05/31 12:22:52 INFO TaskSetManager: Finished task 42.0 in stage 1.0 (TID 118) in 8451 ms on 172.21.12.47 (executor 0) (39/76)
23/05/31 12:22:53 INFO TaskSetManager: Starting task 55.0 in stage 1.0 (TID 131) (172.21.12.48, executor 1, partition 55, NODE_LOCAL, 7185 bytes) 
23/05/31 12:22:53 INFO TaskSetManager: Finished task 36.0 in stage 1.0 (TID 112) in 14159 ms on 172.21.12.48 (executor 1) (40/76)
23/05/31 12:22:53 INFO TaskSetManager: Starting task 56.0 in stage 1.0 (TID 132) (172.21.12.47, executor 0, partition 56, NODE_LOCAL, 7185 bytes) 
23/05/31 12:22:53 INFO TaskSetManager: Finished task 38.0 in stage 1.0 (TID 114) in 11942 ms on 172.21.12.47 (executor 0) (41/76)
23/05/31 12:22:54 INFO TaskSetManager: Starting task 57.0 in stage 1.0 (TID 133) (172.21.12.47, executor 0, partition 57, NODE_LOCAL, 7185 bytes) 
23/05/31 12:22:54 INFO TaskSetManager: Finished task 46.0 in stage 1.0 (TID 122) in 7966 ms on 172.21.12.47 (executor 0) (42/76)
23/05/31 12:22:55 INFO TaskSetManager: Starting task 58.0 in stage 1.0 (TID 134) (172.21.12.48, executor 1, partition 58, NODE_LOCAL, 7185 bytes) 
23/05/31 12:22:55 INFO TaskSetManager: Finished task 39.0 in stage 1.0 (TID 115) in 13915 ms on 172.21.12.48 (executor 1) (43/76)
23/05/31 12:22:56 INFO TaskSetManager: Starting task 59.0 in stage 1.0 (TID 135) (172.21.12.48, executor 1, partition 59, NODE_LOCAL, 7185 bytes) 
23/05/31 12:22:56 INFO TaskSetManager: Finished task 41.0 in stage 1.0 (TID 117) in 13013 ms on 172.21.12.48 (executor 1) (44/76)
23/05/31 12:22:58 INFO TaskSetManager: Starting task 60.0 in stage 1.0 (TID 136) (172.21.12.48, executor 1, partition 60, NODE_LOCAL, 7185 bytes) 
23/05/31 12:22:58 INFO TaskSetManager: Finished task 43.0 in stage 1.0 (TID 119) in 13943 ms on 172.21.12.48 (executor 1) (45/76)
23/05/31 12:22:58 INFO TaskSetManager: Starting task 61.0 in stage 1.0 (TID 137) (172.21.12.47, executor 0, partition 61, NODE_LOCAL, 7185 bytes) 
23/05/31 12:22:58 INFO TaskSetManager: Finished task 47.0 in stage 1.0 (TID 123) in 12169 ms on 172.21.12.47 (executor 0) (46/76)
23/05/31 12:22:59 INFO TaskSetManager: Starting task 62.0 in stage 1.0 (TID 138) (172.21.12.47, executor 0, partition 62, NODE_LOCAL, 7185 bytes) 
23/05/31 12:22:59 INFO TaskSetManager: Finished task 53.0 in stage 1.0 (TID 129) in 7096 ms on 172.21.12.47 (executor 0) (47/76)
23/05/31 12:23:02 INFO TaskSetManager: Starting task 63.0 in stage 1.0 (TID 139) (172.21.12.47, executor 0, partition 63, NODE_LOCAL, 7185 bytes) 
23/05/31 12:23:02 INFO TaskSetManager: Finished task 57.0 in stage 1.0 (TID 133) in 7447 ms on 172.21.12.47 (executor 0) (48/76)
23/05/31 12:23:02 INFO TaskSetManager: Starting task 64.0 in stage 1.0 (TID 140) (172.21.12.48, executor 1, partition 64, NODE_LOCAL, 7185 bytes) 
23/05/31 12:23:02 INFO TaskSetManager: Finished task 44.0 in stage 1.0 (TID 120) in 17636 ms on 172.21.12.48 (executor 1) (49/76)
23/05/31 12:23:02 INFO TaskSetManager: Starting task 65.0 in stage 1.0 (TID 141) (172.21.12.47, executor 0, partition 65, NODE_LOCAL, 7185 bytes) 
23/05/31 12:23:02 INFO TaskSetManager: Finished task 49.0 in stage 1.0 (TID 125) in 14265 ms on 172.21.12.47 (executor 0) (50/76)
23/05/31 12:23:04 INFO TaskSetManager: Starting task 66.0 in stage 1.0 (TID 142) (172.21.12.47, executor 0, partition 66, NODE_LOCAL, 7185 bytes) 
23/05/31 12:23:04 INFO TaskSetManager: Finished task 54.0 in stage 1.0 (TID 130) in 11389 ms on 172.21.12.47 (executor 0) (51/76)
23/05/31 12:23:04 INFO TaskSetManager: Starting task 67.0 in stage 1.0 (TID 143) (172.21.12.47, executor 0, partition 67, NODE_LOCAL, 7185 bytes) 
23/05/31 12:23:04 INFO TaskSetManager: Finished task 51.0 in stage 1.0 (TID 127) in 12214 ms on 172.21.12.47 (executor 0) (52/76)
23/05/31 12:23:05 INFO TaskSetManager: Starting task 68.0 in stage 1.0 (TID 144) (172.21.12.47, executor 0, partition 68, NODE_LOCAL, 7185 bytes) 
23/05/31 12:23:05 INFO TaskSetManager: Finished task 52.0 in stage 1.0 (TID 128) in 13156 ms on 172.21.12.47 (executor 0) (53/76)
23/05/31 12:23:05 INFO TaskSetManager: Starting task 69.0 in stage 1.0 (TID 145) (172.21.12.47, executor 0, partition 69, NODE_LOCAL, 7185 bytes) 
23/05/31 12:23:05 INFO TaskSetManager: Finished task 61.0 in stage 1.0 (TID 137) in 6549 ms on 172.21.12.47 (executor 0) (54/76)
23/05/31 12:23:05 INFO TaskSetManager: Starting task 70.0 in stage 1.0 (TID 146) (172.21.12.47, executor 0, partition 70, NODE_LOCAL, 7185 bytes) 
23/05/31 12:23:05 INFO TaskSetManager: Finished task 56.0 in stage 1.0 (TID 132) in 12226 ms on 172.21.12.47 (executor 0) (55/76)
23/05/31 12:23:07 INFO TaskSetManager: Starting task 71.0 in stage 1.0 (TID 147) (172.21.12.48, executor 1, partition 71, NODE_LOCAL, 7185 bytes) 
23/05/31 12:23:07 INFO TaskSetManager: Finished task 45.0 in stage 1.0 (TID 121) in 21630 ms on 172.21.12.48 (executor 1) (56/76)
23/05/31 12:23:09 INFO TaskSetManager: Starting task 72.0 in stage 1.0 (TID 148) (172.21.12.48, executor 1, partition 72, NODE_LOCAL, 7185 bytes) 
23/05/31 12:23:09 INFO TaskSetManager: Finished task 48.0 in stage 1.0 (TID 124) in 22249 ms on 172.21.12.48 (executor 1) (57/76)
23/05/31 12:23:09 INFO TaskSetManager: Starting task 73.0 in stage 1.0 (TID 149) (172.21.12.47, executor 0, partition 73, NODE_LOCAL, 7185 bytes) 
23/05/31 12:23:09 INFO TaskSetManager: Finished task 65.0 in stage 1.0 (TID 141) in 6891 ms on 172.21.12.47 (executor 0) (58/76)
23/05/31 12:23:10 INFO TaskSetManager: Starting task 74.0 in stage 1.0 (TID 150) (172.21.12.47, executor 0, partition 74, NODE_LOCAL, 7185 bytes) 
23/05/31 12:23:10 INFO TaskSetManager: Finished task 63.0 in stage 1.0 (TID 139) in 8683 ms on 172.21.12.47 (executor 0) (59/76)
23/05/31 12:23:12 INFO TaskSetManager: Starting task 75.0 in stage 1.0 (TID 151) (172.21.12.47, executor 0, partition 75, NODE_LOCAL, 7185 bytes) 
23/05/31 12:23:12 INFO TaskSetManager: Finished task 67.0 in stage 1.0 (TID 143) in 8577 ms on 172.21.12.47 (executor 0) (60/76)
23/05/31 12:23:12 INFO TaskSetManager: Finished task 50.0 in stage 1.0 (TID 126) in 23013 ms on 172.21.12.48 (executor 1) (61/76)
23/05/31 12:23:13 INFO TaskSetManager: Finished task 62.0 in stage 1.0 (TID 138) in 13660 ms on 172.21.12.47 (executor 0) (62/76)
23/05/31 12:23:13 INFO TaskSetManager: Finished task 68.0 in stage 1.0 (TID 144) in 8748 ms on 172.21.12.47 (executor 0) (63/76)
23/05/31 12:23:14 INFO TaskSetManager: Finished task 66.0 in stage 1.0 (TID 142) in 10361 ms on 172.21.12.47 (executor 0) (64/76)
23/05/31 12:23:14 INFO TaskSetManager: Finished task 70.0 in stage 1.0 (TID 146) in 8901 ms on 172.21.12.47 (executor 0) (65/76)
23/05/31 12:23:16 INFO TaskSetManager: Finished task 55.0 in stage 1.0 (TID 131) in 23545 ms on 172.21.12.48 (executor 1) (66/76)
23/05/31 12:23:17 INFO TaskSetManager: Finished task 69.0 in stage 1.0 (TID 145) in 12351 ms on 172.21.12.47 (executor 0) (67/76)
23/05/31 12:23:17 INFO TaskSetManager: Finished task 58.0 in stage 1.0 (TID 134) in 22285 ms on 172.21.12.48 (executor 1) (68/76)
23/05/31 12:23:18 INFO TaskSetManager: Finished task 75.0 in stage 1.0 (TID 151) in 5860 ms on 172.21.12.47 (executor 0) (69/76)
23/05/31 12:23:19 INFO TaskSetManager: Finished task 73.0 in stage 1.0 (TID 149) in 9600 ms on 172.21.12.47 (executor 0) (70/76)
23/05/31 12:23:19 INFO TaskSetManager: Finished task 74.0 in stage 1.0 (TID 150) in 8813 ms on 172.21.12.47 (executor 0) (71/76)
23/05/31 12:23:19 INFO TaskSetManager: Finished task 59.0 in stage 1.0 (TID 135) in 22807 ms on 172.21.12.48 (executor 1) (72/76)
23/05/31 12:23:21 INFO TaskSetManager: Finished task 60.0 in stage 1.0 (TID 136) in 23478 ms on 172.21.12.48 (executor 1) (73/76)
23/05/31 12:23:23 INFO TaskSetManager: Finished task 64.0 in stage 1.0 (TID 140) in 21365 ms on 172.21.12.48 (executor 1) (74/76)
23/05/31 12:23:24 INFO TaskSetManager: Finished task 72.0 in stage 1.0 (TID 148) in 15165 ms on 172.21.12.48 (executor 1) (75/76)
23/05/31 12:23:24 INFO TaskSetManager: Finished task 71.0 in stage 1.0 (TID 147) in 17251 ms on 172.21.12.48 (executor 1) (76/76)
23/05/31 12:23:24 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
23/05/31 12:23:24 INFO DAGScheduler: ResultStage 1 (runJob at SparkHadoopWriter.scala:83) finished in 66.160 s
23/05/31 12:23:24 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
23/05/31 12:23:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
23/05/31 12:23:24 INFO DAGScheduler: Job 0 finished: runJob at SparkHadoopWriter.scala:83, took 74.646376 s
23/05/31 12:23:24 INFO SparkHadoopWriter: Start to commit write Job job_202305311222097391052444080749958_0001.
23/05/31 12:23:24 INFO SparkHadoopWriter: Write Job job_202305311222097391052444080749958_0001 committed. Elapsed time: 230 ms.
==== TeraSort took 75.115s ====
23/05/31 12:23:24 INFO SparkContext: SparkContext is stopping with exitCode 0.
23/05/31 12:23:24 INFO SparkUI: Stopped Spark web UI at http://aion-0129:4040
23/05/31 12:23:24 INFO StandaloneSchedulerBackend: Shutting down all executors
23/05/31 12:23:24 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down
23/05/31 12:23:24 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/05/31 12:23:24 INFO MemoryStore: MemoryStore cleared
23/05/31 12:23:24 INFO BlockManager: BlockManager stopped
23/05/31 12:23:24 INFO BlockManagerMaster: BlockManagerMaster stopped
23/05/31 12:23:24 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/05/31 12:23:24 INFO SparkContext: Successfully stopped SparkContext
23/05/31 12:23:24 INFO ShutdownHookManager: Shutdown hook called
23/05/31 12:23:24 INFO ShutdownHookManager: Deleting directory /tmp/spark-01915051-55a1-43ab-97da-2c702bdcb330
23/05/31 12:23:24 INFO ShutdownHookManager: Deleting directory /tmp/spark-324123b8-f1c2-463b-8c64-ed572300f3fc
omarcu@aion-0129:/opt/spark$ ./bin/spark-submit --master spark://aion-0129:7078 --class com.github.ehiggs.spark.terasort.TeraValidate /home/users/omarcu/spark/spark-terasort-1.2-SNAPSHOT-jar-with-dependencies.jar hdfs://aion-0129:9000/terasort_out hdfs://aion-0129:9000/terasort_validate
23/05/31 12:24:09 INFO SparkContext: Running Spark version 3.4.0
23/05/31 12:24:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/05/31 12:24:09 INFO ResourceUtils: ==============================================================
23/05/31 12:24:09 INFO ResourceUtils: No custom resources configured for spark.driver.
23/05/31 12:24:09 INFO ResourceUtils: ==============================================================
23/05/31 12:24:09 INFO SparkContext: Submitted application: TeraValidate
23/05/31 12:24:09 INFO SparkContext: Spark configuration:
spark.app.name=TeraValidate
spark.app.startTime=1685535849074
spark.app.submitTime=1685535849028
spark.cores.max=16
spark.driver.extraJavaOptions=-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false
spark.driver.host=aion-0129
spark.driver.memory=2g
spark.executor.cores=8
spark.executor.extraJavaOptions=-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false
spark.executor.memory=12g
spark.jars=file:/home/users/omarcu/spark/spark-terasort-1.2-SNAPSHOT-jar-with-dependencies.jar
spark.logConf=true
spark.master=spark://aion-0129:7078
spark.serializer=org.apache.spark.serializer.KryoSerializer
spark.submit.deployMode=client
spark.submit.pyFiles=
23/05/31 12:24:09 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 8, script: , vendor: , memory -> name: memory, amount: 12288, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/05/31 12:24:09 INFO ResourceProfile: Limiting resource is cpus at 8 tasks per executor
23/05/31 12:24:09 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/05/31 12:24:09 INFO SecurityManager: Changing view acls to: omarcu
23/05/31 12:24:09 INFO SecurityManager: Changing modify acls to: omarcu
23/05/31 12:24:09 INFO SecurityManager: Changing view acls groups to: 
23/05/31 12:24:09 INFO SecurityManager: Changing modify acls groups to: 
23/05/31 12:24:09 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: omarcu; groups with view permissions: EMPTY; users with modify permissions: omarcu; groups with modify permissions: EMPTY
23/05/31 12:24:09 INFO Utils: Successfully started service 'sparkDriver' on port 42621.
23/05/31 12:24:09 INFO SparkEnv: Registering MapOutputTracker
23/05/31 12:24:09 INFO SparkEnv: Registering BlockManagerMaster
23/05/31 12:24:09 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/05/31 12:24:09 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/05/31 12:24:09 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/05/31 12:24:09 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-99f4c365-0138-48af-84b7-ad4c077044bc
23/05/31 12:24:09 INFO MemoryStore: MemoryStore started with capacity 1048.8 MiB
23/05/31 12:24:09 INFO SparkEnv: Registering OutputCommitCoordinator
23/05/31 12:24:09 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
23/05/31 12:24:09 INFO Utils: Successfully started service 'SparkUI' on port 4040.
23/05/31 12:24:09 INFO SparkContext: Added JAR file:/home/users/omarcu/spark/spark-terasort-1.2-SNAPSHOT-jar-with-dependencies.jar at spark://aion-0129:42621/jars/spark-terasort-1.2-SNAPSHOT-jar-with-dependencies.jar with timestamp 1685535849074
23/05/31 12:24:10 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://aion-0129:7078...
23/05/31 12:24:10 INFO TransportClientFactory: Successfully created connection to aion-0129/172.21.12.45:7078 after 30 ms (0 ms spent in bootstraps)
23/05/31 12:24:10 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20230531122410-0004
23/05/31 12:24:10 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20230531122410-0004/0 on worker-20230531120144-172.21.12.47-34815 (172.21.12.47:34815) with 8 core(s)
23/05/31 12:24:10 INFO StandaloneSchedulerBackend: Granted executor ID app-20230531122410-0004/0 on hostPort 172.21.12.47:34815 with 8 core(s), 12.0 GiB RAM
23/05/31 12:24:10 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20230531122410-0004/1 on worker-20230531120146-172.21.12.48-34843 (172.21.12.48:34843) with 8 core(s)
23/05/31 12:24:10 INFO StandaloneSchedulerBackend: Granted executor ID app-20230531122410-0004/1 on hostPort 172.21.12.48:34843 with 8 core(s), 12.0 GiB RAM
23/05/31 12:24:10 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36863.
23/05/31 12:24:10 INFO NettyBlockTransferService: Server created on aion-0129:36863
23/05/31 12:24:10 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/05/31 12:24:10 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, aion-0129, 36863, None)
23/05/31 12:24:10 INFO BlockManagerMasterEndpoint: Registering block manager aion-0129:36863 with 1048.8 MiB RAM, BlockManagerId(driver, aion-0129, 36863, None)
23/05/31 12:24:10 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, aion-0129, 36863, None)
23/05/31 12:24:10 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, aion-0129, 36863, None)
23/05/31 12:24:10 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20230531122410-0004/0 is now RUNNING
23/05/31 12:24:10 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20230531122410-0004/1 is now RUNNING
23/05/31 12:24:10 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
23/05/31 12:24:11 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 192.6 KiB, free 1048.6 MiB)
23/05/31 12:24:11 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.7 KiB, free 1048.6 MiB)
23/05/31 12:24:11 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on aion-0129:36863 (size: 32.7 KiB, free: 1048.8 MiB)
23/05/31 12:24:11 INFO SparkContext: Created broadcast 0 from newAPIHadoopFile at TeraValidate.scala:60
23/05/31 12:24:11 INFO FileInputFormat: Total input files to process : 76
23/05/31 12:24:11 INFO SparkContext: Starting job: collect at TeraValidate.scala:98
23/05/31 12:24:11 INFO DAGScheduler: Got job 0 (collect at TeraValidate.scala:98) with 76 output partitions
23/05/31 12:24:11 INFO DAGScheduler: Final stage: ResultStage 0 (collect at TeraValidate.scala:98)
23/05/31 12:24:11 INFO DAGScheduler: Parents of final stage: List()
23/05/31 12:24:11 INFO DAGScheduler: Missing parents: List()
23/05/31 12:24:11 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at mapPartitions at TeraValidate.scala:66), which has no missing parents
23/05/31 12:24:11 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 4.7 KiB, free 1048.6 MiB)
23/05/31 12:24:11 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.6 KiB, free 1048.6 MiB)
23/05/31 12:24:11 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on aion-0129:36863 (size: 2.6 KiB, free: 1048.8 MiB)
23/05/31 12:24:11 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535
23/05/31 12:24:11 INFO DAGScheduler: Submitting 76 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at mapPartitions at TeraValidate.scala:66) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
23/05/31 12:24:11 INFO TaskSchedulerImpl: Adding task set 0.0 with 76 tasks resource profile 0
23/05/31 12:24:12 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.21.12.47:50192) with ID 0,  ResourceProfileId 0
23/05/31 12:24:12 INFO BlockManagerMasterEndpoint: Registering block manager 172.21.12.47:34079 with 7.0 GiB RAM, BlockManagerId(0, 172.21.12.47, 34079, None)
23/05/31 12:24:12 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.21.12.47, executor 0, partition 0, ANY, 7472 bytes) 
23/05/31 12:24:12 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (172.21.12.47, executor 0, partition 1, ANY, 7472 bytes) 
23/05/31 12:24:12 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2) (172.21.12.47, executor 0, partition 2, ANY, 7472 bytes) 
23/05/31 12:24:12 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3) (172.21.12.47, executor 0, partition 3, ANY, 7472 bytes) 
23/05/31 12:24:12 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4) (172.21.12.47, executor 0, partition 4, ANY, 7472 bytes) 
23/05/31 12:24:12 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5) (172.21.12.47, executor 0, partition 5, ANY, 7472 bytes) 
23/05/31 12:24:12 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6) (172.21.12.47, executor 0, partition 6, ANY, 7472 bytes) 
23/05/31 12:24:12 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7) (172.21.12.47, executor 0, partition 7, ANY, 7472 bytes) 
23/05/31 12:24:12 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.21.12.47:34079 (size: 2.6 KiB, free: 7.0 GiB)
23/05/31 12:24:12 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.21.12.48:51108) with ID 1,  ResourceProfileId 0
23/05/31 12:24:12 INFO BlockManagerMasterEndpoint: Registering block manager 172.21.12.48:33571 with 7.0 GiB RAM, BlockManagerId(1, 172.21.12.48, 33571, None)
23/05/31 12:24:13 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.21.12.47:34079 (size: 32.7 KiB, free: 7.0 GiB)
23/05/31 12:24:13 INFO TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8) (172.21.12.48, executor 1, partition 8, ANY, 7472 bytes) 
23/05/31 12:24:13 INFO TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9) (172.21.12.48, executor 1, partition 9, ANY, 7472 bytes) 
23/05/31 12:24:13 INFO TaskSetManager: Starting task 10.0 in stage 0.0 (TID 10) (172.21.12.48, executor 1, partition 10, ANY, 7472 bytes) 
23/05/31 12:24:13 INFO TaskSetManager: Starting task 11.0 in stage 0.0 (TID 11) (172.21.12.48, executor 1, partition 11, ANY, 7472 bytes) 
23/05/31 12:24:13 INFO TaskSetManager: Starting task 12.0 in stage 0.0 (TID 12) (172.21.12.48, executor 1, partition 12, ANY, 7472 bytes) 
23/05/31 12:24:13 INFO TaskSetManager: Starting task 13.0 in stage 0.0 (TID 13) (172.21.12.48, executor 1, partition 13, ANY, 7472 bytes) 
23/05/31 12:24:13 INFO TaskSetManager: Starting task 14.0 in stage 0.0 (TID 14) (172.21.12.48, executor 1, partition 14, ANY, 7472 bytes) 
23/05/31 12:24:13 INFO TaskSetManager: Starting task 15.0 in stage 0.0 (TID 15) (172.21.12.48, executor 1, partition 15, ANY, 7472 bytes) 
23/05/31 12:24:13 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.21.12.48:33571 (size: 2.6 KiB, free: 7.0 GiB)
23/05/31 12:24:13 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.21.12.48:33571 (size: 32.7 KiB, free: 7.0 GiB)
23/05/31 12:24:14 INFO TaskSetManager: Starting task 16.0 in stage 0.0 (TID 16) (172.21.12.47, executor 0, partition 16, ANY, 7472 bytes) 
23/05/31 12:24:14 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 1845 ms on 172.21.12.47 (executor 0) (1/76)
23/05/31 12:24:14 INFO TaskSetManager: Starting task 17.0 in stage 0.0 (TID 17) (172.21.12.47, executor 0, partition 17, ANY, 7472 bytes) 
23/05/31 12:24:14 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2128 ms on 172.21.12.47 (executor 0) (2/76)
23/05/31 12:24:14 INFO TaskSetManager: Starting task 18.0 in stage 0.0 (TID 18) (172.21.12.47, executor 0, partition 18, ANY, 7472 bytes) 
23/05/31 12:24:14 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 2116 ms on 172.21.12.47 (executor 0) (3/76)
23/05/31 12:24:14 INFO TaskSetManager: Starting task 19.0 in stage 0.0 (TID 19) (172.21.12.47, executor 0, partition 19, ANY, 7472 bytes) 
23/05/31 12:24:14 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 2133 ms on 172.21.12.47 (executor 0) (4/76)
23/05/31 12:24:14 INFO TaskSetManager: Starting task 20.0 in stage 0.0 (TID 20) (172.21.12.47, executor 0, partition 20, ANY, 7472 bytes) 
23/05/31 12:24:14 INFO TaskSetManager: Finished task 16.0 in stage 0.0 (TID 16) in 422 ms on 172.21.12.47 (executor 0) (5/76)
23/05/31 12:24:14 INFO TaskSetManager: Starting task 21.0 in stage 0.0 (TID 21) (172.21.12.47, executor 0, partition 21, ANY, 7472 bytes) 
23/05/31 12:24:14 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 2254 ms on 172.21.12.47 (executor 0) (6/76)
23/05/31 12:24:14 INFO TaskSetManager: Starting task 22.0 in stage 0.0 (TID 22) (172.21.12.47, executor 0, partition 22, ANY, 7472 bytes) 
23/05/31 12:24:14 INFO TaskSetManager: Starting task 23.0 in stage 0.0 (TID 23) (172.21.12.47, executor 0, partition 23, ANY, 7472 bytes) 
23/05/31 12:24:14 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 2392 ms on 172.21.12.47 (executor 0) (7/76)
23/05/31 12:24:14 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 2392 ms on 172.21.12.47 (executor 0) (8/76)
23/05/31 12:24:14 INFO TaskSetManager: Starting task 24.0 in stage 0.0 (TID 24) (172.21.12.47, executor 0, partition 24, ANY, 7472 bytes) 
23/05/31 12:24:14 INFO TaskSetManager: Starting task 25.0 in stage 0.0 (TID 25) (172.21.12.47, executor 0, partition 25, ANY, 7472 bytes) 
23/05/31 12:24:14 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 2404 ms on 172.21.12.47 (executor 0) (9/76)
23/05/31 12:24:14 INFO TaskSetManager: Starting task 26.0 in stage 0.0 (TID 26) (172.21.12.47, executor 0, partition 26, ANY, 7472 bytes) 
23/05/31 12:24:14 INFO TaskSetManager: Finished task 18.0 in stage 0.0 (TID 18) in 296 ms on 172.21.12.47 (executor 0) (10/76)
23/05/31 12:24:14 INFO TaskSetManager: Starting task 27.0 in stage 0.0 (TID 27) (172.21.12.47, executor 0, partition 27, ANY, 7472 bytes) 
23/05/31 12:24:14 INFO TaskSetManager: Finished task 17.0 in stage 0.0 (TID 17) in 317 ms on 172.21.12.47 (executor 0) (11/76)
23/05/31 12:24:14 INFO TaskSetManager: Finished task 19.0 in stage 0.0 (TID 19) in 289 ms on 172.21.12.47 (executor 0) (12/76)
23/05/31 12:24:14 INFO TaskSetManager: Starting task 28.0 in stage 0.0 (TID 28) (172.21.12.47, executor 0, partition 28, ANY, 7472 bytes) 
23/05/31 12:24:14 INFO TaskSetManager: Finished task 20.0 in stage 0.0 (TID 20) in 288 ms on 172.21.12.47 (executor 0) (13/76)
23/05/31 12:24:14 INFO TaskSetManager: Starting task 29.0 in stage 0.0 (TID 29) (172.21.12.47, executor 0, partition 29, ANY, 7472 bytes) 
23/05/31 12:24:14 INFO TaskSetManager: Finished task 21.0 in stage 0.0 (TID 21) in 286 ms on 172.21.12.47 (executor 0) (14/76)
23/05/31 12:24:15 INFO TaskSetManager: Starting task 30.0 in stage 0.0 (TID 30) (172.21.12.48, executor 1, partition 30, ANY, 7472 bytes) 
23/05/31 12:24:15 INFO TaskSetManager: Starting task 31.0 in stage 0.0 (TID 31) (172.21.12.48, executor 1, partition 31, ANY, 7472 bytes) 
23/05/31 12:24:15 INFO TaskSetManager: Finished task 15.0 in stage 0.0 (TID 15) in 1893 ms on 172.21.12.48 (executor 1) (15/76)
23/05/31 12:24:15 INFO TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 1898 ms on 172.21.12.48 (executor 1) (16/76)
23/05/31 12:24:15 INFO TaskSetManager: Starting task 32.0 in stage 0.0 (TID 32) (172.21.12.47, executor 0, partition 32, ANY, 7472 bytes) 
23/05/31 12:24:15 INFO TaskSetManager: Starting task 33.0 in stage 0.0 (TID 33) (172.21.12.47, executor 0, partition 33, ANY, 7472 bytes) 
23/05/31 12:24:15 INFO TaskSetManager: Finished task 22.0 in stage 0.0 (TID 22) in 290 ms on 172.21.12.47 (executor 0) (17/76)
23/05/31 12:24:15 INFO TaskSetManager: Finished task 23.0 in stage 0.0 (TID 23) in 289 ms on 172.21.12.47 (executor 0) (18/76)
23/05/31 12:24:15 INFO TaskSetManager: Starting task 34.0 in stage 0.0 (TID 34) (172.21.12.47, executor 0, partition 34, ANY, 7472 bytes) 
23/05/31 12:24:15 INFO TaskSetManager: Starting task 35.0 in stage 0.0 (TID 35) (172.21.12.47, executor 0, partition 35, ANY, 7472 bytes) 
23/05/31 12:24:15 INFO TaskSetManager: Finished task 27.0 in stage 0.0 (TID 27) in 280 ms on 172.21.12.47 (executor 0) (19/76)
23/05/31 12:24:15 INFO TaskSetManager: Finished task 25.0 in stage 0.0 (TID 25) in 290 ms on 172.21.12.47 (executor 0) (20/76)
23/05/31 12:24:15 INFO TaskSetManager: Starting task 36.0 in stage 0.0 (TID 36) (172.21.12.47, executor 0, partition 36, ANY, 7472 bytes) 
23/05/31 12:24:15 INFO TaskSetManager: Starting task 37.0 in stage 0.0 (TID 37) (172.21.12.47, executor 0, partition 37, ANY, 7472 bytes) 
23/05/31 12:24:15 INFO TaskSetManager: Finished task 24.0 in stage 0.0 (TID 24) in 316 ms on 172.21.12.47 (executor 0) (21/76)
23/05/31 12:24:15 INFO TaskSetManager: Finished task 26.0 in stage 0.0 (TID 26) in 311 ms on 172.21.12.47 (executor 0) (22/76)
23/05/31 12:24:15 INFO TaskSetManager: Starting task 38.0 in stage 0.0 (TID 38) (172.21.12.47, executor 0, partition 38, ANY, 7472 bytes) 
23/05/31 12:24:15 INFO TaskSetManager: Finished task 28.0 in stage 0.0 (TID 28) in 269 ms on 172.21.12.47 (executor 0) (23/76)
23/05/31 12:24:15 INFO TaskSetManager: Starting task 39.0 in stage 0.0 (TID 39) (172.21.12.47, executor 0, partition 39, ANY, 7472 bytes) 
23/05/31 12:24:15 INFO TaskSetManager: Finished task 29.0 in stage 0.0 (TID 29) in 273 ms on 172.21.12.47 (executor 0) (24/76)
23/05/31 12:24:15 INFO TaskSetManager: Starting task 40.0 in stage 0.0 (TID 40) (172.21.12.47, executor 0, partition 40, ANY, 7472 bytes) 
23/05/31 12:24:15 INFO TaskSetManager: Finished task 33.0 in stage 0.0 (TID 33) in 266 ms on 172.21.12.47 (executor 0) (25/76)
23/05/31 12:24:15 INFO TaskSetManager: Starting task 41.0 in stage 0.0 (TID 41) (172.21.12.47, executor 0, partition 41, ANY, 7472 bytes) 
23/05/31 12:24:15 INFO TaskSetManager: Finished task 34.0 in stage 0.0 (TID 34) in 270 ms on 172.21.12.47 (executor 0) (26/76)
23/05/31 12:24:15 INFO TaskSetManager: Starting task 42.0 in stage 0.0 (TID 42) (172.21.12.47, executor 0, partition 42, ANY, 7472 bytes) 
23/05/31 12:24:15 INFO TaskSetManager: Finished task 35.0 in stage 0.0 (TID 35) in 275 ms on 172.21.12.47 (executor 0) (27/76)
23/05/31 12:24:15 INFO TaskSetManager: Starting task 43.0 in stage 0.0 (TID 43) (172.21.12.47, executor 0, partition 43, ANY, 7472 bytes) 
23/05/31 12:24:15 INFO TaskSetManager: Finished task 36.0 in stage 0.0 (TID 36) in 272 ms on 172.21.12.47 (executor 0) (28/76)
23/05/31 12:24:15 INFO TaskSetManager: Starting task 44.0 in stage 0.0 (TID 44) (172.21.12.47, executor 0, partition 44, ANY, 7472 bytes) 
23/05/31 12:24:15 INFO TaskSetManager: Starting task 45.0 in stage 0.0 (TID 45) (172.21.12.47, executor 0, partition 45, ANY, 7472 bytes) 
23/05/31 12:24:15 INFO TaskSetManager: Finished task 32.0 in stage 0.0 (TID 32) in 317 ms on 172.21.12.47 (executor 0) (29/76)
23/05/31 12:24:15 INFO TaskSetManager: Finished task 37.0 in stage 0.0 (TID 37) in 275 ms on 172.21.12.47 (executor 0) (30/76)
23/05/31 12:24:15 INFO TaskSetManager: Starting task 46.0 in stage 0.0 (TID 46) (172.21.12.47, executor 0, partition 46, ANY, 7472 bytes) 
23/05/31 12:24:15 INFO TaskSetManager: Finished task 38.0 in stage 0.0 (TID 38) in 270 ms on 172.21.12.47 (executor 0) (31/76)
23/05/31 12:24:15 INFO TaskSetManager: Starting task 47.0 in stage 0.0 (TID 47) (172.21.12.47, executor 0, partition 47, ANY, 7472 bytes) 
23/05/31 12:24:15 INFO TaskSetManager: Finished task 39.0 in stage 0.0 (TID 39) in 267 ms on 172.21.12.47 (executor 0) (32/76)
23/05/31 12:24:15 INFO TaskSetManager: Starting task 48.0 in stage 0.0 (TID 48) (172.21.12.47, executor 0, partition 48, ANY, 7472 bytes) 
23/05/31 12:24:15 INFO TaskSetManager: Finished task 40.0 in stage 0.0 (TID 40) in 267 ms on 172.21.12.47 (executor 0) (33/76)
23/05/31 12:24:15 INFO TaskSetManager: Starting task 49.0 in stage 0.0 (TID 49) (172.21.12.47, executor 0, partition 49, ANY, 7472 bytes) 
23/05/31 12:24:15 INFO TaskSetManager: Finished task 41.0 in stage 0.0 (TID 41) in 262 ms on 172.21.12.47 (executor 0) (34/76)
23/05/31 12:24:15 INFO TaskSetManager: Starting task 50.0 in stage 0.0 (TID 50) (172.21.12.47, executor 0, partition 50, ANY, 7472 bytes) 
23/05/31 12:24:15 INFO TaskSetManager: Finished task 42.0 in stage 0.0 (TID 42) in 262 ms on 172.21.12.47 (executor 0) (35/76)
23/05/31 12:24:15 INFO TaskSetManager: Starting task 51.0 in stage 0.0 (TID 51) (172.21.12.47, executor 0, partition 51, ANY, 7472 bytes) 
23/05/31 12:24:15 INFO TaskSetManager: Finished task 43.0 in stage 0.0 (TID 43) in 263 ms on 172.21.12.47 (executor 0) (36/76)
23/05/31 12:24:15 INFO TaskSetManager: Starting task 52.0 in stage 0.0 (TID 52) (172.21.12.47, executor 0, partition 52, ANY, 7472 bytes) 
23/05/31 12:24:15 INFO TaskSetManager: Starting task 53.0 in stage 0.0 (TID 53) (172.21.12.47, executor 0, partition 53, ANY, 7472 bytes) 
23/05/31 12:24:15 INFO TaskSetManager: Starting task 54.0 in stage 0.0 (TID 54) (172.21.12.48, executor 1, partition 54, ANY, 7472 bytes) 
23/05/31 12:24:15 INFO TaskSetManager: Finished task 45.0 in stage 0.0 (TID 45) in 262 ms on 172.21.12.47 (executor 0) (37/76)
23/05/31 12:24:15 INFO TaskSetManager: Finished task 44.0 in stage 0.0 (TID 44) in 265 ms on 172.21.12.47 (executor 0) (38/76)
23/05/31 12:24:15 INFO TaskSetManager: Finished task 12.0 in stage 0.0 (TID 12) in 2536 ms on 172.21.12.48 (executor 1) (39/76)
23/05/31 12:24:15 INFO TaskSetManager: Starting task 55.0 in stage 0.0 (TID 55) (172.21.12.48, executor 1, partition 55, ANY, 7472 bytes) 
23/05/31 12:24:15 INFO TaskSetManager: Finished task 14.0 in stage 0.0 (TID 14) in 2582 ms on 172.21.12.48 (executor 1) (40/76)
23/05/31 12:24:15 INFO TaskSetManager: Starting task 56.0 in stage 0.0 (TID 56) (172.21.12.47, executor 0, partition 56, ANY, 7472 bytes) 
23/05/31 12:24:15 INFO TaskSetManager: Finished task 46.0 in stage 0.0 (TID 46) in 264 ms on 172.21.12.47 (executor 0) (41/76)
23/05/31 12:24:15 INFO TaskSetManager: Starting task 57.0 in stage 0.0 (TID 57) (172.21.12.47, executor 0, partition 57, ANY, 7472 bytes) 
23/05/31 12:24:15 INFO TaskSetManager: Finished task 47.0 in stage 0.0 (TID 47) in 268 ms on 172.21.12.47 (executor 0) (42/76)
23/05/31 12:24:15 INFO TaskSetManager: Starting task 58.0 in stage 0.0 (TID 58) (172.21.12.48, executor 1, partition 58, ANY, 7472 bytes) 
23/05/31 12:24:15 INFO TaskSetManager: Finished task 11.0 in stage 0.0 (TID 11) in 2678 ms on 172.21.12.48 (executor 1) (43/76)
23/05/31 12:24:15 INFO TaskSetManager: Starting task 59.0 in stage 0.0 (TID 59) (172.21.12.48, executor 1, partition 59, ANY, 7472 bytes) 
23/05/31 12:24:15 INFO TaskSetManager: Finished task 8.0 in stage 0.0 (TID 8) in 2738 ms on 172.21.12.48 (executor 1) (44/76)
23/05/31 12:24:15 INFO TaskSetManager: Starting task 60.0 in stage 0.0 (TID 60) (172.21.12.47, executor 0, partition 60, ANY, 7472 bytes) 
23/05/31 12:24:15 INFO TaskSetManager: Finished task 49.0 in stage 0.0 (TID 49) in 262 ms on 172.21.12.47 (executor 0) (45/76)
23/05/31 12:24:15 INFO TaskSetManager: Starting task 61.0 in stage 0.0 (TID 61) (172.21.12.48, executor 1, partition 61, ANY, 7472 bytes) 
23/05/31 12:24:15 INFO TaskSetManager: Finished task 13.0 in stage 0.0 (TID 13) in 2785 ms on 172.21.12.48 (executor 1) (46/76)
23/05/31 12:24:15 INFO TaskSetManager: Starting task 62.0 in stage 0.0 (TID 62) (172.21.12.47, executor 0, partition 62, ANY, 7472 bytes) 
23/05/31 12:24:15 INFO TaskSetManager: Starting task 63.0 in stage 0.0 (TID 63) (172.21.12.47, executor 0, partition 63, ANY, 7472 bytes) 
23/05/31 12:24:15 INFO TaskSetManager: Starting task 64.0 in stage 0.0 (TID 64) (172.21.12.47, executor 0, partition 64, ANY, 7472 bytes) 
23/05/31 12:24:15 INFO TaskSetManager: Finished task 53.0 in stage 0.0 (TID 53) in 264 ms on 172.21.12.47 (executor 0) (47/76)
23/05/31 12:24:15 INFO TaskSetManager: Finished task 51.0 in stage 0.0 (TID 51) in 270 ms on 172.21.12.47 (executor 0) (48/76)
23/05/31 12:24:15 INFO TaskSetManager: Finished task 52.0 in stage 0.0 (TID 52) in 265 ms on 172.21.12.47 (executor 0) (49/76)
23/05/31 12:24:15 INFO TaskSetManager: Starting task 65.0 in stage 0.0 (TID 65) (172.21.12.47, executor 0, partition 65, ANY, 7472 bytes) 
23/05/31 12:24:15 INFO TaskSetManager: Finished task 48.0 in stage 0.0 (TID 48) in 328 ms on 172.21.12.47 (executor 0) (50/76)
23/05/31 12:24:15 INFO TaskSetManager: Starting task 66.0 in stage 0.0 (TID 66) (172.21.12.47, executor 0, partition 66, ANY, 7472 bytes) 
23/05/31 12:24:15 INFO TaskSetManager: Finished task 50.0 in stage 0.0 (TID 50) in 327 ms on 172.21.12.47 (executor 0) (51/76)
23/05/31 12:24:15 INFO TaskSetManager: Starting task 67.0 in stage 0.0 (TID 67) (172.21.12.48, executor 1, partition 67, ANY, 7472 bytes) 
23/05/31 12:24:15 INFO TaskSetManager: Finished task 10.0 in stage 0.0 (TID 10) in 2845 ms on 172.21.12.48 (executor 1) (52/76)
23/05/31 12:24:16 INFO TaskSetManager: Starting task 68.0 in stage 0.0 (TID 68) (172.21.12.47, executor 0, partition 68, ANY, 7472 bytes) 
23/05/31 12:24:16 INFO TaskSetManager: Starting task 69.0 in stage 0.0 (TID 69) (172.21.12.48, executor 1, partition 69, ANY, 7472 bytes) 
23/05/31 12:24:16 INFO TaskSetManager: Finished task 57.0 in stage 0.0 (TID 57) in 266 ms on 172.21.12.47 (executor 0) (53/76)
23/05/31 12:24:16 INFO TaskSetManager: Finished task 55.0 in stage 0.0 (TID 55) in 301 ms on 172.21.12.48 (executor 1) (54/76)
23/05/31 12:24:16 INFO TaskSetManager: Starting task 70.0 in stage 0.0 (TID 70) (172.21.12.47, executor 0, partition 70, ANY, 7472 bytes) 
23/05/31 12:24:16 INFO TaskSetManager: Finished task 56.0 in stage 0.0 (TID 56) in 293 ms on 172.21.12.47 (executor 0) (55/76)
23/05/31 12:24:16 INFO TaskSetManager: Starting task 71.0 in stage 0.0 (TID 71) (172.21.12.48, executor 1, partition 71, ANY, 7472 bytes) 
23/05/31 12:24:16 INFO TaskSetManager: Finished task 59.0 in stage 0.0 (TID 59) in 264 ms on 172.21.12.48 (executor 1) (56/76)
23/05/31 12:24:16 INFO TaskSetManager: Starting task 72.0 in stage 0.0 (TID 72) (172.21.12.47, executor 0, partition 72, ANY, 7472 bytes) 
23/05/31 12:24:16 INFO TaskSetManager: Finished task 60.0 in stage 0.0 (TID 60) in 268 ms on 172.21.12.47 (executor 0) (57/76)
23/05/31 12:24:16 INFO TaskSetManager: Starting task 73.0 in stage 0.0 (TID 73) (172.21.12.47, executor 0, partition 73, ANY, 7472 bytes) 
23/05/31 12:24:16 INFO TaskSetManager: Starting task 74.0 in stage 0.0 (TID 74) (172.21.12.47, executor 0, partition 74, ANY, 7472 bytes) 
23/05/31 12:24:16 INFO TaskSetManager: Starting task 75.0 in stage 0.0 (TID 75) (172.21.12.47, executor 0, partition 75, ANY, 7472 bytes) 
23/05/31 12:24:16 INFO TaskSetManager: Finished task 63.0 in stage 0.0 (TID 63) in 266 ms on 172.21.12.47 (executor 0) (58/76)
23/05/31 12:24:16 INFO TaskSetManager: Finished task 64.0 in stage 0.0 (TID 64) in 265 ms on 172.21.12.47 (executor 0) (59/76)
23/05/31 12:24:16 INFO TaskSetManager: Finished task 62.0 in stage 0.0 (TID 62) in 271 ms on 172.21.12.47 (executor 0) (60/76)
23/05/31 12:24:16 INFO TaskSetManager: Finished task 65.0 in stage 0.0 (TID 65) in 261 ms on 172.21.12.47 (executor 0) (61/76)
23/05/31 12:24:16 INFO TaskSetManager: Finished task 66.0 in stage 0.0 (TID 66) in 267 ms on 172.21.12.47 (executor 0) (62/76)
23/05/31 12:24:16 INFO TaskSetManager: Finished task 70.0 in stage 0.0 (TID 70) in 261 ms on 172.21.12.47 (executor 0) (63/76)
23/05/31 12:24:16 INFO TaskSetManager: Finished task 68.0 in stage 0.0 (TID 68) in 281 ms on 172.21.12.47 (executor 0) (64/76)
23/05/31 12:24:16 INFO TaskSetManager: Finished task 71.0 in stage 0.0 (TID 71) in 269 ms on 172.21.12.48 (executor 1) (65/76)
23/05/31 12:24:16 INFO TaskSetManager: Finished task 72.0 in stage 0.0 (TID 72) in 251 ms on 172.21.12.47 (executor 0) (66/76)
23/05/31 12:24:16 INFO TaskSetManager: Finished task 75.0 in stage 0.0 (TID 75) in 250 ms on 172.21.12.47 (executor 0) (67/76)
23/05/31 12:24:16 INFO TaskSetManager: Finished task 73.0 in stage 0.0 (TID 73) in 256 ms on 172.21.12.47 (executor 0) (68/76)
23/05/31 12:24:16 INFO TaskSetManager: Finished task 58.0 in stage 0.0 (TID 58) in 638 ms on 172.21.12.48 (executor 1) (69/76)
23/05/31 12:24:16 INFO TaskSetManager: Finished task 74.0 in stage 0.0 (TID 74) in 259 ms on 172.21.12.47 (executor 0) (70/76)
23/05/31 12:24:16 INFO TaskSetManager: Finished task 69.0 in stage 0.0 (TID 69) in 596 ms on 172.21.12.48 (executor 1) (71/76)
23/05/31 12:24:16 INFO TaskSetManager: Finished task 31.0 in stage 0.0 (TID 31) in 1656 ms on 172.21.12.48 (executor 1) (72/76)
23/05/31 12:24:16 INFO TaskSetManager: Finished task 30.0 in stage 0.0 (TID 30) in 1689 ms on 172.21.12.48 (executor 1) (73/76)
23/05/31 12:24:16 INFO TaskSetManager: Finished task 67.0 in stage 0.0 (TID 67) in 943 ms on 172.21.12.48 (executor 1) (74/76)
23/05/31 12:24:16 INFO TaskSetManager: Finished task 54.0 in stage 0.0 (TID 54) in 1256 ms on 172.21.12.48 (executor 1) (75/76)
23/05/31 12:24:16 INFO TaskSetManager: Finished task 61.0 in stage 0.0 (TID 61) in 1043 ms on 172.21.12.48 (executor 1) (76/76)
23/05/31 12:24:16 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
23/05/31 12:24:16 INFO DAGScheduler: ResultStage 0 (collect at TeraValidate.scala:98) finished in 5.125 s
23/05/31 12:24:16 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
23/05/31 12:24:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
23/05/31 12:24:16 INFO DAGScheduler: Job 0 finished: collect at TeraValidate.scala:98, took 5.181461 s
23/05/31 12:24:16 INFO SparkContext: Starting job: count at TeraValidate.scala:101
23/05/31 12:24:16 INFO DAGScheduler: Got job 1 (count at TeraValidate.scala:101) with 76 output partitions
23/05/31 12:24:16 INFO DAGScheduler: Final stage: ResultStage 1 (count at TeraValidate.scala:101)
23/05/31 12:24:16 INFO DAGScheduler: Parents of final stage: List()
23/05/31 12:24:16 INFO DAGScheduler: Missing parents: List()
23/05/31 12:24:16 INFO DAGScheduler: Submitting ResultStage 1 (hdfs://aion-0129:9000/terasort_out NewHadoopRDD[0] at newAPIHadoopFile at TeraValidate.scala:60), which has no missing parents
23/05/31 12:24:16 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 3.5 KiB, free 1048.6 MiB)
23/05/31 12:24:16 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.1 KiB, free 1048.6 MiB)
23/05/31 12:24:17 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on aion-0129:36863 (size: 2.1 KiB, free: 1048.8 MiB)
23/05/31 12:24:17 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1535
23/05/31 12:24:17 INFO DAGScheduler: Submitting 76 missing tasks from ResultStage 1 (hdfs://aion-0129:9000/terasort_out NewHadoopRDD[0] at newAPIHadoopFile at TeraValidate.scala:60) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
23/05/31 12:24:17 INFO TaskSchedulerImpl: Adding task set 1.0 with 76 tasks resource profile 0
23/05/31 12:24:17 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 76) (172.21.12.48, executor 1, partition 0, ANY, 7472 bytes) 
23/05/31 12:24:17 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 77) (172.21.12.47, executor 0, partition 1, ANY, 7472 bytes) 
23/05/31 12:24:17 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 78) (172.21.12.48, executor 1, partition 2, ANY, 7472 bytes) 
23/05/31 12:24:17 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 79) (172.21.12.47, executor 0, partition 3, ANY, 7472 bytes) 
23/05/31 12:24:17 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 80) (172.21.12.48, executor 1, partition 4, ANY, 7472 bytes) 
23/05/31 12:24:17 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 81) (172.21.12.47, executor 0, partition 5, ANY, 7472 bytes) 
23/05/31 12:24:17 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 82) (172.21.12.48, executor 1, partition 6, ANY, 7472 bytes) 
23/05/31 12:24:17 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 83) (172.21.12.47, executor 0, partition 7, ANY, 7472 bytes) 
23/05/31 12:24:17 INFO TaskSetManager: Starting task 8.0 in stage 1.0 (TID 84) (172.21.12.48, executor 1, partition 8, ANY, 7472 bytes) 
23/05/31 12:24:17 INFO TaskSetManager: Starting task 9.0 in stage 1.0 (TID 85) (172.21.12.47, executor 0, partition 9, ANY, 7472 bytes) 
23/05/31 12:24:17 INFO TaskSetManager: Starting task 10.0 in stage 1.0 (TID 86) (172.21.12.48, executor 1, partition 10, ANY, 7472 bytes) 
23/05/31 12:24:17 INFO TaskSetManager: Starting task 11.0 in stage 1.0 (TID 87) (172.21.12.47, executor 0, partition 11, ANY, 7472 bytes) 
23/05/31 12:24:17 INFO TaskSetManager: Starting task 12.0 in stage 1.0 (TID 88) (172.21.12.48, executor 1, partition 12, ANY, 7472 bytes) 
23/05/31 12:24:17 INFO TaskSetManager: Starting task 13.0 in stage 1.0 (TID 89) (172.21.12.47, executor 0, partition 13, ANY, 7472 bytes) 
23/05/31 12:24:17 INFO TaskSetManager: Starting task 14.0 in stage 1.0 (TID 90) (172.21.12.48, executor 1, partition 14, ANY, 7472 bytes) 
23/05/31 12:24:17 INFO TaskSetManager: Starting task 15.0 in stage 1.0 (TID 91) (172.21.12.47, executor 0, partition 15, ANY, 7472 bytes) 
23/05/31 12:24:17 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.21.12.47:34079 (size: 2.1 KiB, free: 7.0 GiB)
23/05/31 12:24:17 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.21.12.48:33571 (size: 2.1 KiB, free: 7.0 GiB)
23/05/31 12:24:17 INFO TaskSetManager: Starting task 16.0 in stage 1.0 (TID 92) (172.21.12.47, executor 0, partition 16, ANY, 7472 bytes) 
23/05/31 12:24:17 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 77) in 294 ms on 172.21.12.47 (executor 0) (1/76)
23/05/31 12:24:17 INFO TaskSetManager: Starting task 17.0 in stage 1.0 (TID 93) (172.21.12.48, executor 1, partition 17, ANY, 7472 bytes) 
23/05/31 12:24:17 INFO TaskSetManager: Starting task 18.0 in stage 1.0 (TID 94) (172.21.12.48, executor 1, partition 18, ANY, 7472 bytes) 
23/05/31 12:24:17 INFO TaskSetManager: Finished task 10.0 in stage 1.0 (TID 86) in 307 ms on 172.21.12.48 (executor 1) (2/76)
23/05/31 12:24:17 INFO TaskSetManager: Starting task 19.0 in stage 1.0 (TID 95) (172.21.12.48, executor 1, partition 19, ANY, 7472 bytes) 
23/05/31 12:24:17 INFO TaskSetManager: Finished task 12.0 in stage 1.0 (TID 88) in 308 ms on 172.21.12.48 (executor 1) (3/76)
23/05/31 12:24:17 INFO TaskSetManager: Starting task 20.0 in stage 1.0 (TID 96) (172.21.12.48, executor 1, partition 20, ANY, 7472 bytes) 
23/05/31 12:24:17 INFO TaskSetManager: Finished task 14.0 in stage 1.0 (TID 90) in 309 ms on 172.21.12.48 (executor 1) (4/76)
23/05/31 12:24:17 INFO TaskSetManager: Finished task 8.0 in stage 1.0 (TID 84) in 314 ms on 172.21.12.48 (executor 1) (5/76)
23/05/31 12:24:17 INFO TaskSetManager: Starting task 21.0 in stage 1.0 (TID 97) (172.21.12.47, executor 0, partition 21, ANY, 7472 bytes) 
23/05/31 12:24:17 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 79) in 321 ms on 172.21.12.47 (executor 0) (6/76)
23/05/31 12:24:17 INFO TaskSetManager: Starting task 22.0 in stage 1.0 (TID 98) (172.21.12.47, executor 0, partition 22, ANY, 7472 bytes) 
23/05/31 12:24:17 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 83) in 339 ms on 172.21.12.47 (executor 0) (7/76)
23/05/31 12:24:17 INFO TaskSetManager: Starting task 23.0 in stage 1.0 (TID 99) (172.21.12.48, executor 1, partition 23, ANY, 7472 bytes) 
23/05/31 12:24:17 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 80) in 349 ms on 172.21.12.48 (executor 1) (8/76)
23/05/31 12:24:17 INFO TaskSetManager: Starting task 24.0 in stage 1.0 (TID 100) (172.21.12.47, executor 0, partition 24, ANY, 7472 bytes) 
23/05/31 12:24:17 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 81) in 388 ms on 172.21.12.47 (executor 0) (9/76)
23/05/31 12:24:17 INFO TaskSetManager: Starting task 25.0 in stage 1.0 (TID 101) (172.21.12.47, executor 0, partition 25, ANY, 7472 bytes) 
23/05/31 12:24:17 INFO TaskSetManager: Finished task 16.0 in stage 1.0 (TID 92) in 226 ms on 172.21.12.47 (executor 0) (10/76)
23/05/31 12:24:17 INFO TaskSetManager: Starting task 26.0 in stage 1.0 (TID 102) (172.21.12.47, executor 0, partition 26, ANY, 7472 bytes) 
23/05/31 12:24:17 INFO TaskSetManager: Finished task 21.0 in stage 1.0 (TID 97) in 222 ms on 172.21.12.47 (executor 0) (11/76)
23/05/31 12:24:17 INFO TaskSetManager: Starting task 27.0 in stage 1.0 (TID 103) (172.21.12.48, executor 1, partition 27, ANY, 7472 bytes) 
23/05/31 12:24:17 INFO TaskSetManager: Finished task 18.0 in stage 1.0 (TID 94) in 241 ms on 172.21.12.48 (executor 1) (12/76)
23/05/31 12:24:17 INFO TaskSetManager: Starting task 28.0 in stage 1.0 (TID 104) (172.21.12.47, executor 0, partition 28, ANY, 7472 bytes) 
23/05/31 12:24:17 INFO TaskSetManager: Starting task 29.0 in stage 1.0 (TID 105) (172.21.12.48, executor 1, partition 29, ANY, 7472 bytes) 
23/05/31 12:24:17 INFO TaskSetManager: Finished task 17.0 in stage 1.0 (TID 93) in 246 ms on 172.21.12.48 (executor 1) (13/76)
23/05/31 12:24:17 INFO TaskSetManager: Finished task 22.0 in stage 1.0 (TID 98) in 215 ms on 172.21.12.47 (executor 0) (14/76)
23/05/31 12:24:17 INFO TaskSetManager: Starting task 30.0 in stage 1.0 (TID 106) (172.21.12.47, executor 0, partition 30, ANY, 7472 bytes) 
23/05/31 12:24:17 INFO TaskSetManager: Finished task 24.0 in stage 1.0 (TID 100) in 212 ms on 172.21.12.47 (executor 0) (15/76)
23/05/31 12:24:17 INFO TaskSetManager: Starting task 31.0 in stage 1.0 (TID 107) (172.21.12.48, executor 1, partition 31, ANY, 7472 bytes) 
23/05/31 12:24:17 INFO TaskSetManager: Starting task 32.0 in stage 1.0 (TID 108) (172.21.12.47, executor 0, partition 32, ANY, 7472 bytes) 
23/05/31 12:24:17 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 78) in 613 ms on 172.21.12.48 (executor 1) (16/76)
23/05/31 12:24:17 INFO TaskSetManager: Finished task 15.0 in stage 1.0 (TID 91) in 609 ms on 172.21.12.47 (executor 0) (17/76)
23/05/31 12:24:17 INFO TaskSetManager: Starting task 33.0 in stage 1.0 (TID 109) (172.21.12.48, executor 1, partition 33, ANY, 7472 bytes) 
23/05/31 12:24:17 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 76) in 626 ms on 172.21.12.48 (executor 1) (18/76)
23/05/31 12:24:17 INFO TaskSetManager: Starting task 34.0 in stage 1.0 (TID 110) (172.21.12.47, executor 0, partition 34, ANY, 7472 bytes) 
23/05/31 12:24:17 INFO TaskSetManager: Finished task 11.0 in stage 1.0 (TID 87) in 686 ms on 172.21.12.47 (executor 0) (19/76)
23/05/31 12:24:17 INFO TaskSetManager: Starting task 35.0 in stage 1.0 (TID 111) (172.21.12.47, executor 0, partition 35, ANY, 7472 bytes) 
23/05/31 12:24:17 INFO TaskSetManager: Finished task 13.0 in stage 1.0 (TID 89) in 694 ms on 172.21.12.47 (executor 0) (20/76)
23/05/31 12:24:17 INFO TaskSetManager: Starting task 36.0 in stage 1.0 (TID 112) (172.21.12.47, executor 0, partition 36, ANY, 7472 bytes) 
23/05/31 12:24:17 INFO TaskSetManager: Finished task 9.0 in stage 1.0 (TID 85) in 705 ms on 172.21.12.47 (executor 0) (21/76)
23/05/31 12:24:17 INFO TaskSetManager: Starting task 37.0 in stage 1.0 (TID 113) (172.21.12.48, executor 1, partition 37, ANY, 7472 bytes) 
23/05/31 12:24:17 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 82) in 793 ms on 172.21.12.48 (executor 1) (22/76)
23/05/31 12:24:17 INFO TaskSetManager: Starting task 38.0 in stage 1.0 (TID 114) (172.21.12.48, executor 1, partition 38, ANY, 7472 bytes) 
23/05/31 12:24:17 INFO TaskSetManager: Finished task 29.0 in stage 1.0 (TID 105) in 264 ms on 172.21.12.48 (executor 1) (23/76)
23/05/31 12:24:17 INFO TaskSetManager: Starting task 39.0 in stage 1.0 (TID 115) (172.21.12.47, executor 0, partition 39, ANY, 7472 bytes) 
23/05/31 12:24:17 INFO TaskSetManager: Finished task 32.0 in stage 1.0 (TID 108) in 218 ms on 172.21.12.47 (executor 0) (24/76)
23/05/31 12:24:17 INFO TaskSetManager: Starting task 40.0 in stage 1.0 (TID 116) (172.21.12.47, executor 0, partition 40, ANY, 7472 bytes) 
23/05/31 12:24:17 INFO TaskSetManager: Finished task 25.0 in stage 1.0 (TID 101) in 325 ms on 172.21.12.47 (executor 0) (25/76)
23/05/31 12:24:17 INFO TaskSetManager: Starting task 41.0 in stage 1.0 (TID 117) (172.21.12.47, executor 0, partition 41, ANY, 7472 bytes) 
23/05/31 12:24:17 INFO TaskSetManager: Finished task 26.0 in stage 1.0 (TID 102) in 306 ms on 172.21.12.47 (executor 0) (26/76)
23/05/31 12:24:17 INFO TaskSetManager: Starting task 42.0 in stage 1.0 (TID 118) (172.21.12.48, executor 1, partition 42, ANY, 7472 bytes) 
23/05/31 12:24:17 INFO TaskSetManager: Finished task 31.0 in stage 1.0 (TID 107) in 267 ms on 172.21.12.48 (executor 1) (27/76)
23/05/31 12:24:17 INFO TaskSetManager: Starting task 43.0 in stage 1.0 (TID 119) (172.21.12.47, executor 0, partition 43, ANY, 7472 bytes) 
23/05/31 12:24:17 INFO TaskSetManager: Finished task 28.0 in stage 1.0 (TID 104) in 341 ms on 172.21.12.47 (executor 0) (28/76)
23/05/31 12:24:17 INFO TaskSetManager: Starting task 44.0 in stage 1.0 (TID 120) (172.21.12.47, executor 0, partition 44, ANY, 7472 bytes) 
23/05/31 12:24:17 INFO TaskSetManager: Starting task 45.0 in stage 1.0 (TID 121) (172.21.12.47, executor 0, partition 45, ANY, 7472 bytes) 
23/05/31 12:24:17 INFO TaskSetManager: Finished task 35.0 in stage 1.0 (TID 111) in 216 ms on 172.21.12.47 (executor 0) (29/76)
23/05/31 12:24:17 INFO TaskSetManager: Finished task 30.0 in stage 1.0 (TID 106) in 317 ms on 172.21.12.47 (executor 0) (30/76)
23/05/31 12:24:17 INFO TaskSetManager: Starting task 46.0 in stage 1.0 (TID 122) (172.21.12.47, executor 0, partition 46, ANY, 7472 bytes) 
23/05/31 12:24:17 INFO TaskSetManager: Finished task 36.0 in stage 1.0 (TID 112) in 212 ms on 172.21.12.47 (executor 0) (31/76)
23/05/31 12:24:17 INFO TaskSetManager: Starting task 47.0 in stage 1.0 (TID 123) (172.21.12.47, executor 0, partition 47, ANY, 7472 bytes) 
23/05/31 12:24:17 INFO TaskSetManager: Finished task 34.0 in stage 1.0 (TID 110) in 257 ms on 172.21.12.47 (executor 0) (32/76)
23/05/31 12:24:18 INFO TaskSetManager: Starting task 48.0 in stage 1.0 (TID 124) (172.21.12.48, executor 1, partition 48, ANY, 7472 bytes) 
23/05/31 12:24:18 INFO TaskSetManager: Finished task 37.0 in stage 1.0 (TID 113) in 243 ms on 172.21.12.48 (executor 1) (33/76)
23/05/31 12:24:18 INFO TaskSetManager: Starting task 49.0 in stage 1.0 (TID 125) (172.21.12.47, executor 0, partition 49, ANY, 7472 bytes) 
23/05/31 12:24:18 INFO TaskSetManager: Finished task 41.0 in stage 1.0 (TID 117) in 243 ms on 172.21.12.47 (executor 0) (34/76)
23/05/31 12:24:18 INFO TaskSetManager: Starting task 50.0 in stage 1.0 (TID 126) (172.21.12.47, executor 0, partition 50, ANY, 7472 bytes) 
23/05/31 12:24:18 INFO TaskSetManager: Finished task 39.0 in stage 1.0 (TID 115) in 264 ms on 172.21.12.47 (executor 0) (35/76)
23/05/31 12:24:18 INFO TaskSetManager: Starting task 51.0 in stage 1.0 (TID 127) (172.21.12.47, executor 0, partition 51, ANY, 7472 bytes) 
23/05/31 12:24:18 INFO TaskSetManager: Starting task 52.0 in stage 1.0 (TID 128) (172.21.12.47, executor 0, partition 52, ANY, 7472 bytes) 
23/05/31 12:24:18 INFO TaskSetManager: Starting task 53.0 in stage 1.0 (TID 129) (172.21.12.48, executor 1, partition 53, ANY, 7472 bytes) 
23/05/31 12:24:18 INFO TaskSetManager: Finished task 40.0 in stage 1.0 (TID 116) in 281 ms on 172.21.12.47 (executor 0) (36/76)
23/05/31 12:24:18 INFO TaskSetManager: Finished task 45.0 in stage 1.0 (TID 121) in 211 ms on 172.21.12.47 (executor 0) (37/76)
23/05/31 12:24:18 INFO TaskSetManager: Finished task 42.0 in stage 1.0 (TID 118) in 247 ms on 172.21.12.48 (executor 1) (38/76)
23/05/31 12:24:18 INFO TaskSetManager: Starting task 54.0 in stage 1.0 (TID 130) (172.21.12.47, executor 0, partition 54, ANY, 7472 bytes) 
23/05/31 12:24:18 INFO TaskSetManager: Finished task 44.0 in stage 1.0 (TID 120) in 301 ms on 172.21.12.47 (executor 0) (39/76)
23/05/31 12:24:18 INFO TaskSetManager: Starting task 55.0 in stage 1.0 (TID 131) (172.21.12.47, executor 0, partition 55, ANY, 7472 bytes) 
23/05/31 12:24:18 INFO TaskSetManager: Finished task 43.0 in stage 1.0 (TID 119) in 328 ms on 172.21.12.47 (executor 0) (40/76)
23/05/31 12:24:18 INFO TaskSetManager: Starting task 56.0 in stage 1.0 (TID 132) (172.21.12.48, executor 1, partition 56, ANY, 7472 bytes) 
23/05/31 12:24:18 INFO TaskSetManager: Finished task 48.0 in stage 1.0 (TID 124) in 254 ms on 172.21.12.48 (executor 1) (41/76)
23/05/31 12:24:18 INFO TaskSetManager: Starting task 57.0 in stage 1.0 (TID 133) (172.21.12.47, executor 0, partition 57, ANY, 7472 bytes) 
23/05/31 12:24:18 INFO TaskSetManager: Finished task 46.0 in stage 1.0 (TID 122) in 375 ms on 172.21.12.47 (executor 0) (42/76)
23/05/31 12:24:18 INFO TaskSetManager: Starting task 58.0 in stage 1.0 (TID 134) (172.21.12.47, executor 0, partition 58, ANY, 7472 bytes) 
23/05/31 12:24:18 INFO TaskSetManager: Starting task 59.0 in stage 1.0 (TID 135) (172.21.12.47, executor 0, partition 59, ANY, 7472 bytes) 
23/05/31 12:24:18 INFO TaskSetManager: Finished task 51.0 in stage 1.0 (TID 127) in 214 ms on 172.21.12.47 (executor 0) (43/76)
23/05/31 12:24:18 INFO TaskSetManager: Finished task 47.0 in stage 1.0 (TID 123) in 389 ms on 172.21.12.47 (executor 0) (44/76)
23/05/31 12:24:18 INFO TaskSetManager: Starting task 60.0 in stage 1.0 (TID 136) (172.21.12.47, executor 0, partition 60, ANY, 7472 bytes) 
23/05/31 12:24:18 INFO TaskSetManager: Starting task 61.0 in stage 1.0 (TID 137) (172.21.12.47, executor 0, partition 61, ANY, 7472 bytes) 
23/05/31 12:24:18 INFO TaskSetManager: Finished task 52.0 in stage 1.0 (TID 128) in 233 ms on 172.21.12.47 (executor 0) (45/76)
23/05/31 12:24:18 INFO TaskSetManager: Finished task 50.0 in stage 1.0 (TID 126) in 264 ms on 172.21.12.47 (executor 0) (46/76)
23/05/31 12:24:18 INFO TaskSetManager: Starting task 62.0 in stage 1.0 (TID 138) (172.21.12.47, executor 0, partition 62, ANY, 7472 bytes) 
23/05/31 12:24:18 INFO TaskSetManager: Finished task 49.0 in stage 1.0 (TID 125) in 276 ms on 172.21.12.47 (executor 0) (47/76)
23/05/31 12:24:18 INFO TaskSetManager: Starting task 63.0 in stage 1.0 (TID 139) (172.21.12.47, executor 0, partition 63, ANY, 7472 bytes) 
23/05/31 12:24:18 INFO TaskSetManager: Finished task 54.0 in stage 1.0 (TID 130) in 213 ms on 172.21.12.47 (executor 0) (48/76)
23/05/31 12:24:18 INFO TaskSetManager: Starting task 64.0 in stage 1.0 (TID 140) (172.21.12.47, executor 0, partition 64, ANY, 7472 bytes) 
23/05/31 12:24:18 INFO TaskSetManager: Finished task 57.0 in stage 1.0 (TID 133) in 214 ms on 172.21.12.47 (executor 0) (49/76)
23/05/31 12:24:18 INFO TaskSetManager: Starting task 65.0 in stage 1.0 (TID 141) (172.21.12.47, executor 0, partition 65, ANY, 7472 bytes) 
23/05/31 12:24:18 INFO TaskSetManager: Finished task 55.0 in stage 1.0 (TID 131) in 311 ms on 172.21.12.47 (executor 0) (50/76)
23/05/31 12:24:18 INFO TaskSetManager: Starting task 66.0 in stage 1.0 (TID 142) (172.21.12.47, executor 0, partition 66, ANY, 7472 bytes) 
23/05/31 12:24:18 INFO TaskSetManager: Finished task 60.0 in stage 1.0 (TID 136) in 211 ms on 172.21.12.47 (executor 0) (51/76)
23/05/31 12:24:18 INFO TaskSetManager: Starting task 67.0 in stage 1.0 (TID 143) (172.21.12.47, executor 0, partition 67, ANY, 7472 bytes) 
23/05/31 12:24:18 INFO TaskSetManager: Starting task 68.0 in stage 1.0 (TID 144) (172.21.12.47, executor 0, partition 68, ANY, 7472 bytes) 
23/05/31 12:24:18 INFO TaskSetManager: Finished task 59.0 in stage 1.0 (TID 135) in 267 ms on 172.21.12.47 (executor 0) (52/76)
23/05/31 12:24:18 INFO TaskSetManager: Finished task 61.0 in stage 1.0 (TID 137) in 247 ms on 172.21.12.47 (executor 0) (53/76)
23/05/31 12:24:18 INFO TaskSetManager: Starting task 69.0 in stage 1.0 (TID 145) (172.21.12.47, executor 0, partition 69, ANY, 7472 bytes) 
23/05/31 12:24:18 INFO TaskSetManager: Starting task 70.0 in stage 1.0 (TID 146) (172.21.12.47, executor 0, partition 70, ANY, 7472 bytes) 
23/05/31 12:24:18 INFO TaskSetManager: Finished task 58.0 in stage 1.0 (TID 134) in 282 ms on 172.21.12.47 (executor 0) (54/76)
23/05/31 12:24:18 INFO TaskSetManager: Finished task 62.0 in stage 1.0 (TID 138) in 256 ms on 172.21.12.47 (executor 0) (55/76)
23/05/31 12:24:18 INFO TaskSetManager: Starting task 71.0 in stage 1.0 (TID 147) (172.21.12.47, executor 0, partition 71, ANY, 7472 bytes) 
23/05/31 12:24:18 INFO TaskSetManager: Finished task 63.0 in stage 1.0 (TID 139) in 213 ms on 172.21.12.47 (executor 0) (56/76)
23/05/31 12:24:18 INFO TaskSetManager: Starting task 72.0 in stage 1.0 (TID 148) (172.21.12.48, executor 1, partition 72, ANY, 7472 bytes) 
23/05/31 12:24:18 INFO TaskSetManager: Finished task 23.0 in stage 1.0 (TID 99) in 1353 ms on 172.21.12.48 (executor 1) (57/76)
23/05/31 12:24:18 INFO TaskSetManager: Starting task 73.0 in stage 1.0 (TID 149) (172.21.12.47, executor 0, partition 73, ANY, 7472 bytes) 
23/05/31 12:24:18 INFO TaskSetManager: Finished task 64.0 in stage 1.0 (TID 140) in 213 ms on 172.21.12.47 (executor 0) (58/76)
23/05/31 12:24:18 INFO TaskSetManager: Starting task 74.0 in stage 1.0 (TID 150) (172.21.12.47, executor 0, partition 74, ANY, 7472 bytes) 
23/05/31 12:24:18 INFO TaskSetManager: Finished task 65.0 in stage 1.0 (TID 141) in 231 ms on 172.21.12.47 (executor 0) (59/76)
23/05/31 12:24:18 INFO TaskSetManager: Starting task 75.0 in stage 1.0 (TID 151) (172.21.12.47, executor 0, partition 75, ANY, 7472 bytes) 
23/05/31 12:24:18 INFO TaskSetManager: Finished task 70.0 in stage 1.0 (TID 146) in 231 ms on 172.21.12.47 (executor 0) (60/76)
23/05/31 12:24:18 INFO TaskSetManager: Finished task 19.0 in stage 1.0 (TID 95) in 1531 ms on 172.21.12.48 (executor 1) (61/76)
23/05/31 12:24:18 INFO TaskSetManager: Finished task 20.0 in stage 1.0 (TID 96) in 1542 ms on 172.21.12.48 (executor 1) (62/76)
23/05/31 12:24:18 INFO TaskSetManager: Finished task 33.0 in stage 1.0 (TID 109) in 1234 ms on 172.21.12.48 (executor 1) (63/76)
23/05/31 12:24:18 INFO TaskSetManager: Finished task 53.0 in stage 1.0 (TID 129) in 738 ms on 172.21.12.48 (executor 1) (64/76)
23/05/31 12:24:18 INFO TaskSetManager: Finished task 67.0 in stage 1.0 (TID 143) in 307 ms on 172.21.12.47 (executor 0) (65/76)
23/05/31 12:24:18 INFO TaskSetManager: Finished task 68.0 in stage 1.0 (TID 144) in 332 ms on 172.21.12.47 (executor 0) (66/76)
23/05/31 12:24:18 INFO TaskSetManager: Finished task 71.0 in stage 1.0 (TID 147) in 320 ms on 172.21.12.47 (executor 0) (67/76)
23/05/31 12:24:18 INFO TaskSetManager: Finished task 66.0 in stage 1.0 (TID 142) in 390 ms on 172.21.12.47 (executor 0) (68/76)
23/05/31 12:24:18 INFO TaskSetManager: Finished task 72.0 in stage 1.0 (TID 148) in 251 ms on 172.21.12.48 (executor 1) (69/76)
23/05/31 12:24:18 INFO TaskSetManager: Finished task 74.0 in stage 1.0 (TID 150) in 242 ms on 172.21.12.47 (executor 0) (70/76)
23/05/31 12:24:19 INFO TaskSetManager: Finished task 27.0 in stage 1.0 (TID 103) in 1470 ms on 172.21.12.48 (executor 1) (71/76)
23/05/31 12:24:19 INFO TaskSetManager: Finished task 73.0 in stage 1.0 (TID 149) in 315 ms on 172.21.12.47 (executor 0) (72/76)
23/05/31 12:24:19 INFO TaskSetManager: Finished task 69.0 in stage 1.0 (TID 145) in 457 ms on 172.21.12.47 (executor 0) (73/76)
23/05/31 12:24:19 INFO TaskSetManager: Finished task 38.0 in stage 1.0 (TID 114) in 1300 ms on 172.21.12.48 (executor 1) (74/76)
23/05/31 12:24:19 INFO TaskSetManager: Finished task 75.0 in stage 1.0 (TID 151) in 321 ms on 172.21.12.47 (executor 0) (75/76)
23/05/31 12:24:19 INFO TaskSetManager: Finished task 56.0 in stage 1.0 (TID 132) in 925 ms on 172.21.12.48 (executor 1) (76/76)
23/05/31 12:24:19 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
23/05/31 12:24:19 INFO DAGScheduler: ResultStage 1 (count at TeraValidate.scala:101) finished in 2.223 s
23/05/31 12:24:19 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
23/05/31 12:24:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
23/05/31 12:24:19 INFO DAGScheduler: Job 1 finished: count at TeraValidate.scala:101, took 2.231103 s
num records: 100000000
checksum: 2fae1609c128bf8
part 0
lastMaxArrayBuffer(0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
min ArrayBuffer(0, 0, 0, 167, 13, 42, 168, 2, 218, 218)
max ArrayBuffer(3, 94, 80, 204, 1, 10, 31, 132, 148, 209)
part 1
lastMaxArrayBuffer(3, 94, 80, 204, 1, 10, 31, 132, 148, 209)
min ArrayBuffer(3, 94, 80, 236, 37, 35, 113, 48, 90, 10)
max ArrayBuffer(6, 188, 161, 139, 171, 182, 156, 132, 57, 91)
part 2
lastMaxArrayBuffer(6, 188, 161, 139, 171, 182, 156, 132, 57, 91)
min ArrayBuffer(6, 188, 161, 210, 202, 43, 140, 58, 251, 31)
max ArrayBuffer(10, 26, 242, 125, 58, 225, 136, 74, 17, 30)
part 3
lastMaxArrayBuffer(10, 26, 242, 125, 58, 225, 136, 74, 17, 30)
min ArrayBuffer(10, 26, 242, 212, 155, 221, 5, 77, 249, 128)
max ArrayBuffer(13, 121, 66, 251, 36, 178, 18, 115, 126, 19)
part 4
lastMaxArrayBuffer(13, 121, 66, 251, 36, 178, 18, 115, 126, 19)
min ArrayBuffer(13, 121, 67, 132, 255, 95, 197, 229, 102, 152)
max ArrayBuffer(16, 215, 147, 239, 127, 108, 30, 233, 15, 191)
part 5
lastMaxArrayBuffer(16, 215, 147, 239, 127, 108, 30, 233, 15, 191)
min ArrayBuffer(16, 215, 148, 59, 167, 143, 44, 230, 52, 151)
max ArrayBuffer(20, 53, 229, 7, 221, 242, 231, 113, 140, 245)
part 6
lastMaxArrayBuffer(20, 53, 229, 7, 221, 242, 231, 113, 140, 245)
min ArrayBuffer(20, 53, 229, 13, 227, 81, 208, 79, 226, 113)
max ArrayBuffer(23, 148, 53, 110, 64, 245, 57, 205, 84, 232)
part 7
lastMaxArrayBuffer(23, 148, 53, 110, 64, 245, 57, 205, 84, 232)
min ArrayBuffer(23, 148, 54, 72, 40, 166, 144, 66, 147, 157)
max ArrayBuffer(26, 242, 134, 169, 54, 161, 158, 197, 64, 221)
part 8
lastMaxArrayBuffer(26, 242, 134, 169, 54, 161, 158, 197, 64, 221)
min ArrayBuffer(26, 242, 134, 205, 127, 120, 97, 159, 107, 229)
max ArrayBuffer(30, 80, 215, 146, 136, 48, 204, 213, 193, 123)
part 9
lastMaxArrayBuffer(30, 80, 215, 146, 136, 48, 204, 213, 193, 123)
min ArrayBuffer(30, 80, 215, 151, 202, 243, 243, 57, 21, 223)
max ArrayBuffer(33, 175, 40, 44, 154, 187, 167, 97, 242, 165)
part 10
lastMaxArrayBuffer(33, 175, 40, 44, 154, 187, 167, 97, 242, 165)
min ArrayBuffer(33, 175, 40, 141, 176, 10, 224, 193, 251, 39)
max ArrayBuffer(37, 13, 121, 65, 71, 99, 57, 28, 27, 31)
part 11
lastMaxArrayBuffer(37, 13, 121, 65, 71, 99, 57, 28, 27, 31)
min ArrayBuffer(37, 13, 121, 162, 31, 255, 47, 210, 164, 164)
max ArrayBuffer(40, 107, 202, 21, 252, 80, 177, 70, 125, 30)
part 12
lastMaxArrayBuffer(40, 107, 202, 21, 252, 80, 177, 70, 125, 30)
min ArrayBuffer(40, 107, 202, 56, 140, 241, 55, 112, 141, 100)
max ArrayBuffer(43, 202, 26, 235, 105, 18, 80, 181, 73, 149)
part 13
lastMaxArrayBuffer(43, 202, 26, 235, 105, 18, 80, 181, 73, 149)
min ArrayBuffer(43, 202, 27, 4, 165, 167, 243, 22, 100, 11)
max ArrayBuffer(47, 40, 107, 175, 78, 112, 219, 28, 22, 186)
part 14
lastMaxArrayBuffer(47, 40, 107, 175, 78, 112, 219, 28, 22, 186)
min ArrayBuffer(47, 40, 107, 209, 79, 179, 141, 162, 119, 218)
max ArrayBuffer(50, 134, 188, 94, 10, 156, 196, 118, 177, 133)
part 15
lastMaxArrayBuffer(50, 134, 188, 94, 10, 156, 196, 118, 177, 133)
min ArrayBuffer(50, 134, 188, 192, 80, 194, 146, 178, 86, 118)
max ArrayBuffer(53, 229, 13, 113, 6, 75, 218, 85, 221, 220)
part 16
lastMaxArrayBuffer(53, 229, 13, 113, 6, 75, 218, 85, 221, 220)
min ArrayBuffer(53, 229, 13, 140, 182, 111, 245, 192, 141, 242)
max ArrayBuffer(57, 67, 94, 31, 193, 222, 197, 16, 195, 202)
part 17
lastMaxArrayBuffer(57, 67, 94, 31, 193, 222, 197, 16, 195, 202)
min ArrayBuffer(57, 67, 94, 83, 254, 62, 46, 33, 235, 176)
max ArrayBuffer(60, 161, 175, 26, 196, 239, 215, 179, 182, 28)
part 18
lastMaxArrayBuffer(60, 161, 175, 26, 196, 239, 215, 179, 182, 28)
min ArrayBuffer(60, 161, 175, 194, 161, 17, 64, 211, 191, 101)
max ArrayBuffer(63, 255, 255, 240, 170, 87, 141, 74, 74, 215)
part 19
lastMaxArrayBuffer(63, 255, 255, 240, 170, 87, 141, 74, 74, 215)
min ArrayBuffer(64, 0, 0, 34, 18, 168, 218, 166, 200, 147)
max ArrayBuffer(67, 94, 80, 215, 28, 24, 65, 207, 160, 253)
part 20
lastMaxArrayBuffer(67, 94, 80, 215, 28, 24, 65, 207, 160, 253)
min ArrayBuffer(67, 94, 80, 229, 149, 103, 102, 79, 67, 76)
max ArrayBuffer(70, 188, 161, 146, 173, 118, 48, 10, 206, 194)
part 21
lastMaxArrayBuffer(70, 188, 161, 146, 173, 118, 48, 10, 206, 194)
min ArrayBuffer(70, 188, 161, 188, 206, 57, 6, 180, 93, 86)
max ArrayBuffer(74, 26, 242, 133, 24, 50, 84, 51, 98, 70)
part 22
lastMaxArrayBuffer(74, 26, 242, 133, 24, 50, 84, 51, 98, 70)
min ArrayBuffer(74, 26, 242, 221, 97, 194, 64, 19, 39, 147)
max ArrayBuffer(77, 121, 67, 54, 114, 8, 176, 12, 217, 88)
part 23
lastMaxArrayBuffer(77, 121, 67, 54, 114, 8, 176, 12, 217, 88)
min ArrayBuffer(77, 121, 67, 96, 150, 51, 194, 239, 205, 96)
max ArrayBuffer(80, 215, 148, 24, 137, 22, 208, 201, 162, 87)
part 24
lastMaxArrayBuffer(80, 215, 148, 24, 137, 22, 208, 201, 162, 87)
min ArrayBuffer(80, 215, 148, 55, 190, 61, 23, 137, 237, 7)
max ArrayBuffer(84, 53, 228, 201, 0, 55, 26, 233, 153, 227)
part 25
lastMaxArrayBuffer(84, 53, 228, 201, 0, 55, 26, 233, 153, 227)
min ArrayBuffer(84, 53, 229, 72, 65, 145, 206, 238, 181, 17)
max ArrayBuffer(87, 148, 53, 207, 239, 97, 193, 238, 41, 118)
part 26
lastMaxArrayBuffer(87, 148, 53, 207, 239, 97, 193, 238, 41, 118)
min ArrayBuffer(87, 148, 53, 253, 134, 167, 249, 47, 38, 10)
max ArrayBuffer(90, 242, 134, 155, 46, 2, 83, 134, 119, 255)
part 27
lastMaxArrayBuffer(90, 242, 134, 155, 46, 2, 83, 134, 119, 255)
min ArrayBuffer(90, 242, 134, 207, 78, 243, 49, 6, 160, 56)
max ArrayBuffer(94, 80, 215, 145, 209, 95, 126, 27, 234, 155)
part 28
lastMaxArrayBuffer(94, 80, 215, 145, 209, 95, 126, 27, 234, 155)
min ArrayBuffer(94, 80, 215, 177, 235, 104, 98, 123, 119, 54)
max ArrayBuffer(97, 175, 40, 89, 173, 169, 18, 237, 125, 174)
part 29
lastMaxArrayBuffer(97, 175, 40, 89, 173, 169, 18, 237, 125, 174)
min ArrayBuffer(97, 175, 40, 151, 4, 235, 181, 3, 186, 70)
max ArrayBuffer(101, 13, 121, 15, 72, 225, 187, 126, 241, 121)
part 30
lastMaxArrayBuffer(101, 13, 121, 15, 72, 225, 187, 126, 241, 121)
min ArrayBuffer(101, 13, 121, 129, 33, 163, 0, 100, 255, 145)
max ArrayBuffer(104, 107, 202, 13, 196, 118, 69, 46, 63, 147)
part 31
lastMaxArrayBuffer(104, 107, 202, 13, 196, 118, 69, 46, 63, 147)
min ArrayBuffer(104, 107, 202, 175, 205, 64, 150, 18, 59, 22)
max ArrayBuffer(107, 202, 26, 186, 179, 183, 47, 136, 2, 148)
part 32
lastMaxArrayBuffer(107, 202, 26, 186, 179, 183, 47, 136, 2, 148)
min ArrayBuffer(107, 202, 27, 49, 56, 72, 205, 226, 249, 66)
max ArrayBuffer(111, 40, 107, 100, 20, 225, 115, 154, 70, 31)
part 33
lastMaxArrayBuffer(111, 40, 107, 100, 20, 225, 115, 154, 70, 31)
min ArrayBuffer(111, 40, 107, 213, 191, 232, 86, 131, 121, 117)
max ArrayBuffer(114, 134, 188, 118, 147, 185, 68, 93, 72, 126)
part 34
lastMaxArrayBuffer(114, 134, 188, 118, 147, 185, 68, 93, 72, 126)
min ArrayBuffer(114, 134, 188, 182, 86, 252, 89, 181, 17, 255)
max ArrayBuffer(117, 229, 13, 116, 28, 63, 119, 176, 216, 90)
part 35
lastMaxArrayBuffer(117, 229, 13, 116, 28, 63, 119, 176, 216, 90)
min ArrayBuffer(117, 229, 13, 153, 121, 37, 5, 178, 125, 183)
max ArrayBuffer(121, 67, 94, 54, 40, 227, 246, 52, 190, 66)
part 36
lastMaxArrayBuffer(121, 67, 94, 54, 40, 227, 246, 52, 190, 66)
min ArrayBuffer(121, 67, 94, 88, 84, 240, 125, 194, 124, 128)
max ArrayBuffer(124, 161, 175, 14, 26, 165, 183, 156, 182, 230)
part 37
lastMaxArrayBuffer(124, 161, 175, 14, 26, 165, 183, 156, 182, 230)
min ArrayBuffer(124, 161, 175, 81, 0, 221, 0, 26, 32, 107)
max ArrayBuffer(127, 255, 255, 241, 83, 14, 250, 250, 176, 149)
part 38
lastMaxArrayBuffer(127, 255, 255, 241, 83, 14, 250, 250, 176, 149)
min ArrayBuffer(128, 0, 0, 82, 40, 182, 196, 219, 240, 175)
max ArrayBuffer(131, 94, 80, 166, 238, 233, 4, 176, 178, 92)
part 39
lastMaxArrayBuffer(131, 94, 80, 166, 238, 233, 4, 176, 178, 92)
min ArrayBuffer(131, 94, 80, 221, 43, 133, 20, 14, 100, 87)
max ArrayBuffer(134, 188, 161, 107, 106, 138, 67, 31, 241, 186)
part 40
lastMaxArrayBuffer(134, 188, 161, 107, 106, 138, 67, 31, 241, 186)
min ArrayBuffer(134, 188, 162, 17, 112, 24, 118, 14, 220, 78)
max ArrayBuffer(138, 26, 242, 74, 123, 88, 18, 201, 224, 23)
part 41
lastMaxArrayBuffer(138, 26, 242, 74, 123, 88, 18, 201, 224, 23)
min ArrayBuffer(138, 26, 242, 203, 219, 174, 151, 18, 213, 156)
max ArrayBuffer(141, 121, 67, 71, 35, 143, 135, 125, 45, 67)
part 42
lastMaxArrayBuffer(141, 121, 67, 71, 35, 143, 135, 125, 45, 67)
min ArrayBuffer(141, 121, 67, 109, 3, 216, 75, 18, 46, 32)
max ArrayBuffer(144, 215, 147, 227, 132, 33, 60, 165, 96, 174)
part 43
lastMaxArrayBuffer(144, 215, 147, 227, 132, 33, 60, 165, 96, 174)
min ArrayBuffer(144, 215, 148, 129, 125, 68, 198, 84, 88, 221)
max ArrayBuffer(148, 53, 228, 249, 187, 178, 171, 187, 129, 66)
part 44
lastMaxArrayBuffer(148, 53, 228, 249, 187, 178, 171, 187, 129, 66)
min ArrayBuffer(148, 53, 229, 48, 213, 239, 83, 220, 240, 141)
max ArrayBuffer(151, 148, 53, 205, 81, 181, 112, 207, 209, 198)
part 45
lastMaxArrayBuffer(151, 148, 53, 205, 81, 181, 112, 207, 209, 198)
min ArrayBuffer(151, 148, 53, 255, 63, 162, 21, 42, 86, 42)
max ArrayBuffer(154, 242, 134, 175, 129, 244, 114, 177, 148, 155)
part 46
lastMaxArrayBuffer(154, 242, 134, 175, 129, 244, 114, 177, 148, 155)
min ArrayBuffer(154, 242, 135, 94, 237, 122, 141, 180, 229, 152)
max ArrayBuffer(158, 80, 215, 138, 196, 224, 28, 216, 123, 108)
part 47
lastMaxArrayBuffer(158, 80, 215, 138, 196, 224, 28, 216, 123, 108)
min ArrayBuffer(158, 80, 215, 191, 46, 100, 114, 208, 164, 240)
max ArrayBuffer(161, 175, 39, 225, 38, 207, 60, 92, 236, 51)
part 48
lastMaxArrayBuffer(161, 175, 39, 225, 38, 207, 60, 92, 236, 51)
min ArrayBuffer(161, 175, 40, 146, 25, 161, 87, 10, 69, 235)
max ArrayBuffer(165, 13, 121, 59, 130, 174, 90, 138, 171, 80)
part 49
lastMaxArrayBuffer(165, 13, 121, 59, 130, 174, 90, 138, 171, 80)
min ArrayBuffer(165, 13, 121, 83, 231, 251, 72, 241, 97, 239)
max ArrayBuffer(168, 107, 202, 26, 64, 80, 220, 114, 30, 39)
part 50
lastMaxArrayBuffer(168, 107, 202, 26, 64, 80, 220, 114, 30, 39)
min ArrayBuffer(168, 107, 202, 87, 242, 127, 215, 106, 236, 162)
max ArrayBuffer(171, 202, 26, 158, 216, 20, 90, 7, 38, 125)
part 51
lastMaxArrayBuffer(171, 202, 26, 158, 216, 20, 90, 7, 38, 125)
min ArrayBuffer(171, 202, 27, 8, 69, 14, 32, 31, 146, 194)
max ArrayBuffer(175, 40, 107, 135, 21, 30, 119, 31, 80, 71)
part 52
lastMaxArrayBuffer(175, 40, 107, 135, 21, 30, 119, 31, 80, 71)
min ArrayBuffer(175, 40, 107, 203, 251, 139, 91, 171, 186, 37)
max ArrayBuffer(178, 134, 188, 60, 91, 181, 9, 46, 14, 248)
part 53
lastMaxArrayBuffer(178, 134, 188, 60, 91, 181, 9, 46, 14, 248)
min ArrayBuffer(178, 134, 188, 167, 121, 199, 112, 13, 74, 233)
max ArrayBuffer(181, 229, 13, 111, 19, 209, 191, 250, 145, 164)
part 54
lastMaxArrayBuffer(181, 229, 13, 111, 19, 209, 191, 250, 145, 164)
min ArrayBuffer(181, 229, 13, 127, 72, 191, 155, 54, 219, 63)
max ArrayBuffer(185, 67, 94, 32, 10, 34, 160, 98, 127, 88)
part 55
lastMaxArrayBuffer(185, 67, 94, 32, 10, 34, 160, 98, 127, 88)
min ArrayBuffer(185, 67, 94, 96, 183, 184, 56, 141, 163, 105)
max ArrayBuffer(188, 161, 175, 39, 61, 187, 87, 90, 220, 196)
part 56
lastMaxArrayBuffer(188, 161, 175, 39, 61, 187, 87, 90, 220, 196)
min ArrayBuffer(188, 161, 175, 47, 201, 193, 137, 141, 173, 83)
max ArrayBuffer(191, 255, 255, 254, 38, 63, 244, 149, 95, 209)
part 57
lastMaxArrayBuffer(191, 255, 255, 254, 38, 63, 244, 149, 95, 209)
min ArrayBuffer(192, 0, 0, 27, 171, 184, 247, 97, 160, 60)
max ArrayBuffer(195, 94, 80, 184, 140, 227, 83, 231, 143, 66)
part 58
lastMaxArrayBuffer(195, 94, 80, 184, 140, 227, 83, 231, 143, 66)
min ArrayBuffer(195, 94, 80, 240, 214, 136, 72, 152, 111, 181)
max ArrayBuffer(198, 188, 161, 173, 203, 134, 58, 153, 179, 22)
part 59
lastMaxArrayBuffer(198, 188, 161, 173, 203, 134, 58, 153, 179, 22)
min ArrayBuffer(198, 188, 161, 183, 217, 95, 73, 52, 62, 41)
max ArrayBuffer(202, 26, 242, 116, 7, 26, 253, 199, 89, 3)
part 60
lastMaxArrayBuffer(202, 26, 242, 116, 7, 26, 253, 199, 89, 3)
min ArrayBuffer(202, 26, 242, 195, 196, 43, 105, 186, 219, 51)
max ArrayBuffer(205, 121, 67, 78, 237, 72, 97, 4, 79, 121)
part 61
lastMaxArrayBuffer(205, 121, 67, 78, 237, 72, 97, 4, 79, 121)
min ArrayBuffer(205, 121, 67, 98, 0, 176, 104, 98, 3, 78)
max ArrayBuffer(208, 215, 148, 27, 197, 180, 90, 226, 155, 5)
part 62
lastMaxArrayBuffer(208, 215, 148, 27, 197, 180, 90, 226, 155, 5)
min ArrayBuffer(208, 215, 148, 107, 99, 58, 162, 112, 168, 165)
max ArrayBuffer(212, 53, 228, 232, 137, 53, 148, 212, 5, 229)
part 63
lastMaxArrayBuffer(212, 53, 228, 232, 137, 53, 148, 212, 5, 229)
min ArrayBuffer(212, 53, 229, 20, 126, 50, 196, 106, 27, 161)
max ArrayBuffer(215, 148, 53, 173, 101, 253, 26, 46, 152, 214)
part 64
lastMaxArrayBuffer(215, 148, 53, 173, 101, 253, 26, 46, 152, 214)
min ArrayBuffer(215, 148, 54, 8, 245, 107, 40, 158, 141, 156)
max ArrayBuffer(218, 242, 134, 163, 205, 53, 68, 54, 154, 48)
part 65
lastMaxArrayBuffer(218, 242, 134, 163, 205, 53, 68, 54, 154, 48)
min ArrayBuffer(218, 242, 134, 242, 52, 103, 177, 250, 6, 79)
max ArrayBuffer(222, 80, 215, 77, 141, 62, 199, 80, 255, 69)
part 66
lastMaxArrayBuffer(222, 80, 215, 77, 141, 62, 199, 80, 255, 69)
min ArrayBuffer(222, 80, 215, 163, 162, 141, 233, 224, 136, 155)
max ArrayBuffer(225, 175, 40, 6, 177, 196, 5, 60, 153, 0)
part 67
lastMaxArrayBuffer(225, 175, 40, 6, 177, 196, 5, 60, 153, 0)
min ArrayBuffer(225, 175, 40, 140, 87, 243, 247, 240, 188, 150)
max ArrayBuffer(229, 13, 121, 45, 93, 209, 192, 168, 57, 90)
part 68
lastMaxArrayBuffer(229, 13, 121, 45, 93, 209, 192, 168, 57, 90)
min ArrayBuffer(229, 13, 121, 83, 144, 16, 188, 223, 225, 84)
max ArrayBuffer(232, 107, 202, 5, 147, 124, 163, 75, 120, 204)
part 69
lastMaxArrayBuffer(232, 107, 202, 5, 147, 124, 163, 75, 120, 204)
min ArrayBuffer(232, 107, 202, 61, 9, 227, 51, 10, 64, 56)
max ArrayBuffer(235, 202, 26, 233, 219, 28, 43, 199, 192, 167)
part 70
lastMaxArrayBuffer(235, 202, 26, 233, 219, 28, 43, 199, 192, 167)
min ArrayBuffer(235, 202, 27, 76, 35, 36, 190, 251, 69, 3)
max ArrayBuffer(239, 40, 107, 160, 64, 225, 161, 239, 74, 138)
part 71
lastMaxArrayBuffer(239, 40, 107, 160, 64, 225, 161, 239, 74, 138)
min ArrayBuffer(239, 40, 107, 244, 217, 147, 84, 102, 161, 226)
max ArrayBuffer(242, 134, 188, 139, 84, 140, 93, 2, 73, 112)
part 72
lastMaxArrayBuffer(242, 134, 188, 139, 84, 140, 93, 2, 73, 112)
min ArrayBuffer(242, 134, 188, 175, 188, 143, 197, 105, 253, 179)
max ArrayBuffer(245, 229, 13, 14, 14, 26, 246, 41, 101, 15)
part 73
lastMaxArrayBuffer(245, 229, 13, 14, 14, 26, 246, 41, 101, 15)
min ArrayBuffer(245, 229, 13, 131, 247, 60, 213, 116, 99, 94)
max ArrayBuffer(249, 67, 94, 55, 142, 71, 34, 87, 137, 180)
part 74
lastMaxArrayBuffer(249, 67, 94, 55, 142, 71, 34, 87, 137, 180)
min ArrayBuffer(249, 67, 94, 108, 92, 71, 232, 41, 32, 125)
max ArrayBuffer(252, 161, 175, 28, 129, 242, 235, 194, 16, 184)
part 75
lastMaxArrayBuffer(252, 161, 175, 28, 129, 242, 235, 194, 16, 184)
min ArrayBuffer(252, 161, 175, 56, 148, 58, 69, 133, 38, 212)
max ArrayBuffer(255, 255, 255, 250, 118, 44, 196, 229, 92, 232)
num records: 100000000
checksum: 2fae1609c128bf8
partitions are properly sorted
23/05/31 12:24:19 INFO SparkContext: Invoking stop() from shutdown hook
23/05/31 12:24:19 INFO SparkContext: SparkContext is stopping with exitCode 0.
23/05/31 12:24:19 INFO SparkUI: Stopped Spark web UI at http://aion-0129:4040
23/05/31 12:24:19 INFO StandaloneSchedulerBackend: Shutting down all executors
23/05/31 12:24:19 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down
23/05/31 12:24:19 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/05/31 12:24:19 INFO MemoryStore: MemoryStore cleared
23/05/31 12:24:19 INFO BlockManager: BlockManager stopped
23/05/31 12:24:19 INFO BlockManagerMaster: BlockManagerMaster stopped
23/05/31 12:24:19 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/05/31 12:24:19 INFO SparkContext: Successfully stopped SparkContext
23/05/31 12:24:19 INFO ShutdownHookManager: Shutdown hook called
23/05/31 12:24:19 INFO ShutdownHookManager: Deleting directory /tmp/spark-a23f9a80-1b4b-425e-9dcb-9bedbbbeb11b
23/05/31 12:24:19 INFO ShutdownHookManager: Deleting directory /tmp/spark-649dda3a-66e0-4dc4-9b99-36560a4bc6de
Let's check now the HDFS is working.
omarcu@aion-0129:/opt/hadoop$ bin/hdfs dfs -ls hdfs://aion-0129:9000/
Found 2 items
drwxr-xr-x   - omarcu supergroup          0 2023-05-31 12:20 hdfs://aion-0129:9000/terasort_in
drwxr-xr-x   - omarcu supergroup          0 2023-05-31 12:23 hdfs://aion-0129:9000/terasort_out
omarcu@aion-0129:/opt/hadoop$ bin/hdfs dfs -ls hdfs://aion-0129:9000/terasort_in
Found 3 items
-rw-r--r--   3 omarcu supergroup          0 2023-05-31 12:20 hdfs://aion-0129:9000/terasort_in/_SUCCESS
-rw-r--r--   3 omarcu supergroup 5000000000 2023-05-31 12:20 hdfs://aion-0129:9000/terasort_in/part-r-00000
-rw-r--r--   3 omarcu supergroup 5000000000 2023-05-31 12:20 hdfs://aion-0129:9000/terasort_in/part-r-00001
omarcu@aion-0129:/opt/hadoop$ bin/hdfs dfs -ls hdfs://aion-0129:9000/terasort_out
Found 77 items
-rw-r--r--   3 omarcu supergroup          0 2023-05-31 12:23 hdfs://aion-0129:9000/terasort_out/_SUCCESS
-rw-r--r--   3 omarcu supergroup  131596700 2023-05-31 12:22 hdfs://aion-0129:9000/terasort_out/part-r-00000
-rw-r--r--   3 omarcu supergroup  131467700 2023-05-31 12:22 hdfs://aion-0129:9000/terasort_out/part-r-00001
-rw-r--r--   3 omarcu supergroup  131808000 2023-05-31 12:22 hdfs://aion-0129:9000/terasort_out/part-r-00002
-rw-r--r--   3 omarcu supergroup  131697300 2023-05-31 12:22 hdfs://aion-0129:9000/terasort_out/part-r-00003
-rw-r--r--   3 omarcu supergroup  131473400 2023-05-31 12:22 hdfs://aion-0129:9000/terasort_out/part-r-00004
-rw-r--r--   3 omarcu supergroup  131686000 2023-05-31 12:22 hdfs://aion-0129:9000/terasort_out/part-r-00005
-rw-r--r--   3 omarcu supergroup  131591300 2023-05-31 12:22 hdfs://aion-0129:9000/terasort_out/part-r-00006
-rw-r--r--   3 omarcu supergroup  131812500 2023-05-31 12:22 hdfs://aion-0129:9000/terasort_out/part-r-00007
-rw-r--r--   3 omarcu supergroup  131443300 2023-05-31 12:22 hdfs://aion-0129:9000/terasort_out/part-r-00008
-rw-r--r--   3 omarcu supergroup  131726000 2023-05-31 12:22 hdfs://aion-0129:9000/terasort_out/part-r-00009
-rw-r--r--   3 omarcu supergroup  131570600 2023-05-31 12:22 hdfs://aion-0129:9000/terasort_out/part-r-00010
-rw-r--r--   3 omarcu supergroup  131688900 2023-05-31 12:22 hdfs://aion-0129:9000/terasort_out/part-r-00011
-rw-r--r--   3 omarcu supergroup  131499200 2023-05-31 12:22 hdfs://aion-0129:9000/terasort_out/part-r-00012
-rw-r--r--   3 omarcu supergroup  131536300 2023-05-31 12:22 hdfs://aion-0129:9000/terasort_out/part-r-00013
-rw-r--r--   3 omarcu supergroup  131717300 2023-05-31 12:22 hdfs://aion-0129:9000/terasort_out/part-r-00014
-rw-r--r--   3 omarcu supergroup  131667000 2023-05-31 12:22 hdfs://aion-0129:9000/terasort_out/part-r-00015
-rw-r--r--   3 omarcu supergroup  131635300 2023-05-31 12:22 hdfs://aion-0129:9000/terasort_out/part-r-00016
-rw-r--r--   3 omarcu supergroup  131653100 2023-05-31 12:22 hdfs://aion-0129:9000/terasort_out/part-r-00017
-rw-r--r--   3 omarcu supergroup  131860600 2023-05-31 12:22 hdfs://aion-0129:9000/terasort_out/part-r-00018
-rw-r--r--   3 omarcu supergroup  131711000 2023-05-31 12:22 hdfs://aion-0129:9000/terasort_out/part-r-00019
-rw-r--r--   3 omarcu supergroup  131628400 2023-05-31 12:22 hdfs://aion-0129:9000/terasort_out/part-r-00020
-rw-r--r--   3 omarcu supergroup  131650800 2023-05-31 12:22 hdfs://aion-0129:9000/terasort_out/part-r-00021
-rw-r--r--   3 omarcu supergroup  131585900 2023-05-31 12:22 hdfs://aion-0129:9000/terasort_out/part-r-00022
-rw-r--r--   3 omarcu supergroup  131399800 2023-05-31 12:22 hdfs://aion-0129:9000/terasort_out/part-r-00023
-rw-r--r--   3 omarcu supergroup  131587000 2023-05-31 12:22 hdfs://aion-0129:9000/terasort_out/part-r-00024
-rw-r--r--   3 omarcu supergroup  131561400 2023-05-31 12:22 hdfs://aion-0129:9000/terasort_out/part-r-00025
-rw-r--r--   3 omarcu supergroup  131573800 2023-05-31 12:22 hdfs://aion-0129:9000/terasort_out/part-r-00026
-rw-r--r--   3 omarcu supergroup  131628900 2023-05-31 12:22 hdfs://aion-0129:9000/terasort_out/part-r-00027
-rw-r--r--   3 omarcu supergroup  131564900 2023-05-31 12:22 hdfs://aion-0129:9000/terasort_out/part-r-00028
-rw-r--r--   3 omarcu supergroup  131345700 2023-05-31 12:22 hdfs://aion-0129:9000/terasort_out/part-r-00029
-rw-r--r--   3 omarcu supergroup  131642500 2023-05-31 12:22 hdfs://aion-0129:9000/terasort_out/part-r-00030
-rw-r--r--   3 omarcu supergroup  131371000 2023-05-31 12:22 hdfs://aion-0129:9000/terasort_out/part-r-00031
-rw-r--r--   3 omarcu supergroup  131401000 2023-05-31 12:22 hdfs://aion-0129:9000/terasort_out/part-r-00032
-rw-r--r--   3 omarcu supergroup  131786500 2023-05-31 12:22 hdfs://aion-0129:9000/terasort_out/part-r-00033
-rw-r--r--   3 omarcu supergroup  131823500 2023-05-31 12:22 hdfs://aion-0129:9000/terasort_out/part-r-00034
-rw-r--r--   3 omarcu supergroup  131594200 2023-05-31 12:22 hdfs://aion-0129:9000/terasort_out/part-r-00035
-rw-r--r--   3 omarcu supergroup  131432200 2023-05-31 12:22 hdfs://aion-0129:9000/terasort_out/part-r-00036
-rw-r--r--   3 omarcu supergroup  131685500 2023-05-31 12:22 hdfs://aion-0129:9000/terasort_out/part-r-00037
-rw-r--r--   3 omarcu supergroup  131599500 2023-05-31 12:22 hdfs://aion-0129:9000/terasort_out/part-r-00038
-rw-r--r--   3 omarcu supergroup  131399000 2023-05-31 12:22 hdfs://aion-0129:9000/terasort_out/part-r-00039
-rw-r--r--   3 omarcu supergroup  131526200 2023-05-31 12:22 hdfs://aion-0129:9000/terasort_out/part-r-00040
-rw-r--r--   3 omarcu supergroup  131551000 2023-05-31 12:22 hdfs://aion-0129:9000/terasort_out/part-r-00041
-rw-r--r--   3 omarcu supergroup  131634500 2023-05-31 12:22 hdfs://aion-0129:9000/terasort_out/part-r-00042
-rw-r--r--   3 omarcu supergroup  131559200 2023-05-31 12:22 hdfs://aion-0129:9000/terasort_out/part-r-00043
-rw-r--r--   3 omarcu supergroup  131486700 2023-05-31 12:23 hdfs://aion-0129:9000/terasort_out/part-r-00044
-rw-r--r--   3 omarcu supergroup  131666300 2023-05-31 12:23 hdfs://aion-0129:9000/terasort_out/part-r-00045
-rw-r--r--   3 omarcu supergroup  131499600 2023-05-31 12:22 hdfs://aion-0129:9000/terasort_out/part-r-00046
-rw-r--r--   3 omarcu supergroup  131593900 2023-05-31 12:22 hdfs://aion-0129:9000/terasort_out/part-r-00047
-rw-r--r--   3 omarcu supergroup  131541800 2023-05-31 12:23 hdfs://aion-0129:9000/terasort_out/part-r-00048
-rw-r--r--   3 omarcu supergroup  131491300 2023-05-31 12:23 hdfs://aion-0129:9000/terasort_out/part-r-00049
-rw-r--r--   3 omarcu supergroup  131597600 2023-05-31 12:23 hdfs://aion-0129:9000/terasort_out/part-r-00050
-rw-r--r--   3 omarcu supergroup  131534100 2023-05-31 12:23 hdfs://aion-0129:9000/terasort_out/part-r-00051
-rw-r--r--   3 omarcu supergroup  131606500 2023-05-31 12:23 hdfs://aion-0129:9000/terasort_out/part-r-00052
-rw-r--r--   3 omarcu supergroup  131518800 2023-05-31 12:22 hdfs://aion-0129:9000/terasort_out/part-r-00053
-rw-r--r--   3 omarcu supergroup  131541800 2023-05-31 12:23 hdfs://aion-0129:9000/terasort_out/part-r-00054
-rw-r--r--   3 omarcu supergroup  131455900 2023-05-31 12:23 hdfs://aion-0129:9000/terasort_out/part-r-00055
-rw-r--r--   3 omarcu supergroup  131653300 2023-05-31 12:23 hdfs://aion-0129:9000/terasort_out/part-r-00056
-rw-r--r--   3 omarcu supergroup  131555500 2023-05-31 12:23 hdfs://aion-0129:9000/terasort_out/part-r-00057
-rw-r--r--   3 omarcu supergroup  131560300 2023-05-31 12:23 hdfs://aion-0129:9000/terasort_out/part-r-00058
-rw-r--r--   3 omarcu supergroup  131467000 2023-05-31 12:23 hdfs://aion-0129:9000/terasort_out/part-r-00059
-rw-r--r--   3 omarcu supergroup  131488600 2023-05-31 12:23 hdfs://aion-0129:9000/terasort_out/part-r-00060
-rw-r--r--   3 omarcu supergroup  131637600 2023-05-31 12:23 hdfs://aion-0129:9000/terasort_out/part-r-00061
-rw-r--r--   3 omarcu supergroup  131573300 2023-05-31 12:23 hdfs://aion-0129:9000/terasort_out/part-r-00062
-rw-r--r--   3 omarcu supergroup  131712500 2023-05-31 12:23 hdfs://aion-0129:9000/terasort_out/part-r-00063
-rw-r--r--   3 omarcu supergroup  131506800 2023-05-31 12:23 hdfs://aion-0129:9000/terasort_out/part-r-00064
-rw-r--r--   3 omarcu supergroup  131508200 2023-05-31 12:23 hdfs://aion-0129:9000/terasort_out/part-r-00065
-rw-r--r--   3 omarcu supergroup  131634500 2023-05-31 12:23 hdfs://aion-0129:9000/terasort_out/part-r-00066
-rw-r--r--   3 omarcu supergroup  131626300 2023-05-31 12:23 hdfs://aion-0129:9000/terasort_out/part-r-00067
-rw-r--r--   3 omarcu supergroup  131606400 2023-05-31 12:23 hdfs://aion-0129:9000/terasort_out/part-r-00068
-rw-r--r--   3 omarcu supergroup  131423000 2023-05-31 12:23 hdfs://aion-0129:9000/terasort_out/part-r-00069
-rw-r--r--   3 omarcu supergroup  131691700 2023-05-31 12:23 hdfs://aion-0129:9000/terasort_out/part-r-00070
-rw-r--r--   3 omarcu supergroup  131529000 2023-05-31 12:23 hdfs://aion-0129:9000/terasort_out/part-r-00071
-rw-r--r--   3 omarcu supergroup  131333500 2023-05-31 12:23 hdfs://aion-0129:9000/terasort_out/part-r-00072
-rw-r--r--   3 omarcu supergroup  131553400 2023-05-31 12:23 hdfs://aion-0129:9000/terasort_out/part-r-00073
-rw-r--r--   3 omarcu supergroup  131615400 2023-05-31 12:23 hdfs://aion-0129:9000/terasort_out/part-r-00074
-rw-r--r--   3 omarcu supergroup  131415500 2023-05-31 12:23 hdfs://aion-0129:9000/terasort_out/part-r-00075
*Finally, let's do a word count with input file from HDFS.

We generate a file.

Use create.sh.

0 [omarcu@access1 ~]$ cat create.sh 
#!/bin/bash -l


for repl in {1..10}
do
echo " a b c d e f g h i j k l m n o p q r s t t u v w x y z" >> input.xml1
done
./create.sh

omarcu@aion-0129:/opt/hadoop$ bin/hdfs dfs -copyFromLocal /home/users/omarcu/input.xml1 hdfs://aion-0129:9000/
omarcu@aion-0129:/opt/hadoop$ bin/hdfs dfs -ls hdfs://aion-0129:9000/            
Found 3 items
-rw-r--r--   3 omarcu supergroup        550 2023-05-31 12:32 hdfs://aion-0129:9000/input.xml1

Now, take a Spark shell and follow the commands:
omarcu@aion-0129:/opt/spark$ /opt/spark/bin/spark-shell --master spark://aion-0129:7078
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
23/05/31 12:33:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Spark context Web UI available at http://aion-0129:4040
Spark context available as 'sc' (master = spark://aion-0129:7078, app id = app-20230531123321-0005).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.4.0
      /_/

Using Scala version 2.12.17 (OpenJDK 64-Bit Server VM, Java 11.0.19)
Type in expressions to have them evaluated.
Type :help for more information.

scala> import org.apache.spark.rdd.RDD
import org.apache.spark.rdd.RDD

scala> val filePath = "hdfs://aion-0129:9000/input.xml1"
filePath: String = hdfs://aion-0129:9000/input.xml1

scala> val textRDD: RDD[String] = spark.sparkContext.textFile(filePath)
textRDD: org.apache.spark.rdd.RDD[String] = hdfs://aion-0129:9000/input.xml1 MapPartitionsRDD[1] at textFile at <console>:24

scala> val wordCountsRDD: RDD[(String, Int)] = textRDD.flatMap(line => line.split(" ")).map(word => (word, 1)).reduceByKey(_ + _)
wordCountsRDD: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[4] at reduceByKey at <console>:24

scala> wordCountsRDD.collect().foreach(println)
(d,10)                                                                          
(z,10)
(p,10)
(x,10)
(t,20)
(b,10)
(h,10)
(,10)
(n,10)
(f,10)
(j,10)
(v,10)
(r,10)
(l,10)
(w,10)
(s,10)
(e,10)
(a,10)
(i,10)
(k,10)
(y,10)
(u,10)
(o,10)
(q,10)
(g,10)
(m,10)
(c,10)

scala> 
SOCKS5 configuration
Install Firefox and https://addons.mozilla.org/en-US/firefox/addon/foxyproxy-standard/ then follow the tutorial at https://support.ipvanish.com/hc/en-us/articles/360001410573-How-to-Configure-the-SOCKS5-Proxy-in-Firefox-
Prepare with:
ssh -D 1080 -C aion-cluster
This is how Spark looks:



Machine and Deep Learning on the UL HPC Platform

 Copyright (c) 2013-2019 UL HPC Team  <hpc-sysadmins@uni.lu>


This tutorial demonstrates how to develop and run on the UL HPC platform a deep learning application using the Tensorflow framework.

The scope of this tutorial is single node execution, multi-CPU and multi-GPU. Another tutorial covers multi-node execution.

Pre-requisites

Ensure you are able to connect to the UL HPC clusters. For all tests and compilation, you MUST work on a computing node

This hands-on is not a tutorial on Deep Learning (DL). For that, we encourage you to take a look at:

MIT introduction to deep learning
Nvidia resources on deep learning, and developer site
the courses taught at Master Datascience Paris Saclay and available on https://github.com/m2dsupsdlclass/lectures-labs
1. Preliminary installation

Step 1.a Connect to a cluster node

To configure the environment, we only need one core of one node.

Request such resource from the iris-cluster:

[]$ ssh iris-cluster
[]$ si -n1 -c1
[]$ # Inspect your resources, several ways:
[]$ scontrol show job $SLURM_JOB_ID
[]$ sj $SLURM_JOB_ID  # alias for the above
[]$ env | grep SLURM  # similar information via env variables
Step 1.b Prepare python virtualenv(s)

Our working environment will consist of:

Python 3,
CUDA toolkit for the GPU tests,
Tensorflow, both for CPU and GPU.
Tensorflow comes in different versions for CPU and GPU. We install both, and will select the correct one (matching the reserved node) before running the code.

To do so, we can rely on virtualenv. If you have never used virtualenv before, please have a look at Python1 tutorial.

[]$ # Load the required version of python, and the CUDA libraries:
[]$ module load lang/Python/3.6.4-foss-2018a system/CUDA numlib/cuDNN
[]$ module list # ml
To quickly recover our configuration, when developing interactively, we can save a module configuration:

[]$ # Save the module in a specific module list named 'tf':
[]$ module save tf
[]$ module purge
[]$ module restore tf
Set up virtualenv:

[]$ # Create a common directory for your python 3 environments:
[]$ mkdir ~/venv && cd ~/venv
[]$ # for the CPU tensorflow version
[]$ virtualenv tfcpu
[]$ # for the GPU version
[]$ virtualenv tfgpu
Step 1.c. Install Tensorflow

See also installation notes

You should have 2 virtualenvs created: tfcpu and tfgpu.

First install for the CPU environment (that matches the current reservation request):

[]$ source ~/venv/tfcpu/bin/activate
(tfcpu) []$ pip install tensorflow  # check the prompt for the environment
(tfcpu) []$ # Check the installation:
(tfcpu) []$ python -c "import tensorflow as tf; print('tf:{}, keras:{}.'.format(tf.__version__, tf.keras.__version__))"
At the time of writing, recent versions of numpy broke a tensorflow tutorial source file. In case of exception when unpickling the training data, apply a patch imdb.patch, listed below:

--- lib/python3.6/site-packages/tensorflow/python/keras/datasets/imdb_.py   2019-05-16 14:51:36.074289000 +0200
+++ lib/python3.6/site-packages/tensorflow/python/keras/datasets/imdb.py    2019-05-16 14:52:12.984972000 +0200
@@ -82,7 +82,7 @@
       path,
       origin=origin_folder + 'imdb.npz',
       file_hash='599dadb1135973df5b59232a0e9a887c')
-  with np.load(path) as f:
+  with np.load(path, allow_pickle=True) as f:
     x_train, labels_train = f['x_train'], f['y_train']
     x_test, labels_test = f['x_test'], f['y_test']
Apply the patch:

(tfcpu) []$ cp imdb.patch $VIRTUAL_ENV
(tfcpu) []$ cd $VIRTUAL_ENV
(tfcpu) []$ patch -p0 <imdb.patch
Then install for the GPU environment. You can install from a CPU node, but not execute any GPU specific code. To change virtual environments, you do not need to deactivate the tfcpu environment first:

[]$ source ~/venv/tfgpu/bin/activate
(tfgpu) []$ pip install tensorflow-gpu  # check the prompt for the environment
(tfgpu) []$ # Check the installation:
(tfgpu) []$ python -c "import tensorflow as tf; print('tf:{}, keras:{}.'.format(tf.__version__, tf.keras.__version__))"
Apply the patch again (patch file should still be there):

(tfgpu) []$ cd $VIRTUAL_ENV
(tfgpu) []$ patch -p0 <imdb.patch
2. Interactive development of a Tensorflow application

In this step, we will develop a tensorflow model on a CPU node (more available), and execute it interactively on both CPU and GPU nodes.

Pre-requisites

Connect to an iris cluster node, GPU is not required. See above to request such a resource.

If Python is not already loaded:

[]$ module restore tf
If your virtual environment is not available, activate it:

[]$ source ~/venv/tfcpu/bin/activate
(tfcpu) []$
Tensorflow example model

Head to the TensorFlow text classification tutorial and follow the steps there to assemble a Tensorflow application.

You should end up with a standalone python program that defines, trains and predicts a model.

GPU interactive execution

Here, we will execute the functioning program developed above on a GPU node, interactively.

Request such resource from the iris-cluster:

[]$ ssh iris-cluster
[]$ si-gpu -G 1 -n1 -c1
Load the modules, and the choose the proper python environment:

[]$ module restore tf
[]$ source ~/venv/tfgpu/bin/activate
(tfgpu) []$
Run the same tensorflow tutorial program that was developed under the CPU node.

3. Batch execution of a Tensorlow application

Now, we will execute the same Tensorflow program in batch mode, on a CPU or GPU node.

One program from the tutorial is here imdb-train.py.

To run in batch mode, you need a launcher file, that is passed as an argument to sbatch. From the previous interactive sessions, you can write such launcher files.

If you get stuck, here is an initial version for the CPU node:

#!/bin/bash -l

#SBATCH --job-name="CPU imdb"
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=1
#SBATCH --time=0-00:10:00
#SBATCH --partition=batch
#SBATCH --qos=normal

module restore tf
source ~/venv/tfcpu/bin/activate
srun python imdb-train.py
Adjust the script for the GPU node. (See tfgpu.sh for one example)

What happens if you change the resources assigned to this job? (number of cores, number of GPUs)

Scalable Deep Learning on the UL HPC Platform

 Copyright (c) 2013-2019 UL HPC Team  <hpc-sysadmins@uni.lu>


The objective of this tutorial is to practice running Horovod (and Keras/TensorFlow) on the UL HPC iris cluster.

It's important that you read the slides first.

Horovod with TensorFlow, multi-node & multi-GPU tests

As an initial test, you will now use the following launcher to:
reserve 2 GPU nodes and all their GPUs (8) - edit the launcher to match this
start Horovod through its horovodrun wrapper
#!/bin/bash -l
#SBATCH -J HorovodTFGPU
#SBATCH -o %x_%j.out
#SBATCH -N 2
#SBATCH --ntasks-per-node=4
#SBATCH --gres=gpu:4
#SBATCH -t 1:0:0
#SBATCH -p gpu

## The following only needed during HPC School 2019.06
module load swenv/default-env/devel

## Load TF and Horovod that link to CUDA, cuDNN & NCCL
module load lib/TensorFlow/1.13.1-fosscuda-2019a-Python-3.7.2
module load tools/Horovod/0.16.3-fosscuda-2019a-Python-3.7.2

## Create tests directory and clone the TF benchmarks inside
mkdir $SCRATCH/tests-horovod && cd $SCRATCH/tests-horovod
git clone https://github.com/tensorflow/benchmarks
cd benchmarks
git checkout cnn_tf_v1.13_compatible

## Horovod execution
horovodrun -np $SLURM_NTASKS \
    python scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py \
      --model resnet101 --batch_size 64 --variable_update horovod
Now check:
how many images/sec did the benchmark show at the end of its execution?
what results do you get from a single node (4 GPUs)? and from a single GPU?
If you load the non-accelerated versions for TF and Horovod (the ones on the foss instead of fosscuda toolchain)
what result do you get from a regular compute node without GPUs (use the batch partition) when using it fully, i.e. 28 cores?
how many full regular nodes do you need to use to replicate the benchmark result from a single accelerated node with its 4 GPUs?
Horovod with Keras and TensorFlow

For this part we will use the (excellent) SC18 Tutorial: Deep Learning At Scale.

You will need to git clone https://github.com/NERSC/sc18-dl-tutorial on the Iris cluster (preferably under your $SCRATCH). Then we will need to adapt its input configuration files under configs and the launcher scripts.

You will find under the current (UL HPC) tutorial's repository customized files to be used for the Iris cluster:

configs/
├── cifar10_cnn.yaml
├── cifar10_resnet.yaml
├── hello.yaml
└── imagenet_resnet.yaml
scripts/
├── cifar_cnn.sh
├── cifar_resnet.sh
├── imagenet_resnet.sh
└── setup.sh
Typically you will start by launching the cifar-cnn.sh example, and will quickly discover it's running slow (check the appropriate output in logs/. What will you need to adapt? What's different from the *_resnet.sh launchers? (take a look at what train.py does in --distributed mode).

Horovod



Horovod (website) is a framework for scaling training based on the Ring All-Reduce protocol. Unlike approaches that use a centralized memory for aggregating and broadcasting weights' updates during stochastic gradient descent (SGD) computations, Horovod takes advantage of the computing machine's communication links, such as NVLink, to maximize performance. Horovod integrates with popular modern deep learning frameworks like Keras2, TensorFlow2, PyTorch2, with a few code changes making it easy to incorporate into existing workflows.

By using Horovod, you can attempt to accelerate the distributed training time T compared to the time for a single accelerator G. However, communication can reduce scalability, and the batch size is often inversely proportional to the communication time.

The theoretical estimation for T is T=G/W+C(W), with W the number of workers, and C the communiction time between workers.

Pre-requiste

It is assumed you already use a modern deep learning framework like Tensorflow2 or PyTorch2.

Installation

ULHPC team propose either to load a previously installed or you can install it yourself.

Pre-installed approach:

source /work/projects/ulhpc-tutorials/PS10-Horovod/env.sh
We strongly recommend to use the provided environment. However, if you want to install it yourself, we provide help in the appendix 1.

The following command ensures that you can now use Horovod:

horovodrun --check-build
The command should produce the following output:

Available Frameworks:
    [X] TensorFlow
    [X] PyTorch
    [ ] MXNet

Available Controllers:
    [X] MPI
    [X] Gloo

Available Tensor Operations:
    [X] NCCL
    [ ] DDL
    [ ] CCL
    [X] MPI
    [X] Gloo
Ensures that the deep learning framework you wish to use is checked, along with MPI and NCCL.

Horovod typical code

The proposed codes contains the following blocks of codes:

Initalizing Horvod. The Horovod object it is generally named "hvd". It contains collective communication primitives and callbacks.
Adaptating the local_batch according the desired global_bath_size and the number of workers
Pinning AI-accelerator to workers in a bijective way
Creating data generators which will transfer asynchronously and efficiently data samples Disk->RAM->VRAM. The transfer between I/O to RAM is a "shard" and from the RAM to the VRAM the "local batch".
Building the neural network in each GPU VRAM
Training it
Evaluating it (time & accuracy)
Bonus : You can add some features (e.g, Horovod callbacks) for adding more features to your code but they come with a speed overheads. Example: verbosity, monitoring the validation metric, regular checkpointing after each epoch, learning rate scheduling with loss plateau detection, ...

Proposed code samples:

Test code for checking horovod initialization

ULHPC Tensorflow/Keras code example

ULHPC Torch code example

Official Horovod code examples

Testing multi-node multi-GPU Horovod

For testing large-scale training we launch test_horovod.py on 2 nodes, each node containing 4 GPUs

Script multinode-multigpu-test.sh:

#!/bin/sh -l
#SBATCH -c 2              # 2 CPU-core for each process
#SBATCH -N 2              # 2 nodes
#SBATCH -p gpu
#SBATCH --gpus-per-node 3 # Each process will see 3 GPUs
#SBATCH -t 30
#SBATCH --export=ALL

mpirun -n 6 python test_horovod.py
sbatch multinode-multigpu-test.sh
The SLURM correctly output:

List of TF visible physical GPUs :  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU')]
MPI_size = 6, MPI_rank = 0, MPI_local_size = 3,  MPI_local_rank = 0 platform = iris-179
List of TF visible physical GPUs :  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU')]
MPI_size = 6, MPI_rank = 1, MPI_local_size = 3,  MPI_local_rank = 0 platform = iris-180
List of TF visible physical GPUs :  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU')]
MPI_size = 6, MPI_rank = 2, MPI_local_size = 3,  MPI_local_rank = 1 platform = iris-179
List of TF visible physical GPUs :  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU')]
MPI_size = 6, MPI_rank = 4, MPI_local_size = 3,  MPI_local_rank = 2 platform = iris-179
List of TF visible physical GPUs :  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU')]
MPI_size = 6, MPI_rank = 3, MPI_local_size = 3,  MPI_local_rank = 1 platform = iris-180
List of TF visible physical GPUs :  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU')]
MPI_size = 6, MPI_rank = 5, MPI_local_size = 3,  MPI_local_rank = 2 platform = iris-180
Examples

Tensorflow2

Running tensorflow2 code with 1 GPU:

$ mpirun -n 1 python tensorflow_horovod.py
[...]
Epoch 4/4
195/195 - 147s - loss: 0.1119 - 147s/epoch - 753ms/step
Loss:  1.7116930484771729  accuracy:  0.4459134638309479
Running tensorflow2 code with 2 GPUs:

$ mpirun -n 2 python tensorflow_horovod.py
[...]
Epoch 4/4
Epoch 4/4
195/195 - 92s - loss: 0.1164 - 92s/epoch - 472ms/step
195/195 - 92s - loss: 0.1173 - 92s/epoch - 469ms/step
Loss:  1.3920882940292358  accuracy:  0.5380609035491943
Loss:  1.3958407640457153  accuracy:  0.5354567170143127
The training time are improved by using 2 GPUs compared to 1.

PyTorch2

Running PyTorch2 code with 1 GPU:

$ mpirun -n 1 python pytorch_horovod.py
[...]
Epoch: 4 141 sec.
Loss:  -0.7153724431991577  accuracy:  0.7164999842643738
Running PyTorch2 code with 2 GPUs:

$ mpirun -n 2 python pytorch_horovod.py
[...]
Epoch: 4 85 sec.
Loss:  -0.6600856781005859  accuracy:  0.6620000004768372
Epoch: 4 85 sec.
Loss:  -0.6600856781005859  accuracy:  0.6620000004768372
The prediction quality remains similar (around 70% +/- 4%) however the training time with 2 GPUs is 1.65 times faster.

Going further towards scalability

Bigger batch reduce the communication need. If your are facing scalability issue increase the batch size.

Large Batch Size (LBS) such as >1024 may hurts the convergence, for mitigating this:
Learning Rate scheduling. This can help compensate for the challenges posed by larger batch sizes and aid in achieving better convergence.
Adam optimizer offers better experimental results than SGD. The adaptive nature of the Adam optimizer can help alleviate some of the convergence issues associated with LBS. This blog post
Adapting the neural network architecture for scalability. For example, some suggest that wider model can scale better: L Chen et al 2018
Appendices

Appendix 1: Install Horovod yourself

Before installing Horovod you need to get dependendices: MPI, CUDA, CUDNN, NCCL. All of them requires matching versions :)

There are some already installed software for helping you in the horovod installation quest.

The provided script sets up the environment variables required for installing Horovod's dependencies.

HPCAI_ROOT="/work/projects/ulhpc-tutorials/PS10-Horovod/"

# MPI, CUDA, and compilers
module load toolchain/intelcuda

# CUDNN
export CUDNN_PATH=${HPCAI_ROOT}/soft/cudnn/install/cudnn-linux-x86_64-8.8.1.3_cuda11-archive
export LD_LIBRARY_PATH=${CUDNN_PATH}/lib/:${LD_LIBRARY_PATH}
export CPATH=${CUDNN_PATH}/include/:${CPATH}

# NCCL
NCCL_DEBUG=INFO # allow to check if NCCL is active
# NCCL_ROOT=${HPCAI_ROOT}/miniconda/install/miniconda/lib/python3.10/site-packages/nvidia/nccl/ #nccl also present there
NCCL_ROOT=/work/projects/ulhpc-tutorials/PS10-Horovod/soft/nccl/install/nccl_2.17.1-1+cuda11.0_x86_64/
NCCL_INCLUDE_DIR=${NCCL_ROOT}/include/
NCCL_LIBRARY=${NCCL_ROOT}/lib/

LD_LIBRARY_PATH=${NCCL_ROOT}/lib/:${LD_LIBRARY_PATH}
CPATH=${NCCL_ROOT}/include/:${CPATH}

HOROVOD_NCCL_HOME=${NCCL_ROOT}
HOROVOD_NCCL_INCLUDE=${NCCL_ROOT}/include/
HOROVOD_NCCL_LIB=${NCCL_ROOT}/lib/

# HOROVOD
HOROVOD_GPU_OPERATIONS=NCCL
HOROVOD_WITH_TENSORFLOW=1
HOROVOD_WITH_PYTORCH=1
HOROVOD_WITH_MPI=1

# GIVE XLA CUDA PATH
export XLA_FLAGS=--xla_gpu_cuda_data_dir=/opt/apps/resif/iris-rhel8/2020b/gpu/software/CUDAcore/11.1.1/
Now let's install Horovod with NCCL (1 single command):

HOROVOD_GPU_OPERATIONS=NCCL HOROVOD_NCCL_INCLUDE=$HOROVOD_NCCL_INCLUDE HOROVOD_NCCL_LIB=$HOROVOD_NCCL_LIB pip install --no-cache-dir --force-reinstall horovod
Checking Horovod

The following command ensures that you can now use Horovod:

horovodrun --check-build
The command should produce the following output:

Available Frameworks:
    [X] TensorFlow
    [X] PyTorch
    [ ] MXNet

Available Controllers:
    [X] MPI
    [X] Gloo

Available Tensor Operations:
    [X] NCCL
    [ ] DDL
    [ ] CCL
    [X] MPI
    [X] Gloo
Ensures that the deep learning framework you wish to use is checked, along with MPI and NCCL.

Appendix 2:

Bigger batch reduce the communication need. If your are facing scalability issue increase the batch size.

Large Batch Size (LBS) such as >1024 may hurts the convergence, for mitigating this:
Learning Rate scheduling. This can help compensate for the challenges posed by larger batch sizes and aid in achieving better convergence.
Adam optimizer offers better experimental results than SGD. The adaptive nature of the Adam optimizer can help alleviate some of the convergence issues associated with LBS. https://medium.com/mini-distill/effect-of-batch-size-on-training-dynamics-21c14f7a716e
Re-thinking the neural network architecture for scalability https://proceedings.neurips.cc/paper/2018/file/e7c573c14a09b84f6b7782ce3965f335-Paper.pdf

HuggingFace powered with DeepSpeed on HPC

What is HuggingFace ?

HuggingFace is a company that provides a variety of open-source Transformers codes, particularly in the domain of Large Language Models (LLM). They are well-known for their huggingface Python package, which offers Transformers architecture codes, and pre-trained models for various tasks. Those codes allow us to build LLM and fine-tune them on custom applications, saving us the need to write complex PyTorch code or train them from scratch.

What is DeepSpeed ?

DeepSpeed is an open-source deep-learning optimization library developed by Microsoft. It is designed to improve the training efficiency and scale of large deep-learning models. DeepSpeed provides a set of capabilities, particularly an optimizer named Zero. Zero optimizer contains:

Data-parallelism: It splits the batch computing into smaller batches (named local batches), distributing these batches across multiple GPUs, and simultaneously training the model on each local batch to accelerate the training process.
CPU Offloading: During both the forward and backward steps, it smartly moves the current layer's computations to the GPU for speed, while keeping the other layers stored in the CPU to make use of its larger memory capacity. This allows handle models with billions of parameters that might exceed the GPU memory.
16-bit arithmetic for faster computing, faster gradient communication between GPUs, and lower memory footprint.
List of other features: https://www.deepspeed.ai/docs/config-json/
This tutorial is structured as follow:

Installation of Anaconda
Installationf of DeepSpeed, HuggingFace
HuggingFace+DeepSpeed code
Performance analysis
This tutorial has been evaluated on Iris (ULHPC) and MeluXina (National HPC).

Installation of Anaconda

Please skip this if you have already Anaconda. Why using anaconda and not pip directly ? Because anaconda comes with conda command which makes it much easier the installation and maintenance of libraries such as CUDA, CUDNN, libaio...

We document 2 ways to install it: option a) using module, or option b) installing from source (more advanced).

Step 1 - Option A: locate your HPC module:

module spider conda allows you to locate and give you the command to load miniconda.

Example on Iris:

module load lang/Anaconda3/2020.11

Step 1 - Option B: install it yourself

Installing it yourself allows to have a full control of your miniconda installation but note Anaconda may consume more than 35Go.

wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
Then you can run the script:

chmod +x Miniconda3-latest-Linux-x86_64.sh
./Miniconda3-latest-Linux-x86_64.sh
Step 2: Source your environment

Please don't forget to update the path, example:

# >>> conda initialize >>>
# !! Contents within this block are managed by 'conda init' !!
__conda_setup="$('/opt/apps/resif/iris-rhel8/2020b/skylake/software/Anaconda3/2020.11/bin/conda' 'shell.bash' 'hook' 2> /dev/null)"
if [ $? -eq 0 ]; then
    eval "$__conda_setup"
else
    if [ -f "/opt/apps/resif/iris-rhel8/2020b/skylake/software/Anaconda3/2020.11/etc/profile.d/conda.sh" ]; then
        . "/opt/apps/resif/iris-rhel8/2020b/skylake/software/Anaconda3/2020.11/etc/profile.d/conda.sh"
    else
        export PATH="/opt/apps/resif/iris-rhel8/2020b/skylake/software/Anaconda3/2020.11/bin:$PATH"
    fi
fi
unset __conda_setup
# <<< conda initialize <<<
After this command your terminal invit should look like this:

(base) 0 [ppochelu@iris-169 ~](3291585 1N/T/1CN)$

Please note the (base) which confirm the success of the previous command block

Step 3: Create the virtual environment:

Don't forget to do an anaconda virtual environment because you have not the right to update the main environment that the HPC administrators give you.

conda  create  -n  myenv

Step 4: Activate your virtualenvironment

Don't forget to activate your environment each time you will open a new terminal conda activate myenv

you should have something like this:

(myenv) 0 [ppochelu@iris-169 ~](3291585 1N/T/1CN)$

DeepSpeed installation

DeepSpeed is not mandatory if you want to do LLM but allows you to exploit fully HPC witch is required for really Large Language Model.

DeepSpeed installation

Notice: we force the version number for reproducibility purpose. Please feel free to remove the version number (e.g., "==0.12.3") for installing the last version.

conda install -c "nvidia/label/cuda-12.1.0" cuda-toolkit # cuda 12.1.0 is compatible with PyTorch 2.1
conda install cudnn==8.9.2.26
pip install torch==2.1.1
pip install torchvision

conda install libaio==0.3.113 # libaio enable disk offloading technics ("NVMe")

DS_BUILD_AIO=1   pip install --no-cache-dir --force-reinstall  deepspeed==0.12.3
Quick DeepSpeed installation check

Installating DeepSpeed enable the command ds_report

(myenv) 0 [ppochelu@iris-169 ~](3291585 1N/T/1CN)$ ds_report
[2023-11-29 11:43:59,123] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [OKAY]
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
async_io ............... [YES] ...... [OKAY]
fused_adam ............. [NO] ....... [OKAY]
cpu_adam ............... [NO] ....... [OKAY]
cpu_adagrad ............ [NO] ....... [OKAY]
cpu_lion ............... [NO] ....... [OKAY]
 [WARNING]  Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
evoformer_attn ......... [NO] ....... [NO]
fused_lamb ............. [NO] ....... [OKAY]
fused_lion ............. [NO] ....... [OKAY]
inference_core_ops ..... [NO] ....... [OKAY]
cutlass_ops ............ [NO] ....... [OKAY]
quantizer .............. [NO] ....... [OKAY]
ragged_device_ops ...... [NO] ....... [OKAY]
ragged_ops ............. [NO] ....... [OKAY]
random_ltd ............. [NO] ....... [OKAY]
 [WARNING]  sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
 [WARNING]  using untested triton version (2.1.0), only 1.0.0 is known to be compatible
sparse_attn ............ [NO] ....... [NO]
spatial_inference ...... [NO] ....... [OKAY]
transformer ............ [NO] ....... [OKAY]
stochastic_transformer . [NO] ....... [OKAY]
transformer_inference .. [NO] ....... [OKAY]
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/home/users/ppochelu/.local/lib/python3.8/site-packages/torch']
torch version .................... 2.1.1+cu121
deepspeed install path ........... ['/home/users/ppochelu/.local/lib/python3.8/site-packages/deepspeed']
deepspeed info ................... 0.12.3, unknown, unknown
torch cuda version ............... 12.1
torch hip version ................ None
nvcc version ..................... 12.1
deepspeed wheel compiled w. ...... torch 2.0, cuda 11.7
shared memory (/dev/shm) size .... 377.27 GB
3 intersting lines are:

torch version .................... 2.1.1+cu121
CUDA is correctly seen with 12.1 version

nvcc version ..................... 12.1
NVCC allows Collective Communication primitive such as Ring All reduce.

async_io ............... [YES]
which means Non Volatile Memory tensor compute capability are enabled.

Installing HuggingFace

The below command installs HuggingFace tranformers library with the optional DeepSpeed dependency

pip install huggingface[deepspeed]
PDSH

There command you can do from on MeluXina HPC, but could works on iris too. If you iris, you can skip because it already contains PDSH from computing nodes.

Installing PDSH

PDSH_ROOT=${PWD} # <---- WARNING replace with the path where do you it is installed
mkdir -p ${PDSH_ROOT}/download
mkdir -p ${PDSH_ROOT}/install
mkdir -p ${PDSH_ROOT}/build

# Download
cd ${PDSH_ROOT}/download
wget https://storage.googleapis.com/google-code-archive-source/v2/code.google.com/pdsh/source-archive.zip

# Extract the build
unzip source-archive.zip
mv ${PDSH_ROOT}/download/pdsh/ ${PDSH_ROOT}/build/pdsh

cd ${PDSH_ROOT}/build/pdsh/

# Install
./configure --prefix=${PDSH_ROOT}/install --enable-static-modules
make
make install
Sourcing PDSH

Don't forget to source PDSH everytime you will use DeepSpeed

LD_LIBRARY_PATH=${PDSH_ROOT}/install/lib/:$LD_LIBRARY_PATH
PATH=${PDSH_ROOT}/install/bin/:$PATH
export PDSH_RCMD_TYPE=ssh
HuggingFace+DeepSpeed code

First, we present a standard complete workflow for a Large Language Model (LLM) using HuggingFace, covering data loading, model training, and predictions. Next, we will modify this code to integrate HuggingFace with DeepSpeed, maximizing the utilization of High-Performance Computing (HPC) resources.

Mono-GPU HuggingFace code

The code contains those steps: data loading, tokenization, fine-tuning, evaluation, saving/restoring and inference.

Notice: we use opt-125m LLM, it is small LLM compared to others, making it ideal for the development phase and quick experiments. Afterwards, we could easily replace "facebook/opt-125m" string with bigger and more relevant LLM. The list of huggingFace LLM is foundable there: https://huggingface.co/models

# URL : HuggingFace code samples: https://huggingface.co/docs/transformers/training

model_name="facebook/opt-125m" # <--- select your LLM model

#########################################
# DETERMINISM FOR COMPARING CONVERGENCE #
#########################################
from transformers import enable_full_determinism
enable_full_determinism(42)

#################################
# DATA LOADING (takes 1 minute) #
#################################
from datasets import load_dataset
dataset = load_dataset("yelp_review_full")
print(dataset["train"][10])
print(dataset.shape)

###################################
# DATA PROCESSING (TOKENIZATION)  #
##################################
# Remove the 'label' feature from the dataset because we are not doing classification
dataset = dataset.remove_columns('label')

# ## Tokenize dataset according to pre-trained model
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=512)
tokenizer.pad_token = tokenizer.eos_token
def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True)

# Tokenization
tokenized_datasets = dataset.map(tokenize_function, batched=True)
n_rows = 1024 # Training on 1024 takes 5 minutes on 1GPU
small_train_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(n_rows))
small_eval_dataset = tokenized_datasets["test"].shuffle(seed=42).select(range(n_rows))

# ## Data collating
from transformers import DataCollatorForLanguageModeling
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

#################
# MODEL LOADING #
#################
from transformers import AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained(model_name) # OPT Model contains 125m parameters

################################
# MODEL TRAINING CONFIGURATION #
###############################
# ## Training hyperparameters
from transformers import TrainingArguments
training_args = TrainingArguments(
    output_dir="test_trainer-125m",
    evaluation_strategy="epoch",
    learning_rate=1e-4,
    weight_decay=0.01)

############
# TRAINING #
############
from transformers import Trainer
from accelerate import Accelerator
accelerator = Accelerator() # https://huggingface.co/docs/accelerate/package_reference/accelerator
model = accelerator.prepare(model) # put the model on the GPU
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset,
    eval_dataset=small_eval_dataset,
    data_collator=data_collator) # Training config
trainer.train() # Launch the taining loop
accelerator.wait_for_everyone() # Cleanup accelerator resources after training

########################
# AUTOMATIC EVALUATION #
########################
eval_results = trainer.evaluate()
print(f"Loss: {eval_results['eval_loss']:.2f}")

#########################
# INFERENCE PREPARATION #
########################
trainer.save_model("./opt_trained_model/") # Save the trained model with parameters
del trainer # <--- remove from memory for emulating inference deployment  after training
from transformers import AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained("./opt_trained_model/")

#############
# INFERENCE #
#############
from transformers import pipeline
generator = pipeline("text-generation", model=model, tokenizer=tokenizer, do_sample=True, device=0)
prompt = "I really love computers because"
pred=generator(prompt)
print(pred)
Multi-node multi-GPU DeepSpeed Code

Python code (LLM.py): The new block of code are highlight with comment "<----- DeepSpeed"

model_name="facebook/opt-125m" # <--- select your LLM model

# PROCESSES CONFIG <----- DeepSpeed
import os
pconfig=dict()
pconfig["master_addr"] = os.getenv("MASTER_ADDR", "localhost")
pconfig["master_port"] = int(os.getenv("MASTER_PORT", 9994))
pconfig["rank"] = int(os.getenv("RANK", "0"))
pconfig["local_rank"] = int(os.getenv("LOCAL_RANK", "0"))
pconfig["world_size"] = int(os.getenv("WORLD_SIZE", "1"))
print(pconfig)

# DETERMINISM FOR COMPARING CONVERGENCE
from transformers import enable_full_determinism
enable_full_determinism(42)

# DATA LOADING (takes 1 minute)
from datasets import load_dataset
dataset = load_dataset("yelp_review_full")
print(dataset["train"][10])
print(dataset.shape)


# DATA PROCESSING (TOKENIZATION) 
# Remove the 'label' feature from the dataset because we are not doing classification
dataset = dataset.remove_columns('label')

# ## Tokenize dataset according to pre-trained model
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("facebook/opt-125m", model_max_length=512)
tokenizer.pad_token = tokenizer.eos_token
def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True)

# Tokenization
tokenized_datasets = dataset.map(tokenize_function, batched=True)
n_rows = 1024
small_train_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(n_rows))
small_eval_dataset = tokenized_datasets["test"].shuffle(seed=42).select(range(n_rows))

# ## Data collating
from transformers import DataCollatorForLanguageModeling
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)


# MODEL LOADING
# ## Load OPT Model of 125m parameters
from transformers import AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained(model_name)

# MODEL TRAINING CONFIGURATION <----- DeepSpeed
bs=16
lbs=bs//pconfig["world_size"]
ds_config={
    "per_device_train_batch_size":lbs,
    "train_batch_size": bs,
    "train_micro_batch_size_per_gpu": lbs,
    "optimizer": {"type": "Adam"},
    "zero_optimization": {
        "stage": 3,
        "offload_optimizer": {
            "device": "cpu",
            "pin_memory": True
        },
        "offload_param": {
            "device": "cpu",
            "pin_memory": True
        },
        "overlap_comm": True,
        "contiguous_gradients": True
    }
}

from transformers import TrainingArguments
training_args = TrainingArguments(
    output_dir="test_trainer-125m",
    evaluation_strategy="epoch",
    learning_rate=1e-4,
    weight_decay=0.01,
    per_device_train_batch_size=lbs,
    deepspeed=ds_config # <----- DeepSpeed
)


# TRAINING
from transformers import Trainer
#from accelerate import Accelerator # <-- Disable accelerator with DeepSpeed
#accelerator = Accelerator()
#model = accelerator.prepare(model)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset,
    eval_dataset=small_eval_dataset,
    data_collator=data_collator,
)

trainer.train()

# Cleanup accelerator resources
#accelerator.wait_for_everyone()

# TESTING
print(model.eval())
eval_results = trainer.evaluate()
print(f"Loss: {eval_results['eval_loss']:.2f}")
SLURM launch script (launch_slurm_llm.sh):

#!/bin/sh -l
#SBATCH -A <your project number>
#SBATCH --partition=gpu
#SBATCH --qos=default
#SBATCH -c 128
#SBATCH -t 40
#SBATCH -N 2
#SBATCH --export=ALL

# get host name
hosts_file="hosts.txt"
scontrol show hostname $SLURM_JOB_NODELIST > $hosts_file

# Collect public key and accept them
while read -r node; do
    ssh-keyscan "$node" >> ~/.ssh/known_hosts
done < "$hosts_file"

# Create the host file containing node names and the number of GPUs
function makehostfile() {
perl -e '$slots=split /,/, $ENV{"SLURM_STEP_GPUS"};
$slots=4 if $slots==0;
@nodes = split /\n/, qx[scontrol show hostnames $ENV{"SLURM_JOB_NODELIST"}];
print map { "$b$_ slots=$slots\n" } @nodes'
}
makehostfile > hostfile

# Don't forget to source PDSH each time you need DeepSpeed
source /project/home/p200117/DistributedDeepLearning/soft/pdsh/env.sh

# Launch HuggingFace+DeepSpeed code by varying the number of GPUs and nodes
deepspeed --num_gpus 1 --num_nodes 1  --hostfile hostfile ./LLM2.py > 1.txt
deepspeed --num_gpus 2 --num_nodes 1  --hostfile hostfile ./LLM2.py > 2.txt
deepspeed --num_gpus 4 --num_nodes 1  --hostfile hostfile ./LLM2.py > 4.txt

# 2 nodes, and 4 GPUs per node, it makes 8 GPUs in total:
deepspeed --num_gpus 4  --num_nodes 2 --hostfile hostfile ./LLM2.py > 8.txt
Code launch:

sbatch ./launch_slurm_llm.sh
Results with a combination of cat and grep:

(base) [u101013@login03 deepspeed]$ cat 2.txt | grep 'train_runtime'
{'train_runtime': 190.3416, 'train_samples_per_second': 16.139, 'train_steps_per_second': 1.009, 'train_loss': 2.1075216929117837, 'epoch': 3.0}
(base) [u101013@login03 deepspeed]$ cat 2.txt | grep 'eval_loss'
{'eval_loss': 2.10894775390625, 'eval_runtime': 17.0182, 'eval_samples_per_second': 60.171, 'eval_steps_per_second': 3.761, 'epoch': 1.0}
{'eval_loss': 1.9250223636627197, 'eval_runtime': 12.3188, 'eval_samples_per_second': 83.125, 'eval_steps_per_second': 5.195, 'epoch': 2.0}
{'eval_loss': 1.865960955619812, 'eval_runtime': 12.254, 'eval_samples_per_second': 83.564, 'eval_steps_per_second': 5.223, 'epoch': 3.0}
Performance analysis

# GPUs	Training time (sec.)	Loss epoch#1	Loss epoch#2	Loss epoch#3
1	307	1,74	1,48	1,4
2	190	2,11	1,93	1,87
4	141	2	1,81	1,72
8	116	1,85	1,64	1,48
16	114	2	1,85	1,79
Conclusion:

More GPUs decrease the number of computing time.
The loss changes when we use a different number of GPUs because the order of images isn't the same based on the number of GPUs during training.
We fixed arbitrary values, more in-depth analysis is required for maximizing convergence speed: learning rate scheduling, batch size, model size.
Billions parameters LLM

LLM consumes a lot of memory: model's parameters, features flowing through layers, optimizer information, ... This is why some state-of-art LLM model does not fit entirely in the most advanced GPU memory.

For enabling training model with billions of parameter the HuggingFace+DeepSpeed code, we perform 2 improvements:

Fix the batch_size per device to the minimum value which is 1 to minimize memory consumption.
Enable DeepSpeed CPU offloading technics with "offload_optimizer" and ``"offload_param".
model_name="facebook/opt-125m"
[...]
lbs=1
bs=lbs*pconfig["world_size"]
ds_config={
  "per_device_train_batch_size":lbs,
  "train_batch_size": bs,
  "train_micro_batch_size_per_gpu": lbs,
  "optimizer": {"type": "AdamW"},
        "zero_optimization": {
        "stage": 3,
        "contiguous_gradients": True,
        "overlap_comm": True,
        "reduce_bucket_size": 1e7,
        "offload_optimizer": {
            "device": "cpu"
         },
        "offload_param": {
            "device": "cpu"
       }
   }
}
[...]
Notice: 3D parallelism does not split the memory consumption inside a layers but spread layers on GPU and CPU. This is why, when we have too big layer it can still crash memory.

LLM with restricted access

Some HuggingFace LLM requires extra permission such as Meta's Llama2 LLM named meta-llama/Llama-2-7b-hf. To use them, you need to follow those steps:

Create an account on https://huggingface.co/
Ask permission for a given LLM. Example: https://huggingface.co/meta-llama/Llama-2-7b-hf . You will receive mail notification from 24h to 48h later.
Generate a "read" token: https://huggingface.co/settings/tokens and copy the token.
Call the command: huggingface-cli login --token <your_token_pasted>
Now you can access the model. You can change the first line of the provided code in this tutorial with the new LLM name:

model_name="meta-llama/Llama-2-7b-hf"
[...]

Introduction to containers on ULHPC

Advantages of containers

Portability: run the same experiments on multiple environments (HPC centers, local machine, cloud services) without having to set up all necessary software on each environment.
Reproducibility: you can re-run the same experiment years later without worrying about:
which version of your software or its dependencies were used when you first ran your experiment
which operating system version / modules are currently on the HPC
Simplify team-work:
Share your container in your team to ensure everyone works with the same software environment
Version your containers to use new software version while still keeping older version for reproducibility
Onboard new team member faster
Simplify peer-review: share your container to help other teams anywhere in the world to quickly reproduce your experiments
Limitations

On the ULHPC, you cannot:

build or run Docker containers
build a Singularity container from a definition file
On the ULHPC, you can:

build a Singularity container from a Docker container
run any Singularity container
To build, you can:

use your own machine
use our container Gitlab project and benefit from its CI / container registry
Pratical sessions plan

Convert a publicly available Docker container to a Singularity container
Build a custom Docker container and convert it into a Singularity container
Use GPUs in a Singularity container
Practical session 1: Python container

The objective of this section are:

to create a simple container to be used on the ULHPC
to learn how to use the container
Creation of the container

First you need to connect to the ULHPC, preferably on AION. Then you need to request an interactive session as you cannot perform the following action from an access node.

[jschleich@access1 ~]$ si
Then, you first need to load the Singularity module to be able to use it. After that step, you can pull (download) an already available Docker container from Dockerhub and convert it into a Singularity container.

When the conversion is over, you have one new file which is named after the container and its version.

$ module load tools/Singularity
$ singularity pull docker://python:3.10
[...]
$ ll
total 307712
-rwxr-xr-x 1 jschleich clusterusers 314679296 May 30 10:43 python_3.10.0.sif
Note that if you prefer another version that 3.10, you can browse the available versions and amend the command line.

How to use the container

This section introduces run and exec briefly. For more information, the documentation of Singularity / Apptainer (fork of Singularity) can be found here.

Run vs Exec

The singularity run command is used to execute a Singularity container as if it were a standalone program. It runs the container image and executes the default action defined within the container. It is similar to directly running an executable file, but within the isolated environment provided by the Singularity container.

On the other hand, the singularity exec command allows you to execute a specific command or script within a Singularity container. It provides a way to run a specific program or script inside the container without executing the default action defined within the container.

In summary, singularity run is used to execute the default action defined in the container, while singularity exec is used to run a specific command or script within the container.

Let's first use run on with our new container. Without surprise, a Python prompts is displayed. From here, you can type Python code which will be run directly in the container.

Singularity run

$ singularity run python_3.10.0.sif
Python 3.10.0 (default, Dec  3 2021, 00:21:30) [GCC 10.2.1 20210110] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>>
Singularity exec

Let's now try exec. As we do not use the default action of the container, we need to specify what should be executed. In this example, I chose bash in order to execute a new interactive Bash shell. We are welcomed with a new prompt, Singularity> and then we can type in any commmand. Again, these commands will be exectuted from inside the container.

$ singularity exec python_3.10.0.sif bash
Singularity> python --version
Python 3.10.10
Binding feature

Now that we know how to run things in a container, we need to be able to interact with outside of the container, e.g., to access input files or to write output files. To do so, we can specify a list of directories (and even files) from outside the container, e.g., in our home / project / scratch directories to be accessible from inside the container.

When you use the --bind or -B option with Singularity, you specify a binding between a directory (or file) on the host system (in our case the ULHPC cluster) and a directory inside the container. This creates a link between the two, enabling the container to read from or write to the specified host directory or file.

The following example considers a Python script that will write some dummy content to a file in a specific output directory. The script itself is stored in an input directory.

The following folder structure is assumed:

.
|-- input
|   `-- script.py
|-- ouput
|   `-- my-file
|-- python_3.10.0.sif
The Python file script.py is as follows:

import os
import sys

def write_file(folder_path, file_name, content):
    file_path = os.path.join(folder_path, file_name)
    with open(file_path, 'w') as file:
        file.write(content)
    print(f"File '{file_name}' created in '{folder_path}'.")

if __name__ == '__main__':
    if len(sys.argv) != 3:
        print("Usage: python script.py <folder_path> <file_name>")
        sys.exit(1)

    folder_path = sys.argv[1]
    file_name = sys.argv[2]
    content = "Dummy content for the purpose of the demonstration"

    write_file(folder_path, file_name, content)
To execute the script in the container, see the following command:

singularity exec \
    -B input/:/container-input \
    -B ouput:/container-output \
    python_3.10.0.sif \
    python /container-input/script.py /container-output my-file
The command does the following:

It binds the input folder of the host to a container folder /container-input
It binds the output folder of the host to a container folder /container-output
It executes the file script.py which is located in the container at the following address: /container-input/script.py
It outputs a file named my-file to the container folder /container-output which is also output on the host file system.
Note: you can amend all folder names as you wish and you could use the same host directory for both input and output.

Auto-binding feature

By default, Singularity automatically binds several directories and in particular it binds the home folder of the host. This features simplifies the usage of Singularity for the users, however it can also lead to unexpected behaviours and frustration.

This is particularly true if you have different Python packages installed in your host home folder and in the container as the container may use the host packages instead of the container one.

This behaviour can be observed below. Here we can see all the packages which can be seen from inside the container. The list was truncated for the sake of conciseness. The container was not built with all those packages, however the container sees them because the auto-binding feature led to host packages being available to the container.

$ singularity run python_3.10.0.sif bash
Singularity> pip list
Package                 Version
----------------------- ---------
boltons                 23.0.0
brotlipy                0.7.0
certifi                 2022.12.7
cffi                    1.15.1
charset-normalizer      2.0.4
conda                   23.3.1
conda-content-trust     0.1.3
conda-package-handling  2.0.2
conda_package_streaming 0.7.0
cryptography            38.0.4
idna                    3.4
jsonpatch               1.32
jsonpointer             2.0
We can prevent that behaviour by adding the --no-home option and we can see that the list of packages is much smaller as it only contains the one from the container.

$ singularity run --no-home python_3.10.0.sif bash
Singularity> pip list
Package    Version
---------- -------
pip        21.2.4
setuptools 57.5.0
wheel      0.37.0
Note: you can use the --home option to specify a specific host directory to be mounted as the container home folder. See below:

$ singularity run --home `pwd` python_3.10.0.sif bash
Singularity> pip list
Package    Version
---------- -------
pip        21.2.4
setuptools 57.5.0
wheel      0.37.0
Practical session 2: custom R container

The objective of this section are:

to create a simple container to be used on the ULHPC
to learn how to use the container
Container specification and build

FROM registry.gitlab.uni.lu/hlst/ulhpc-containers/r4:4.2.2
ENV LC_ALL "C"
RUN R --slave -e 'install.packages("ggplot2",repos="https://cran.rstudio.com/")'
The above Docker file does the following things:

It requests a version of R which is maintained by the ULHPC team (r4:4.2.2) and use it as a base
It sets the LC_ALL environment variable to prevent some compilation issues
It installs the ggplot2 package. You can add as many package as you require for your experimental setup.
The build process, including testing of the container image is performed by Gitlab CI (Continuous Integration). The built container is then stored in the Gitlab container registry.

Use the container on ULHPC

Connection to the ULHPC

First you need to connect to the ULHPC, preferably on AION. Then you need to request an interactive session as you cannot perform the following action from an access node:

[jschleich@access1 ~]$ si
Conversion into a Singularity container

Then you need to load the Singularity module and then you can pull the Docker container from the Gitlab container registry and convert it into a Singularity container:

$ module load tools/Singularity
$ singularity pull docker://registry.gitlab.uni.lu/hlst/ulhpc-containers/r-tutorial:1.0.0
[...]
$ ll
-rwxr-xr-x   1 jschleich clusterusers 627M May 30 15:35 r-tutorial_1.0.0.sif
Execution of the container

You can then run the container simply by doing:

singularity run --no-home r-tutorial_1.0.0.sif
You should see the following result:

R version 4.2.2 (2022-10-31) -- "Innocent and Trusting"
Copyright (C) 2022 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.
Practical session 3: use GPUs with container

The objective of this section is to learn how to use the GPU with Singularity.

Pytorch container on GPU node

First you need to book a GPU from a GPU node. This gives you one GPU, one core for 30 min, which should be more than enough to test the following intructions.

si-gpu
Then, you need to load the Singularity module:

module load tools/singularity
You can copy the following script which uses PyTorch and requests a GPU to work.

import torch
import math

dtype = torch.float
device = torch.device("cuda:0")

# Create random input and output data
x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)
y = torch.sin(x)

# Randomly initialize weights
a = torch.randn((), device=device, dtype=dtype)
b = torch.randn((), device=device, dtype=dtype)
c = torch.randn((), device=device, dtype=dtype)
d = torch.randn((), device=device, dtype=dtype)

learning_rate = 1e-6
for t in range(2000):
    # Forward pass: compute predicted y
    y_pred = a + b * x + c * x ** 2 + d * x ** 3

    # Compute and print loss
    loss = (y_pred - y).pow(2).sum().item()
    if t % 100 == 99:
        print(t, loss)

    # Backprop to compute gradients of a, b, c, d with respect to loss
    grad_y_pred = 2.0 * (y_pred - y)
    grad_a = grad_y_pred.sum()
    grad_b = (grad_y_pred * x).sum()
    grad_c = (grad_y_pred * x ** 2).sum()
    grad_d = (grad_y_pred * x ** 3).sum()

    # Update weights using gradient descent
    a -= learning_rate * grad_a
    b -= learning_rate * grad_b
    c -= learning_rate * grad_c
    d -= learning_rate * grad_d


print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')

On the ULHPC, several Singularity containers are maintained via the ULHPC container project. This is a public project and you are encouraged to use it for you own containers.

Some of those maintained containers are Singularity container and can be found at the following address /work/projects/singularity/ulhpc/. The other containers are Docker containers which are stored and versioned in the container registry.

For our practical session, we will use a Singularity container which can be found at: /work/projects/singularity/ulhpc/pytorch-22.04.sif.

If you try to execute the container via the following command:

singularity run /work/projects/singularity/ulhpc/pytorch-22.04.sif
You will notice the following message:

WARNING: The NVIDIA Driver was not detected.  GPU functionality will not be available.
It means that the container does not have access to the GPU of the host node. If you try to execute the aforementioned Python script, you will see the following error:

Traceback (most recent call last):
  File "pytorch-test-gpu.py", line 9, in <module>
    x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)
  File "/opt/conda/lib/python3.8/site-packages/torch/cuda/__init__.py", line 216, in _lazy_init
    torch._C._cuda_init()
RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
In order to access the GPU, you have to add the --nv option. Under the hood, what that does is that Singularity will mount / bind whatever it needs from the host node to use the NVIDIA GPU.

singularity run --nv /work/projects/singularity/ulhpc/pytorch-22.04.sif python pytorch-test-gpu.py
Other containers using GPU

Have a look here: /work/projects/singularity/ulhpc and do not hesitate to request some more.

Final remark

Please use the ULHPC container project and do not hesitate to contact me in case of doubt on how to use it.

UL HPC Tutorial: HPC Containers with Singularity

 Copyright (c) 2018-2021 UL HPC Team <hpc-team@uni.lu>




Main Objectives of this Session

Discussion on container systems

what they are and where they help
common container systems
will focus on Singularity container system
how to use Singularity containers on the UL HPC platform

how to build containers from a definition file
how to import pre-existing containers
how to use applications embedded in containers
containerized parallel applications execution
A brief intro. to containers

Purpose of containers?

Application portability
containers bundle together an entire runtime env. (OS to apps.)
easy replication of environments
Services isolation
separate microservices in different containers
Do more with less
fast instantiation and tear-down
little memory/CPU overhead
Technology main points

OS-level virtualization - light virtualization
don't spin up a full virtual machine
Close to native bare metal speed
Common container systems

Docker

A new (2013-) take on containers (OpenVZ and LXC came before)
High uptake in Enterprise (microservices) & science (reproducibility)
In use everywhere (esp. DevOps), available on most Cloud infra.
Shifter

Linux containers for HPC, developed at NERSC
Uses Docker functionality but makes it safe in shared HPC systems
Image gateway used to convert Docker images before use
Singularity

Containers for science, initially developed at LBNL
Not based on Docker, but can directly import/run Docker images
Also HPC oriented, diff. take to running MPI software than Shifter
Provides an [Image Registry]{https://github.com/singularityhub/sregistry}
Singularity in a nutshell



build environment: your workstation (admin. required)
production environmemnt: UL HPC clusters
Source: Kurtzer GM, Sochat V, Bauer MW (2017) Singularity: Scientific containers for mobility of compute. PLoS ONE 12(5): e0177459

Singularity setups

Build env - Debian/Ubuntu

Install dependencies
sudo apt-get update && sudo apt-get install -y build-essential \
     libssl-dev uuid-dev libgpgme11-dev squashfs-tools \
     libseccomp-dev wget pkg-config git cryptsetup
Installing go
export VERSION=1.15 OS=linux ARCH=amd64 && \
  wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \
  sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \
  rm go$VERSION.$OS-$ARCH.tar.gz

echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \
source ~/.bashrc
Download singularity source
export VERSION="v3.6.4"
git clone https://github.com/hpcng/singularity.git
cd singularity
git checkout ${VERSION}
Compiling and installing singularity from source
./mconfig && \
    make -C builddir && \
    sudo make -C builddir install
Build env - CentOS & RHEL

The epel (Extra Packages for Enterprise Linux) repos contain Singularity
The singularity package is actually split into two packages called singularity-runtime
The package singularity which also gives you the ability to build Singularity containers
sudo yum update -y
sudo yum install -y epel-release
sudo yum update -y
sudo yum install -y singularity-runtime singularity
See also: https://sylabs.io/guides/3.0/user-guide/installation.html#install-on-linux

Build env - macOS

Prerequisites - install Brew, VirtualBox and Vagrant
/usr/bin/ruby -e "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)"
brew cask install virtualbox
brew cask install vagrant
brew cask install vagrant-manager
Initialize an Ubuntu VM and install Singularity inside
mkdir singularity-vm && cd singularity-vm
export VM=sylabs/singularity-3.2-ubuntu-bionic64
vagrant init $VM
vagrant up
vagrant ssh
See also: https://sylabs.io/guides/3.0/user-guide/installation.html#install-on-windows-or-mac

Use on the UL HPC clusters

Production environment (no admin rights)
Images can be pulled from official repository (dockerhub,shub,...)
Images CANNOT be built on the ULHPC platform
module load tools/Singularity
Now that Singularity is there...

singularity
Usage:
  singularity [global options...] <command>
Available Commands:
  build       Build a Singularity image
  cache       Manage the local cache
  capability  Manage Linux capabilities for users and groups
  config      Manage various singularity configuration (root user only)
  delete      Deletes requested image from the library
  exec        Run a command within a container
  inspect     Show metadata for an image
  instance    Manage containers running as services
  key         Manage OpenPGP keys
  oci         Manage OCI containers
  plugin      Manage Singularity plugins
  pull        Pull an image from a URI
  push        Upload image to the provided URI
  remote      Manage singularity remote endpoints
  run         Run the user-defined default command within a container
  run-help    Show the user-defined help for an image
  search      Search a Container Library for images
  shell       Run a shell within a container
  sif         siftool is a program for Singularity Image Format (SIF) file manipulation
  sign        Attach digital signature(s) to an image
  test        Run the user-defined tests within a container
  verify      Verify cryptographic signatures attached to an image
  version     Show the version for Singularity
Quick start with Singularity

Pulling from DockerHub

You can pull images from DockerHub
Example for a specific python version
singularity pull docker://python:3.8.0b1-alpine3.9
singularity exec python_3.8.0b1-alpine3.9.sif python3
singularity shell python_3.8.0b1-alpine3.9.sif
The ouput is the following:
./python_3.8.0b1-alpine3.9.sif 
Python 3.8.0b1 (default, Jun  5 2019, 23:34:27) 
[GCC 8.3.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> print("Running python from within the container.") 
This brought us an immutable image with (tiny) Alpine Linux & Python 3.8.0b1 from the Docker Registry. The image is not writeable, but has access to our home directory by default.
Building from scratch

Sandbox mode: container development in your build env (laptop)
sudo singularity build --sandbox \
     python_3.7.3-stretch docker://python:3.7.3-stretch
sudo singularity exec --writable \
     python_3.7.3-stretch/ pip3 install numpy nose pytest hypothesis
singularity exec python_3.7.3-stretch \
     python3 -c "import numpy; numpy.test()"
This time the Docker Image was downloaded and unpacked to a directory (sandbox mode). Changes within the directory can be made persistent with the writable flag.

Building a sif image from the sandbox
done when ready to use envrionment in production
just have to send it to your dedicated space
sudo singularity build python_3.7.3-stretch.sif python_3.7.3-stretch/ 
Now image can be transferred, e.g. to the Iris cluster and used normally.

Containers' access to filesystem(s)
Home directories are bind mounted by default
Your user(name) and group(s) are dynamically added
thus files created maintain normal permissions
Other paths need to be explicitly set
## Just an example - do not copy-paste
# Build 
sudo singularity build custom.sif python_3.7.3-stretch/
# Execute 
singularity exec --bind /work/projects/myprj/:/mnt \
            custom.sif python3 /mnt/my_nice_code.py
singularity exec --bind /work/projects/myprj:/work/projects/myprj \
            --bind /scratch/users/$USER:/scratch/users/$USER \
            custom.sif python3 /work/projects/myprj/nice_code.py -o \
            /scratch/users/$USER/output_dir/
With the first command we create a compressed, SIF - Singularity Image File from the sandbox folder. Then, we run the python3 interpreter from this image on code and data existing outside the container.

More details on SIF: https://archive.sylabs.io/2018/03/sif-containing-your-containers/

Definition headers

%setup: commands in the %setup section are first executed on the host system outside of the container after the base OS has been installed
%files: the %files section allows you to copy files into the container
%app*: redundant to build different containers for each app with nearly equivalent dependencies
%post: install new software and libraries, write configuration files, create new directories
%test: the %test section runs at the very end of the build process to validate the container using a method of your choice
%environment: The %environment section allows you to define environment variables that will be set at runtime
%startscript: the contents of the %startscript section are written to a file within the container at build time. This file is executed when the instance start command is issued
%runscript: the contents of the %runscript section are written to a file within the container that is executed when the container image is run (either via the singularity run command or by executing the container directly as a command
%labels: the %labels section is used to add metadata to the file /.singularity.d/labels.json within your container. The general format is a name-value pair
%help: any text in the %help section is transcribed into a metadata file in the container during the build. This text can then be displayed using the run-help command
Applications

Bootstrap: docker
From: ubuntu

%environment
    GLOBAL=variables
    AVAILABLE="to all apps"
##############################
# foo
##############################
%apprun foo
    exec echo "RUNNING FOO"
%applabels foo
   BESTAPP FOO
%appinstall foo
   touch foo.exec
%appenv foo
    SOFTWARE=foo
    export SOFTWARE
%apphelp foo
    This is the help for foo.
%appfiles foo
   foo.txt
##############################
# bar
##############################
%apphelp bar
    This is the help for bar.
%applabels bar
   BESTAPP BAR
%appinstall bar
    touch bar.exec
%appenv bar
    SOFTWARE=bar
    export SOFTWARE
Add %app prefix to headers and finish with the application name
Ex: singularity run --app foo my_container.sif
Advanced Singularity

Objectives:
Provide a Jupyter notebook container with full features
features: IPython Parallel, Virtualenv, CUDA, MPI
Step 1: Jupyter

We will consider the next singularity definition file
Bootstrap: library
From: ubuntu:18.04
Stage: build
%setup
    touch /file_on_host
    touch ${SINGULARITY_ROOTFS}/file_on_guest
%files
    /file_on_host /opt
%environment
    export PORT=8889
    export LC_ALL=C
%post
    apt-get update
    apt-get upgrade -y
    apt-get install -y software-properties-common
    add-apt-repository multiverse
    apt-get install -y python3 python3-pip python3-venv
    python3 -m pip install jupyter==1.0.0
%runscript
    echo "Container was created $NOW"
    echo "Arguments received: $*"
    exec python3 "$@"
%startscript
    echo "Started new instance on $(date)"
%test
    grep -q NAME=\"Ubuntu\" /etc/os-release
    if [ $? -eq 0 ]; then
        echo "Container base is Ubuntu as expected."
    else
        echo "Container base is not Ubuntu."
    fi
    python3 -m pip show jupyter
%labels
    Author ekieffer
    Version v0.0.1
%help
    This is a demo container used to illustrate a def
    file that uses all supported sections.
Run those command on your laptop. Please note rsynch may take a few minutes (250mo transfer).

sudo singularity build jupyter.sif jupyter.def
rsync -avz jupyter.def iris-cluster:jupyter.sif # to the cluster

Next, we need to prepare a launcher ...
#!/bin/bash -l
#SBATCH -J Singularity_Jupyter
#SBATCH -N 1 # Nodes
#SBATCH -n 1 # Tasks
#SBATCH -c 2 # Cores assigned to each task
#SBATCH --time=0-01:00:00
#SBATCH -p batch
#SBATCH --qos=normal
#SBATCH --mail-user=<firstname>.<lastname>@uni.lu
#SBATCH --mail-type=BEGIN,END

module load tools/Singularity
# Avoid to modify your current jupyter config
export JUPYTER_CONFIG_DIR="$HOME/jupyter_sing/$SLURM_JOBID/"
export JUPYTER_PATH="$HOME/jupyter_sing/$SLURM_JOBID/jupyter_path"
export JUPYTER_DATA_DIR="$HOME/jupyter_sing/$SLURM_JOBID/jupyter_data"
export JUPYTER_RUNTIME_DIR="$HOME/jupyter_sing/$SLURM_JOBID/jupyter_runtime"
export IPYTHONDIR="$HOME/ipython_sing/$SLURM_JOBID"
mkdir -p $IPYTHONDIR

export IP_ADDRESS=$(hostname -I | awk '{print $1}')

echo "On your laptop: ssh -p 8022 -J ${USER}@access-${ULHPC_CLUSTER}.uni.lu ${USER}@<node-name> -NL 8889:localhost:8889"
echo "Replace <node-name> with the compute node name received by slurm. Command squeue -u ${USER} to see it. Example: "aion-0085" or "iris-0112".

```bash    
singularity instance start jupyter.sif jupyter
singularity exec instance://jupyter jupyter \
    notebook --ip localhost --no-browser --port 8889 &
pid=$!
sleep 5s
singularity exec instance://jupyter  jupyter notebook list
wait $pid
echo "Stopping instance"
singularity instance stop jupyter
To start the container job: sbatch <launcher_name>.sh

Once your job is running (see squeue command), you can use ssh forwarding to connect to the notebook from your laptop. Open a terminal on your laptop and copy-paste the ssh forward command in the slurm-****.out logfile. You should be now able to reach your notebook.

MobaXterm users:

MobaXterm has tunneling a feature to setup ssh forwarding. Click on the "Tuneling".



Then click on "New SSH tunnel".



Use the ssh forward command in the slurm-****.out logfile to setup correctly the tunnel.

Then open your browser and go to the url: http://127.0.0.1:8889/. Jupyter should ask you for a password (see screenshot below). This password can be set before running the jupyter notebook and his part of the initial configuartion detailed at Jupyter official documentation.

if by mistake, you forgot to setup this password, have a look in the slurm-****.out file in which the output of the command jupyter notebook list has been recorded.

On your laptop: ssh -p 8022 -J ppochelu@access-aion.uni.lu ppochelu@aion-0085 -L 8889:localhost:8889
INFO:    instance started successfully
[I 13:46:53.550 NotebookApp] Writing notebook server cookie secret to /home/users/ekieffer/jupyter_sing/2169124/jupyter_runtime/notebook_cookie_secret
[I 13:46:53.916 NotebookApp] Serving notebooks from local directory: /home/users/ekieffer/singularity_tests
[I 13:46:53.916 NotebookApp] Jupyter Notebook 6.1.5 is running at:
[I 13:46:53.916 NotebookApp] http://172.17.6.75:8889/?token=f42786e3f025b30e28b8b6534bf74b2aab25e40ea229b20a
[I 13:46:53.916 NotebookApp]  or http://127.0.0.1:8889/?token=f42786e3f025b30e28b8b6534bf74b2aab25e40ea229b20a
[I 13:46:53.916 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[C 13:46:53.924 NotebookApp] 

    To access the notebook, open this file in a browser:
        file:///home/users/ekieffer/jupyter_sing/2169124/jupyter_runtime/nbserver-20-open.html
    Or copy and paste one of these URLs:
        http://172.17.6.75:8889/?token=f42786e3f025b30e28b8b6534bf74b2aab25e40ea229b20a
     or http://127.0.0.1:8889/?token=f42786e3f025b30e28b8b6534bf74b2aab25e40ea229b20a
Currently running servers:
http://172.17.6.75:8889/?token=f42786e3f025b30e28b8b6534bf74b2aab25e40ea229b20a :: /home/users/ekieffer/singularity_tests
[I 13:49:54.399 NotebookApp] 302 GET / (172.17.2.11) 0.92ms
[I 13:49:54.423 NotebookApp] 302 GET /tree? (172.17.2.11) 1.17ms
[I 13:51:04.820 NotebookApp] 302 GET /?token=f42786e3f025b30e28b8b6534bf74b2aab25e40ea229b20a (172.17.2.11) 0.82ms
[I 13:51:20.005 NotebookApp] Writing notebook-signing key to /home/users/ekieffer/jupyter_sing/2169124/jupyter_data/notebook_secret
[W 13:51:20.007 NotebookApp] Notebook HPC school 2020.ipynb is not trusted
[I 13:51:20.361 NotebookApp] Kernel started: 8961291f-7e23-4b9f-b382-7e90b4e5c9b7, name: python3
[I 13:53:15.702 NotebookApp] Starting buffering for 8961291f-7e23-4b9f-b382-7e90b4e5c9b7:f77c8f489d3542548b1a4d6a83d28e52

Jupyter provides you a token to connect to the notebook.

Note that the number of cpus reported by python itself will be 28 (regular nodes)
This is obvisouly wrong. Check  $SLURM_CPUS_PER_TASK
Never use os.cpu_count() nor multiprocessing.cpu_count()


Step 2: Jupyter + custom Kernels

One can wonder WHY we want to work with venv in a singularity image:

You do not need to rebuild the container evreytime you want to install a new package
We will first apply some changes in our singularity definition file to activate the virtual environement automatically when calling a command. For this purpose, we will use the %runscript header now.
We built a new sif image (jupyter.sif) with the following jupyter.def definition:
Bootstrap: library
From: ubuntu:18.04
Stage: build

%setup
    touch /file_on_host
    touch ${SINGULARITY_ROOTFS}/file_on_guest

%files
    /file_on_host /opt

%environment
    export PORT=8889
    export LC_ALL=C

%post
    apt-get update
    apt-get upgrade -y
    apt-get install -y software-properties-common
    add-apt-repository multiverse
    apt-get install -y python3 python3-pip python3-venv
    python3 -m pip install jupyter cgroup-utils

%runscript
    VENV=$1
    echo "Sourcing $VENV"
    shift
    exec bash -c "source $VENV/bin/activate;$@"

%startscript
    echo "Started new instance on $(date)"

%test
    grep -q NAME=\"Ubuntu\" /etc/os-release
    if [ $? -eq 0 ]; then
        echo "Container base is Ubuntu as expected."
    else
        echo "Container base is not Ubuntu."
    fi
    python3 -m pip show jupyter


%labels
    Author ekieffer
    Version v0.0.1

%help
    This is a demo container used to illustrate a def file that uses all
    supported sections.
On your laptop: sudo singularity build jupyter.sif jupter.def
#!/bin/bash -l
#SBATCH -J Singularity_Jupyter_venv
#SBATCH -N 1 # Nodes
#SBATCH -n 1 # Tasks
#SBATCH -c 2 # Cores assigned to each tasks
#SBATCH --time=0-01:00:00
#SBATCH -p batch
#SBATCH --qos=normal
#SBATCH --mail-user=<firstname>.<lastname>@uni.lu
#SBATCH --mail-type=BEGIN,END



module load tools/Singularity
export VENV="$HOME/.envs/venv"
export JUPYTER_CONFIG_DIR="$HOME/jupyter_sing/$SLURM_JOBID/"
export JUPYTER_PATH="$VENV/share/jupyter":"$HOME/jupyter_sing/$SLURM_JOBID/jupyter_path"
export JUPYTER_DATA_DIR="$HOME/jupyter_sing/$SLURM_JOBID/jupyter_data"
export JUPYTER_RUNTIME_DIR="$HOME/jupyter_sing/$SLURM_JOBID/jupyter_runtime"
export IPYTHONDIR="$HOME/ipython_sing/$SLURM_JOBID"

mkdir -p $JUPYTER_CONFIG_DIR
mkdir -p $IPYTHONDIR

export IP_ADDRESS=$(hostname -I | awk '{print $1}')


echo "On your laptop: ssh -p -J ${USER}@access-${ULHPC_CLUSTER}.uni.lu ${USER}@<node-name> -NL  8889:localhost:8889" 
echo "Replace <node-name> with the compute node name received by slurm. Command squeue -u ${USER} to see it. Example: "aion-0085" or "iris-0112".

singularity instance start jupyter.sif jupyter

if [ ! -d "$VENV" ];then
    singularity exec instance://jupyter python3 -m venv $VENV --system-site-packages
    # singularity run instance://jupyter $VENV "python3 -m pip install <your_packages>"
    singularity run instance://jupyter $VENV "python3 -m ipykernel install --sys-prefix --name HPC_SCHOOL_ENV --display-name HPC_SCHOOL_ENV"

fi



singularity run instance://jupyter  $VENV "jupyter notebook --ip localhost --no-browser --port 8889" &
pid=$!
sleep 5s
singularity run instance://jupyter $VENV "jupyter notebook list"
singularity run instance://jupyter $VENV "jupyter --paths"
singularity run instance://jupyter $VENV "jupyter kernelspec list"
wait $pid
echo "Stopping instance"
singularity instance stop jupyter
Step 3: Jupyter + custom Kernels + IPyParallel

The installation of the IPyParallel package should be done during the image generation
Copy-paste the jupyter.def to jupyter_parallel.def
Add in the %post section, python3 -m pip install ipyparallel
#!/bin/bash -l
#SBATCH -J Singularity_Jupyter_parallel
#SBATCH -N 2 # Nodes
#SBATCH -n 10 # Tasks
#SBATCH -c 2 # Cores assigned to each tasks
#SBATCH --time=0-01:00:00
#SBATCH -p batch
#SBATCH --qos=normal
#SBATCH --mail-user=<firstname>.<lastname>@uni.lu
#SBATCH --mail-type=BEGIN,END



module load tools/Singularity

export VENV="$HOME/.envs/venv_parallel_${ULHPC_CLUSTER}"
export JUPYTER_CONFIG_DIR="$HOME/jupyter_sing/$SLURM_JOBID/"
export JUPYTER_PATH="$VENV/share/jupyter":"$HOME/jupyter_sing/$SLURM_JOBID/jupyter_path"
export JUPYTER_DATA_DIR="$HOME/jupyter_sing/$SLURM_JOBID/jupyter_data"
export JUPYTER_RUNTIME_DIR="$HOME/jupyter_sing/$SLURM_JOBID/jupyter_runtime"
export IPYTHONDIR="$HOME/ipython_sing/$SLURM_JOBID"

mkdir -p $JUPYTER_CONFIG_DIR
mkdir -p $IPYTHONDIR


export IP_ADDRESS=$(hostname -I | awk '{print $1}')
export XDG_RUNTIME_DIR=""

#create a new ipython profile appended with the job id number
profile=job_${SLURM_JOB_ID}

JUPYTER_SRUN="srun -w $(hostname) --exclusive -N 1 -n 1 -c 1 "
IPCONTROLLER_SRUN="srun -w $(hostname) --exclusive -N 1 -n 1 -c 1 "
IPENGINES_SRUN="srun --exclusive -n $((${SLURM_NTASKS}-2)) -c ${SLURM_CPUS_PER_TASK} "


echo "On your laptop: ssh -p 8022 ${USER}@access-${ULHPC_CLUSTER}.uni.lu <node-name> -NL 8889:localhost:8889"
echo "Replace <node-name> with the compute node name received by slurm. Command squeue -u ${USER} to see it. Example: "aion-0085" or "iris-0112".

echo "
if [ ! -d "$VENV" ];then
    ${JUPYTER_SRUN} -J "JUP: Create venv" singularity exec jupyter_parallel.sif python3 -m venv $VENV --system-site-packages
    # singularity run jupyter_parallel.sif $VENV "python3 -m pip install <your_packages>"
    ${JUPYTER_SRUN} -J "JUP: Install ipykernel" singularity run jupyter_parallel.sif $VENV "python3 -m ipykernel install --sys-prefix --name HPC_SCHOOL_ENV_IPYPARALLEL --display-name HPC_SCHOOL_ENV_IPYPARALLEL"

fi

${JUPYTER_SRUN} -J "JUP: Create profile" singularity run jupyter_parallel.sif $VENV "ipython profile create --parallel ${profile}"

# Enable IPython clusters tab in Jupyter notebook
${JUPYTER_SRUN} -J "JUP: add ipy ext" singularity run jupyter_parallel.sif $VENV "jupyter nbextension enable --py ipyparallel"

${JUPYTER_SRUN} -J "JUP: Start jupyter notebook" singularity run jupyter_parallel.sif $VENV "jupyter notebook --ip localhost --no-browser --port 8889" &

sleep 5s
${JUPYTER_SRUN}  -J "JUP: Get notebook list" singularity run jupyter_parallel.sif $VENV "jupyter notebook list"
${JUPYTER_SRUN} -J "JUP: Get jupyter paths info" singularity run jupyter_parallel.sif $VENV "jupyter --paths"
${JUPYTER_SRUN} -J "JUP: Get jupyter kernels" singularity run jupyter_parallel.sif $VENV "jupyter kernelspec list"



## Ipyparallel for ditributed notebook

## Start Controller and Engines
${IPCONTROLLER_SRUN} -J "IPCONTROLLER" singularity run jupyter_parallel.sif $VENV "ipcontroller --ip="*" --profile=${profile}" &
sleep 10

##srun: runs ipengine on each available core
${IPENGINES_SRUN} -J "IPENGINES" singularity run jupyter_parallel.sif $VENV "ipengine --profile=${profile} --location=$(hostname)" &
sleep 25

wait
You can check that all steps have been completed or are running using sacct -j job_number.
When opening the job log (slurm-job_number.out), you should see when the engines start:
On your laptop: ssh -p 8022 -J ekieffer@access-iris.uni.lu ekieffer@iris-0057 -NL 8889:localhost:8889 
INFO:    instance started successfully
Sourcing /home/users/ekieffer/.envs/venv_parallel
Installed kernelspec HPC_SCHOOL_ENV_IPYPARALLEL in /home/users/ekieffer/.envs/venv_parallel/share/jupyter/kernels/hpc_school_env_ipyparallel
Sourcing /home/users/ekieffer/.envs/venv_parallel
[ProfileCreate] Generating default config file: &apos;/home/users/ekieffer/ipython_sing/2134685/profile_job_2134685/ipython_config.py&apos;
[ProfileCreate] Generating default config file: &apos;/home/users/ekieffer/ipython_sing/2134685/profile_job_2134685/ipython_kernel_config.py&apos;
[ProfileCreate] Generating default config file: &apos;/home/users/ekieffer/ipython_sing/2134685/profile_job_2134685/ipcontroller_config.py&apos;
[ProfileCreate] Generating default config file: &apos;/home/users/ekieffer/ipython_sing/2134685/profile_job_2134685/ipengine_config.py&apos;
[ProfileCreate] Generating default config file: &apos;/home/users/ekieffer/ipython_sing/2134685/profile_job_2134685/ipcluster_config.py&apos;
Sourcing /home/users/ekieffer/.envs/venv_parallel
Enabling tree extension ipyparallel/main...
      - Validating: <font color="#4E9A06">OK</font>
Sourcing /home/users/ekieffer/.envs/venv_parallel
2020-11-13 15:29:52.292 [IPControllerApp] Hub listening on tcp://*:49933 for registration.
2020-11-13 15:29:52.294 [IPControllerApp] Hub using DB backend: &apos;DictDB&apos;
2020-11-13 15:29:52.574 [IPControllerApp] hub::created hub
2020-11-13 15:29:52.575 [IPControllerApp] writing connection info to /home/users/ekieffer/ipython_sing/2134685/profile_job_2134685/security/ipcontroller-client.json
2020-11-13 15:29:52.576 [IPControllerApp] writing connection info to /home/users/ekieffer/ipython_sing/2134685/profile_job_2134685/security/ipcontroller-engine.json
2020-11-13 15:29:52.578 [IPControllerApp] task::using Python leastload Task scheduler
2020-11-13 15:29:52.578 [IPControllerApp] Heartmonitor started
2020-11-13 15:29:52.590 [IPControllerApp] Creating pid file: /home/users/ekieffer/ipython_sing/2134685/profile_job_2134685/pid/ipcontroller.pid
2020-11-13 15:29:52.601 [scheduler] Scheduler started [leastload]
2020-11-13 15:29:52.604 [IPControllerApp] client::client b&apos;\x00k\x8bEg&apos; requested &apos;connection_request&apos;
2020-11-13 15:29:52.604 [IPControllerApp] client::client [b&apos;\x00k\x8bEg&apos;] connected
Sourcing /home/users/ekieffer/.envs/venv_parallel
Sourcing /home/users/ekieffer/.envs/venv_parallel
2020-11-13 15:30:02.960 [IPEngineApp] Loading url_file &apos;/home/users/ekieffer/ipython_sing/2134685/profile_job_2134685/security/ipcontroller-engine.json&apos;
2020-11-13 15:30:02.980 [IPEngineApp] Registering with controller at tcp://127.0.0.1:49933
2020-11-13 15:30:02.989 [IPControllerApp] client::client b&apos;2ac54d70-5ec5cb4f47a0ff21c199b7e9&apos; requested &apos;registration_request&apos;
2020-11-13 15:30:03.082 [IPEngineApp] Starting to monitor the heartbeat signal from the hub every 3010 ms.
2020-11-13 15:30:03.088 [IPEngineApp] Completed registration with id 0
2020-11-13 15:30:04.160 [IPEngineApp] Loading url_file &apos;/home/users/ekieffer/ipython_sing/2134685/profile_job_2134685/security/ipcontroller-engine.json&apos;
2020-11-13 15:30:04.188 [IPEngineApp] Registering with controller at tcp://172.17.6.57:49933
2020-11-13 15:30:04.199 [IPControllerApp] client::client b&apos;e7a7b917-dd93682af4c3649148a0b60c&apos; requested &apos;registration_request&apos;
2020-11-13 15:30:04.295 [IPEngineApp] Starting to monitor the heartbeat signal from the hub every 3010 ms.
2020-11-13 15:30:04.301 [IPEngineApp] Completed registration with id 1
2020-11-13 15:30:07.582 [IPControllerApp] registration::finished registering engine 1:e7a7b917-dd93682af4c3649148a0b60c
2020-11-13 15:30:07.583 [IPControllerApp] engine::Engine Connected: 1
2020-11-13 15:30:07.586 [IPControllerApp] registration::finished registering engine 0:2ac54d70-5ec5cb4f47a0ff21c199b7e9
2020-11-13 15:30:07.587 [IPControllerApp] engine::Engine Connected: 0
Sourcing /home/users/ekieffer/.envs/venv_parallel
[I 15:30:27.510 NotebookApp] Writing notebook server cookie secret to /home/users/ekieffer/jupyter_sing/2134685/jupyter_runtime/notebook_cookie_secret
[I 15:30:27.772 NotebookApp] Loading IPython parallel extension
[I 15:30:27.773 NotebookApp] Serving notebooks from local directory: /home/users/ekieffer/singularity_tests
[I 15:30:27.773 NotebookApp] Jupyter Notebook 6.1.5 is running at:
[I 15:30:27.774 NotebookApp] http://172.17.6.57:8889/?token=15c128acd6eee2d4f8e1d1561fe11ab8dc6f6d2b730a7cfe
[I 15:30:27.774 NotebookApp]  or http://127.0.0.1:8889/?token=15c128acd6eee2d4f8e1d1561fe11ab8dc6f6d2b730a7cfe
[I 15:30:27.774 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[C 15:30:27.780 NotebookApp]

    To access the notebook, open this file in a browser:
        file:///home/users/ekieffer/jupyter_sing/2134685/jupyter_runtime/nbserver-100-open.html
    Or copy and paste one of these URLs:
        http://172.17.6.57:8889/?token=15c128acd6eee2d4f8e1d1561fe11ab8dc6f6d2b730a7cfe
     or http://127.0.0.1:8889/?token=15c128acd6eee2d4f8e1d1561fe11ab8dc6f6d2b730a7cfe
Sourcing /home/users/ekieffer/.envs/venv_parallel
Currently running servers:
http://172.17.6.57:8889/?token=15c128acd6eee2d4f8e1d1561fe11ab8dc6f6d2b730a7cfe :: /home/users/ekieffer/singularity_tests
Sourcing /home/users/ekieffer/.envs/venv_parallel
config:
    /home/users/ekieffer/jupyter_sing/2134685/
    /usr/etc/jupyter
    /usr/local/etc/jupyter
    /etc/jupyter
data:
    /home/users/ekieffer/.envs/venv_parallel/share/jupyter
    /home/users/ekieffer/jupyter_sing/2134685/jupyter_path
    /home/users/ekieffer/jupyter_sing/2134685/jupyter_data
    /usr/local/share/jupyter
    /usr/share/jupyter
runtime:
    /home/users/ekieffer/jupyter_sing/2134685/jupyter_runtime
Sourcing /home/users/ekieffer/.envs/venv_parallel
Available kernels:
  hpc_school_env_ipyparallel    /home/users/ekieffer/.envs/venv_parallel/share/jupyter/kernels/hpc_school_env_ipyparallel
  python3                       /usr/local/share/jupyter/kernels/python3


Step 4: Jupyter + custom Kernels + CUDA

In this last section, we will use a GPU-enabled container from NVIDIA:
Only need to change the Boostrap and From sections
NVIDIA containers can be obtained from DockerHub
For more details, please have a look at NVIDIA
Tensorflow GPU-enabled container will be consider hereafter
Bootstrap: docker
From: nvcr.io/nvidia/tensorflow:20.10-tf2-py3
Stage: build

%setup
    touch /file_on_host
    touch ${SINGULARITY_ROOTFS}/file_on_guest

%files
    /file_on_host /opt

%environment
    export PORT=8889
    export LC_ALL=C

%post
    apt-get update
    apt-get upgrade -y
    python3 -m pip install virtualenv jupyter ipyparallel cgroup-utils

%runscript
    VENV=$1
    echo "Sourcing $VENV"
    shift
    exec bash -c "source $VENV/bin/activate;$@"

%startscript
    echo "Started new instance on $(date)"

%test
    grep -q NAME=\"Ubuntu\" /etc/os-release
    if [ $? -eq 0 ]; then
        echo "Container base is Ubuntu as expected."
    else
        echo "Container base is not Ubuntu."
    fi
    python3 -m pip show jupyter

%labels
    Author ekieffer
    Version v0.0.1

%help
    This is a demo container used to illustrate a def file that uses all
    supported sections.
#!/bin/bash -l
#SBATCH -J Singularity_Jupyter_parallel_cuda
#SBATCH -N 1 # Nodes
#SBATCH -n 1 # Tasks
#SBATCH -c 4 # Cores assigned to each tasks
#SBATCH --time=0-01:00:00
#SBATCH -p gpu
#SBATCH -G 1
#SBATCH --qos=normal
#SBATCH --mail-user=<firstname>.<lastname>@uni.lu
#SBATCH --mail-type=BEGIN,END



module load tools/Singularity

export VENV="$HOME/.envs/venv_cuda_${ULHPC_CLUSTER}"
export JUPYTER_CONFIG_DIR="$HOME/jupyter_sing/$SLURM_JOBID/"
export JUPYTER_PATH="$VENV/share/jupyter":"$HOME/jupyter_sing/$SLURM_JOBID/jupyter_path"
export JUPYTER_DATA_DIR="$HOME/jupyter_sing/$SLURM_JOBID/jupyter_data"
export JUPYTER_RUNTIME_DIR="$HOME/jupyter_sing/$SLURM_JOBID/jupyter_runtime"
export IPYTHONDIR="$HOME/ipython_sing/$SLURM_JOBID"

mkdir -p $JUPYTER_CONFIG_DIR
mkdir -p $IPYTHONDIR

export IP_ADDRESS=$(hostname -I | awk '{print $1}')
export XDG_RUNTIME_DIR=""

#create a new ipython profile appended with the job id number
profile=job_${SLURM_JOB_ID}


echo "On your laptop: ssh -p 8022 ${USER}@access-${ULHPC_CLUSTER}.uni.lu ${USER}@<node-name> -NL 8889:localhost:8889" 
echo "Replace <node-name> with the compute node name received by slurm. Command squeue -u ${USER} to see it. Example: "aion-0085" or "iris-0112".

if [ ! -d "$VENV" ];then
    # For some reasons, there is an issue with venv -- using virtualenv instead
    singularity exec --nv jupyter_parallel_cuda.sif python3 -m virtualenv $VENV --system-site-packages
    singularity run --nv jupyter_parallel_cuda.sif $VENV "python3 -m pip install --upgrade pip" 
    # singularity run --nv jupyter_parallel.sif $VENV "python3 -m pip install <your_packages>"
    singularity run --nv jupyter_parallel_cuda.sif $VENV "python3 -m ipykernel install --sys-prefix --name HPC_SCHOOL_ENV_CUDA --display-name HPC_SCHOOL_ENV_CUDA"

fi

singularity run --nv jupyter_parallel_cuda.sif $VENV "ipython profile create --parallel ${profile}"

# Enable IPython clusters tab in Jupyter notebook
singularity run --nv jupyter_parallel_cuda.sif $VENV "jupyter nbextension enable --py ipyparallel"

singularity run --nv jupyter_parallel_cuda.sif $VENV "jupyter notebook --ip localhost --no-browser --port 8889" &
sleep 5s
singularity run --nv jupyter_parallel_cuda.sif $VENV "jupyter notebook list"
singularity run --nv jupyter_parallel_cuda.sif $VENV "jupyter --paths"
singularity run --nv jupyter_parallel_cuda.sif $VENV "jupyter kernelspec list"

wait


The notebook has now access to nvidia-smi command and the tensorflow library confirms that we have access to the GPU device 0
What's next ?

If you feel confortable now with Singularity, you can then follow the tutorial on Singularity with Infiniband which shows how to configure singularity containers to use Infiniband.

UL HPC Tutorial: Singularity with Infiniband

 Copyright (c) 2018-2021 UL HPC Team <hpc-team@uni.lu>


Singularity setups

Please refer to the Singularity introduction tutorial for the setups.

Singularity with Infiniband

Objectives: (through this example we get the singularity configuration for running over Infiniband, tested on Iris cluster)
Create a container with the required Infiniband libraries on Ubuntu18.04
Install KerA (stream storage, similar to Apache Kafka)
Install DFI (data flow interface over RDMA see https://doi.org/10.1145/3448016.3452816)
Run a distributed KerA (1 coordinator and 2 brokers)
Configuration validated on the Iris cluster
Step 1: Required software

Install a VM, user zetta, with Ubuntu 18.04 having docker and singularity installed.
Git clone KerA from https://gitlab.uni.lu/ocm/kera.git and follow Step 1 from https://gitlab.uni.lu/ocm/kera.git (git submodule update --init --recursive)
The kera project contains a Dockerfile and its docker-entry-point.sh
Step 2: Create the docker container

Clean and create the kera docker that will be later used by singularity
sudo docker system prune -a
#Update kera/GNUmakefile JAVAHOME variable, set to JAVAHOME := /root/.sdkman/candidates/java/current
sudo docker build . --tag kera
The Dockerfile contains steps to install required libraries for compiling KerA and DFI, including the right Mellanox driver for Iris
# ##############################################################################
# 
# Builder stage
#
# ##############################################################################


FROM ubuntu:18.04 AS builder

ARG JAVA_VERSION=11.0.2-open
ENV SDKMAN_DIR=/root/.sdkman

WORKDIR /opt
COPY . /opt/kera

#
# Install dependencies
#

RUN apt-get update \ 
 &&  DEBIAN_FRONTEND="noninteractive" apt-get -y install tzdata

RUN apt-get update \
 && apt-get install --yes \
      apt-transport-https \
      build-essential \
      apt-utils \
      ca-certificates \
      curl \
      doxygen \
      g++ \
      gdb \
      git \
      libboost-filesystem-dev \
      libboost-program-options-dev \
      libboost-system-dev \
      libibverbs-dev \
      libpcre++-dev \
      libssl-dev \
      libzookeeper-mt-dev \
      procps \
      protobuf-compiler \
      python3 \
      python3-pip \
      software-properties-common \
      unzip \
      wget \
      zip \
      gcc \
      make \
      perl \
      dkms \
      linux-headers-$(uname -r) \
      gnupg \
      lsb-release \
      libprotobuf-dev \
      libcrypto++-dev \
      libevent-dev \
      libboost-all-dev \
      libpcre3-dev \
      libgtest-dev \
      zookeeper \
      tk \
      libnl-3-dev \
      udev \
      tcl \
      libnl-route-3-dev \
      bison \
      flex \
      libmnl0 \
      gfortran \
      libgfortran4 \
      cmake libtool pkg-config autoconf automake libzmq3-dev libgtest-dev libnuma-dev libcppunit-dev numactl libaio-dev libevent-dev \
 && rm -rf /var/lib/apt/lists/*

RUN cd /usr/src/gtest && cmake CMakeLists.txt && make && cp *.a /usr/lib

RUN cd kera \
 && mnt/MLNX_OFED_LINUX-5.1-2.5.8.0-ubuntu18.04-x86_64/mlnxofedinstall  --skip-unsupported-devices-check --without-dkms --add-kernel-support --kernel 5.4.0-42-generic --kernel-sources /usr/src/linux-headers-5.4.0-42-generic/ --without-fw-update --force

RUN cd kera/DFI \
 && mkdir release \
 && cd release/ \
 && cmake .. -DCMAKE_BUILD_TYPE=Release \
 && make \
 && make install


#
# Install Java
#

RUN curl -s "https://get.sdkman.io" | bash \
 && echo "sdkman_auto_answer=true" > $SDKMAN_DIR/etc/config \
 && echo "sdkman_auto_selfupdate=false" >> $SDKMAN_DIR/etc/config

# Source sdkman to make the sdk command available and install java candidate
RUN bash -c "source $SDKMAN_DIR/bin/sdkman-init.sh && sdk install java $JAVA_VERSION"

# Add candidate path to $PATH environment variable
ENV JAVA_HOME="$SDKMAN_DIR/candidates/java/current"
ENV PATH="$JAVA_HOME/bin:$PATH"

#
# Install cmake
#

RUN curl -sSL https://cmake.org/files/v3.11/cmake-3.11.1-Linux-x86_64.tar.gz | tar -xzC . \
 && mv cmake-3.11.1-Linux-x86_64 cmake

ENV PATH="/opt/cmake/bin/:$PATH"

#
# Compile KerArrow
#

RUN cd /opt/kera/kerarrow \
 && mkdir -p cpp/release \
 && cd cpp/release/ \
 && cmake .. -DCMAKE_BUILD_TYPE=Release -DARROW_PLASMA=on \
 && make -j12 \
 && make install 


#
# Compile KerA
#


RUN cd kera \
 && make clean \
 && make -j12 DEBUG=no INFINIBAND=yes \
 && make install


# ##############################################################################
# 
# Final stage
# No JDK is included in the final image.
#
# ##############################################################################

FROM ubuntu:18.04

#
# Installing binaries
#

COPY --from=builder \
 /opt/kera/install/bin /opt/kera/install/bin

COPY --from=builder \
 /etc/init.d/* /etc/init.d/

COPY --from=builder \
 /usr/local/bin/plasma_store \
 /opt/kera/install/bin/

#
# Installing libraries
#

COPY --from=builder \
 /opt/kera/install/lib/kera/* \
 /opt/kera/install/lib/kera/

COPY --from=builder \
 /etc/* /etc/

COPY --from=builder \
 /usr/lib/x86_64-linux-gnu/libibverbs/libmlx5-rdmav25.so /usr/lib/x86_64-linux-gnu/libibverbs/

COPY --from=builder \
 /opt/kera/install/lib/kera/lib* \
 /opt/kera/kerarrow/cpp/release/release/lib* \
 /usr/local/lib/

COPY --from=builder \
 /usr/lib/x86_64-linux-gnu/libzookeeper* \
 /usr/lib/x86_64-linux-gnu/libboost* \
 /usr/lib/x86_64-linux-gnu/libprotobuf* \
 /usr/lib/x86_64-linux-gnu/libpcre* \
 /usr/lib/x86_64-linux-gnu/libibverbs* \
 /usr/lib/x86_64-linux-gnu/libssl* \
 /usr/lib/x86_64-linux-gnu/libcrypto* \
 /usr/lib/x86_64-linux-gnu/lib* \
 /usr/lib/x86_64-linux-gnu/

COPY --from=builder \
 /lib/x86_64-linux-gnu/* /lib/x86_64-linux-gnu/

COPY --from=builder \
 /usr/include/infiniband/* /usr/include/infiniband/

COPY --from=builder \
 /usr/include/* /usr/include/

COPY --from=builder \
 /lib/modules/* /lib/modules/

COPY --from=builder \
 /usr/local/lib/lib* /usr/local/lib/

COPY --from=builder \
 /usr/local/include/dfi /usr/local/include/dfi

ENV LD_LIBRARY_PATH="/usr/local/lib/:$LD_LIBRARY_PATH"

RUN ldd /usr/local/lib/libkera.so \
 && ldd /opt/kera/install/bin/coordinator \
 && ldd /opt/kera/install/bin/server \
 && ldd /usr/local/lib/libdfi.so

COPY ./docker-entrypoint.sh /
ENTRYPOINT ["/docker-entrypoint.sh"]
The docker-entrypoint.sh will later be used when running the singularity container on the Iris cluster.
#!/bin/bash

set -e


case "$1" in
    sh|bash)
        set -- "$@"
        exec "$@"
    ;;
    coordinator)
        shift
        echo "Running coordinator on `hostname` ${SLURM_PROCID}"
        /opt/kera/install/bin/coordinator "$@" 1>$HOME/coordinator.out 2>&1 &
        status=$?
        if [ $status -ne 0 ]; then
            echo "Failed to start KerA coordinator: $status"
            exit $status
        fi

        exec tail -f $(ls -Art $HOME/coordinator.out | tail -n 1)
    ;;
    broker)
        if [[ ${SLURM_PROCID} -eq 0 ]]; then
         echo "Doing nothing `hostname` $HOSTNAME"
        else
         echo "Installing on `hostname` $HOSTNAME"
        # The plasma store creates the /tmp/plasma socket on startup
        /opt/kera/install/bin/plasma_store -m 1000000000 -s /tmp/plasma 1>$HOME/plasma-${SLURM_PROCID}.out 2>&1 &
        status=$?
        if [ $status -ne 0 ]; then
            echo "Failed to start KerA server: $status"
            exit $status
        fi

        sleep 2

        shift
        /opt/kera/install/bin/server "$@" 1>$HOME/server-${SLURM_PROCID}.out 2>&1 &
        status=$?
        if [ $status -ne 0 ]; then
            echo "Failed to start plasma: $status"
            exit $status
        fi

        exec tail -f $(ls -Art $HOME/plasma-${SLURM_PROCID}.out | tail -n 1) & tail -f $(ls -Art $HOME/server-${SLURM_PROCID}.out | tail -n 1)

        fi
    ;;
esac

Step 3: Create the singularity container

Either directly create the kera.sif singularity container, or use a sandbox to eventually modify/add before exporting to sif format
#directly create a singularity container
sudo singularity build kera.sif docker-daemon://kera:latest
#create the sandbox directory from existing docker kera container, then create the kera.sif
sudo singularity build --sandbox keraUbuntu1804 docker-daemon://kera:latest
sudo singularity build kera.sif keraUbuntu1804/
Step 4: Create a script to install KerA on Iris

The following script runKerA.sh runs singularity kera.sif container for creating 1 coordinator and 2 brokers, each on a container instance
#!/bin/bash -l
#SBATCH -J Singularity_KerA_Coord
#SBATCH -N 3 # Nodes
#SBATCH -n 3 # Tasks
#SBATCH --ntasks-per-node=1
#SBATCH --mem=9GB
#SBATCH -c 3 # Cores assigned to each task
#SBATCH --time=0-00:15:00
#SBATCH -p batch
#SBATCH --qos=normal
#SBATCH --mail-user=firstname.lastname@uni.lu
#SBATCH --mail-type=BEGIN,END

module load tools/Singularity

hostName="$(hostname -s)-ib0"
IP=$(getent hosts $hostName | awk '{print $1}')
echo "On your laptop: ssh -p 8022 -NL 8889:$hostName:8889 ${USER}@access-iris.uni.lu"

echo "SLURM_JOBID  = ${SLURM_JOBID}"
echo "SLURM_JOB_NODELIST = ${SLURM_JOB_NODELIST}"
echo "SLURM_NNODES = ${SLURM_NNODES}"
echo "SLURM_NTASK  = ${SLURM_NTASKS}"
echo "Submission directory = ${SLURM_SUBMIT_DIR}"

export KERA_WORKER_CORES=${SLURM_CPUS_PER_TASK:-1}

echo "Cores: $KERA_WORKER_CORES"

export DAEMON_MEM=${SLURM_MEM_PER_CPU:=2048}
export KERA_MEM=$(( ${DAEMON_MEM}*${KERA_WORKER_CORES} ))

export KERA_MASTER_HOST=$(hostname -s)

export NWLOCATOR="infrc:host"

#srun --exclusive -N 1 -n 1 -l -o $HOME/coordout \ 
 singularity run --bind /dev/infiniband,/etc/libibverbs.d  kera.sif \
 coordinator -C $NWLOCATOR=${hostName},port=11100,dev=mlx5_0 --maxCores ${SLURM_CPUS_PER_TASK:-2} -n --reset --clusterName test &

pid=$!
sleep 10s

echo "Starting brokers"

KERA_BROKER_LAUNCHER=${HOME}/kera-start-brokers-${SLURM_JOBID}.sh
echo " - create broker launcher script '${KERA_BROKER_LAUNCHER}'"
cat << 'EOF' > ${KERA_BROKER_LAUNCHER}
#!/bin/bash

echo "I am ${SLURM_PROCID} running on:"
hostname

KERA_WORKER_CORES=${SLURM_CPUS_PER_TASK:-1}

echo "Cores: $KERA_WORKER_CORES"

DAEMON_MEM=${SLURM_MEM_PER_CPU:=2048}
KERA_MEM=$(( ${DAEMON_MEM}*${KERA_WORKER_CORES} ))

echo "memory: $KERA_MEM"

KERA_WORKER_HOST="`hostname`-ib0"
echo ${KERA_WORKER_HOST}


# --bind /opt/mellanox,/sys/class/infiniband \
#  --bind /sys/class/infiniband_cm,/sys/class/infiniband_mad \
#  --bind /sys/class/infiniband_verbs

singularity run --bind /dev/infiniband,/etc/libibverbs.d kera.sif \
 broker -L $NWLOCATOR=${KERA_WORKER_HOST},port=11101,dev=mlx5_0 --totalMasterMemory ${KERA_MEM} \
 -C $NWLOCATOR=$1,port=11100,dev=mlx5_0 --cleanerBalancer fixed:50 -D -d --detectFailures 0 -h 1 \
 -f /tmp/storagemaster1 --maxCores ${SLURM_CPUS_PER_TASK:-2} --clusterName test -r 0 --masterOnly \
 --numberActiveGroupsPerStreamlet 1 --masterActiveGroupsPerStreamlet 1

EOF
chmod +x ${KERA_BROKER_LAUNCHER}

# Start the KerA brokers; pass coordinator hostname as param  service ; for srun: --bind /etc/init.d/
srun --exclusive -N 3 -n 3 --ntasks-per-node=1 -l -o $HOME/broker-$(hostname -s).out \
 ${KERA_BROKER_LAUNCHER} ${hostName} &

sleep 900s
wait $pid

echo $HOME

echo "Ready Stopping instance"

Now you can install KerA on Iris. Singularity run will execute the docker-entrypoint.sh which is configured to run on every slurm task except the one running the coordinator.
Login to Iris
sbatch runKerA.sh
Step 5: How the output looks

The Coordinator (coordinator.out)
1629490163.314847961 CoordinatorMain.cc:110 in main NOTICE[1]: Command line: /opt/kera/install/bin/coordinator -C infrc:host=iris-079-ib0,port=11100,dev=mlx5_0 --maxCores 3 -n --reset --clusterName test
1629490163.314866331 CoordinatorMain.cc:111 in main NOTICE[1]: Coordinator process id: 23390
1629490163.422875345 Infiniband.h:106 in DeviceList WARNING[1]: identified infiniband device: mlx5_0
1629490163.422888057 Infiniband.h:117 in lookup WARNING[1]: looking to open infiniband device: mlx5_0 searching mlx5_0
1629490163.442540847 InfRcTransport.cc:263 in InfRcTransport NOTICE[1]: InfRc listening on UDP: 172.19.6.79:11100
1629490163.444650651 InfRcTransport.cc:272 in InfRcTransport NOTICE[1]: Local Infiniband lid is 108
1629490163.621857932 CoordinatorMain.cc:122 in main NOTICE[1]: coordinator: Listening on infrc:host=iris-079-ib0,port=11100,dev=mlx5_0
1629490163.621874294 CoordinatorMain.cc:125 in main NOTICE[1]: PortTimeOut=-1
1629490163.621876873 PortAlarm.cc:174 in setPortTimeout NOTICE[1]: Set PortTimeout to -1 (ms: -1 to disable.)
1629490163.621883782 CoordinatorMain.cc:146 in main WARNING[1]: Reset requested: deleting external storage for workspace '/ramcloud/test'
1629490163.621891380 CoordinatorMain.cc:151 in main NOTICE[1]: Cluster name is 'test', external storage workspace is '/ramcloud/test/'
1629490163.623065380 CoordinatorClusterClock.cc:170 in recoverClusterTime WARNING[1]: couldn't find "coordinatorClusterClock" object in external storage; starting new clock from zero; benign if starting new cluster from scratch, may cause linearizability failures otherwise
1629490163.623074634 CoordinatorClusterClock.cc:176 in recoverClusterTime NOTICE[1]: initializing CoordinatorClusterClock: startingClusterTime = 0
1629490163.624238702 CoordinatorUpdateManager.cc:82 in init WARNING[7]: couldn't find "coordinatorUpdateManager" object in external storage; starting new cluster from scratch
1629490163.625497673 CoordinatorServerList.cc:412 in recover NOTICE[7]: CoordinatorServerList recovery completed: 0 master(s), 0 backup(s), 0 update(s) to disseminate, server list version is 0
1629490163.625508879 TableManager.cc:808 in recover NOTICE[7]: Table recovery complete: 0 table(s)
1629490163.625516433 CoordinatorService.cc:125 in init NOTICE[7]: Coordinator state has been recovered from external storage; starting service
1629490163.627406243 MemoryMonitor.cc:76 in handleTimerEvent NOTICE[8]: Memory usage now 676 MB (increased 676 MB)
1629490180.817172953 CoordinatorServerList.cc:160 in enlistServer NOTICE[5]: Enlisting server at infrc:host=iris-080-ib0,port=11101,dev=mlx5_0 (server id 1.0) supporting services: MASTER_SERVICE, ADMIN_SERVICE
1629490181.047367269 CoordinatorServerList.cc:160 in enlistServer NOTICE[5]: Enlisting server at infrc:host=iris-082-ib0,port=11101,dev=mlx5_0 (server id 2.0) supporting services: MASTER_SERVICE, ADMIN_SERVICE

The Brokers (server-1.out, server-2.out)
1629490175.584982556 ServerMain.cc:289 in main NOTICE[1]: Command line: /opt/kera/install/bin/server -L infrc:host=iris-080-ib0,port=11101,dev=mlx5_0 --totalMasterMemory 6144 -C infrc:host=iris-079-ib0,port=11100,dev=mlx5_0 --cleanerBalancer fixed:50 -D -d --detectFailures 0 -h 1 -f /tmp/storagemaster1 --maxCores 3 --clusterName test -r 0 --masterOnly --numberActiveGroupsPerStreamlet 1 --masterActiveGroupsPerStreamlet 1
1629490175.585006261 ServerMain.cc:290 in main NOTICE[1]: Server process id: 22528
1629490175.633273359 Infiniband.h:106 in DeviceList WARNING[1]: identified infiniband device: mlx5_0
1629490175.633290399 Infiniband.h:117 in lookup WARNING[1]: looking to open infiniband device: mlx5_0 searching mlx5_0
1629490175.650759076 InfRcTransport.cc:263 in InfRcTransport NOTICE[1]: InfRc listening on UDP: 172.19.6.80:11101
1629490175.652109813 InfRcTransport.cc:272 in InfRcTransport NOTICE[1]: Local Infiniband lid is 110
1629490175.824960501 ServerMain.cc:319 in main NOTICE[1]: MASTER_SERVICE, ADMIN_SERVICE: Listening on infrc:host=iris-080-ib0,port=11101,dev=mlx5_0
1629490175.825434143 ServerMain.cc:360 in main NOTICE[1]: Using 0 backups
1629490175.825442948 ServerConfig.h:619 in setLogAndHashTableSize NOTICE[1]: Master to allocate 6442450944 bytes total, 1048576 of which are for the hash table
1629490175.825444554 ServerConfig.h:621 in setLogAndHashTableSize NOTICE[1]: Master will have 767 segments and 16384 lines in the hash table
1629490175.825445520 ServerConfig.h:625 in setLogAndHashTableSize NOTICE[1]: Hash table will have one entry for every 49144 bytes in the log
1629490175.825460967 ServerMain.cc:365 in main NOTICE[1]: PortTimeOut=-1
1629490175.825462037 PortAlarm.cc:174 in setPortTimeout NOTICE[1]: Set PortTimeout to -1 (ms: -1 to disable.)
1629490175.825756452 MemoryMonitor.cc:76 in handleTimerEvent NOTICE[4]: Memory usage now 647 MB (increased 647 MB)
1629490175.825781222 Server.cc:101 in run NOTICE[1]: Starting services
1629490175.825795738 Server.cc:165 in createAndRegisterServices NOTICE[1]: Starting master service
1629490175.825796613 Server.cc:166 in createAndRegisterServices NOTICE[1]: Master is using 0 backups
1629490175.825849161 LargeBlockOfMemory.h:255 in mmapGigabyteAligned NOTICE[1]: Populating pages; progress 0 of 6143 MB
1629490176.442748331 LargeBlockOfMemory.h:255 in mmapGigabyteAligned NOTICE[1]: Populating pages; progress 1024 of 6143 MB
1629490177.050605836 LargeBlockOfMemory.h:255 in mmapGigabyteAligned NOTICE[1]: Populating pages; progress 2048 of 6143 MB
1629490177.636708533 LargeBlockOfMemory.h:255 in mmapGigabyteAligned NOTICE[1]: Populating pages; progress 3072 of 6143 MB
1629490178.230278663 LargeBlockOfMemory.h:255 in mmapGigabyteAligned NOTICE[1]: Populating pages; progress 4096 of 6143 MB
1629490178.811673036 LargeBlockOfMemory.h:255 in mmapGigabyteAligned NOTICE[1]: Populating pages; progress 5120 of 6143 MB
1629490179.396368849 SegletAllocator.cc:171 in initializeEmergencyHeadReserve NOTICE[1]: Reserved 2 seglets for emergency head segments (16 MB). 765 seglets (6120 MB) left in default pool.
1629490179.657004084 InfRcTransport.h:118 in registerMemory NOTICE[1]: Registered 6441402368 bytes at 0x40000000
1629490179.657069138 LargeBlockOfMemory.h:255 in mmapGigabyteAligned NOTICE[1]: Populating pages; progress 0 of 1 MB
1629490179.658002890 SegletAllocator.cc:206 in initializeCleanerReserve NOTICE[1]: Reserved 1 seglets for the cleaner (8 MB). 764 seglets (6112 MB) left in default pool.
1629490179.658010446 LogCleaner.cc:898 in FixedBalancer NOTICE[1]: Using fixed balancer with 50% disk cleaning
1629490179.659099100 MultiFileStorage.cc:1063 in MultiFileStorage NOTICE[1]: Backup storage opened with 4294967296 bytes available; allocated 512 frame(s) across 1 file(s) with 8388608 bytes per frame
1629490179.740875053 BackupStorage.cc:82 in benchmark NOTICE[1]: Backup storage speeds (min): 1251 MB/s read
1629490179.740897688 BackupStorage.cc:83 in benchmark NOTICE[1]: Backup storage speeds (avg): 1613 MB/s read,
1629490179.740898718 BackupStorage.cc:89 in benchmark NOTICE[1]: RANDOM_REFINE_AVG BackupStrategy selected
1629490179.740931574 MultiFileStorage.cc:1556 in tryLoadSuperblock NOTICE[1]: Stored superblock had a bad checksum: stored checksum was 0, but stored data had checksum 88a5c087
1629490179.740935386 MultiFileStorage.cc:1556 in tryLoadSuperblock NOTICE[1]: Stored superblock had a bad checksum: stored checksum was 0, but stored data had checksum 88a5c087
1629490179.740936381 MultiFileStorage.cc:1342 in loadSuperblock WARNING[1]: Backup couldn't find existing superblock; starting as fresh backup.
1629490179.740939997 PersistenceManager.cc:88 in PersistenceManager NOTICE[1]: Backup storing replicas with clusterName 'test'. Future backups must be restarted with the same clusterName for replicas stored on this backup to be reused.
1629490179.740941776 PersistenceManager.cc:102 in PersistenceManager NOTICE[1]: Replicas stored on disk have a different clusterName ('__unnamed__'). Scribbling storage to ensure any stale replicas left behind by old backups aren't used by future backups
socket() suceeded for pathname /tmp/plasma
Socket pathname is ok.
socket_fd: 11
1629490179.888345368 Server.cc:168 in createAndRegisterServices NOTICE[1]: Master service started
1629490179.888356298 Server.cc:180 in createAndRegisterServices NOTICE[1]: Starting admin service
1629490179.888359239 Server.cc:184 in createAndRegisterServices NOTICE[1]: Admin service started
1629490179.888360194 Server.cc:103 in run NOTICE[1]: Services started
1629490179.888361061 Server.cc:108 in run NOTICE[1]: Pinning memory
1629490180.794964742 Server.cc:110 in run NOTICE[1]: Memory pinned
1629490180.795133039 MemoryMonitor.cc:76 in handleTimerEvent NOTICE[4]: Memory usage now 7844 MB (increased 7197 MB)
1629490180.795160786 Server.cc:211 in enlist NOTICE[4]: Enlisting with coordinator
1629490180.813952484 CoordinatorSession.cc:119 in getSession NOTICE[4]: Opened session with coordinator at infrc:host=iris-079-ib0,port=11100,dev=mlx5_0
1629490180.814215878 Server.cc:218 in enlist NOTICE[4]: Enlisted; serverId 1.0
socket() suceeded for pathname /tmp/plasma
Socket pathname is ok.
socket_fd: 12
1629490180.814895298 Server.cc:229 in enlist NOTICE[4]: Created objectServerId 1 ? 1
1629490180.814918048 MasterService.cc:935 in initOnceEnlisted NOTICE[4]: My server ID is 1.0
1629490180.814923772 PersistenceManager.cc:150 in initOnceEnlisted NOTICE[4]: pm My server ID is 1.0
1629490180.821292852 ServerList.cc:200 in applyServerList NOTICE[7]: Server 1.0 is up (server list version 1)
1629490180.836741479 PersistenceManager.cc:156 in initOnceEnlisted NOTICE[4]: PersistenceManager 1.0 will store replicas under cluster name 'test'
1629490181.044582504 ServerList.cc:200 in applyServerList NOTICE[7]: Server 2.0 is up (server list version 2)

1629490175.568762266 ServerMain.cc:289 in main NOTICE[1]: Command line: /opt/kera/install/bin/server -L infrc:host=iris-082-ib0,port=11101,dev=mlx5_0 --totalMasterMemory 6144 -C infrc:host=iris-079-ib0,port=11100,dev=mlx5_0 --cleanerBalancer fixed:50 -D -d --detectFailures 0 -h 1 -f /tmp/storagemaster1 --maxCores 3 --clusterName test -r 0 --masterOnly --numberActiveGroupsPerStreamlet 1 --masterActiveGroupsPerStreamlet 1
1629490175.568805199 ServerMain.cc:290 in main NOTICE[1]: Server process id: 4420
1629490175.635965138 Infiniband.h:106 in DeviceList WARNING[1]: identified infiniband device: mlx5_0
1629490175.635992010 Infiniband.h:117 in lookup WARNING[1]: looking to open infiniband device: mlx5_0 searching mlx5_0
1629490175.656650536 InfRcTransport.cc:263 in InfRcTransport NOTICE[1]: InfRc listening on UDP: 172.19.6.82:11101
1629490175.657721727 InfRcTransport.cc:272 in InfRcTransport NOTICE[1]: Local Infiniband lid is 112
1629490175.926597137 ServerMain.cc:319 in main NOTICE[1]: MASTER_SERVICE, ADMIN_SERVICE: Listening on infrc:host=iris-082-ib0,port=11101,dev=mlx5_0
1629490175.927065771 ServerMain.cc:360 in main NOTICE[1]: Using 0 backups
1629490175.927074036 ServerConfig.h:619 in setLogAndHashTableSize NOTICE[1]: Master to allocate 6442450944 bytes total, 1048576 of which are for the hash table
1629490175.927075508 ServerConfig.h:621 in setLogAndHashTableSize NOTICE[1]: Master will have 767 segments and 16384 lines in the hash table
1629490175.927076437 ServerConfig.h:625 in setLogAndHashTableSize NOTICE[1]: Hash table will have one entry for every 49144 bytes in the log
1629490175.927092323 ServerMain.cc:365 in main NOTICE[1]: PortTimeOut=-1
1629490175.927093398 PortAlarm.cc:174 in setPortTimeout NOTICE[1]: Set PortTimeout to -1 (ms: -1 to disable.)
1629490175.930640196 MemoryMonitor.cc:76 in handleTimerEvent NOTICE[5]: Memory usage now 647 MB (increased 647 MB)
1629490175.930684453 Server.cc:101 in run NOTICE[1]: Starting services
1629490175.930690202 Server.cc:165 in createAndRegisterServices NOTICE[1]: Starting master service
1629490175.930691081 Server.cc:166 in createAndRegisterServices NOTICE[1]: Master is using 0 backups
1629490175.930716322 LargeBlockOfMemory.h:255 in mmapGigabyteAligned NOTICE[1]: Populating pages; progress 0 of 6143 MB
1629490176.512783068 LargeBlockOfMemory.h:255 in mmapGigabyteAligned NOTICE[1]: Populating pages; progress 1024 of 6143 MB
1629490177.123410103 LargeBlockOfMemory.h:255 in mmapGigabyteAligned NOTICE[1]: Populating pages; progress 2048 of 6143 MB
1629490177.728711052 LargeBlockOfMemory.h:255 in mmapGigabyteAligned NOTICE[1]: Populating pages; progress 3072 of 6143 MB
1629490178.323984649 LargeBlockOfMemory.h:255 in mmapGigabyteAligned NOTICE[1]: Populating pages; progress 4096 of 6143 MB
1629490178.923948175 LargeBlockOfMemory.h:255 in mmapGigabyteAligned NOTICE[1]: Populating pages; progress 5120 of 6143 MB
1629490179.526665376 SegletAllocator.cc:171 in initializeEmergencyHeadReserve NOTICE[1]: Reserved 2 seglets for emergency head segments (16 MB). 765 seglets (6120 MB) left in default pool.
1629490179.771735859 InfRcTransport.h:118 in registerMemory NOTICE[1]: Registered 6441402368 bytes at 0x40000000
1629490179.771841667 LargeBlockOfMemory.h:255 in mmapGigabyteAligned NOTICE[1]: Populating pages; progress 0 of 1 MB
1629490179.773289133 SegletAllocator.cc:206 in initializeCleanerReserve NOTICE[1]: Reserved 1 seglets for the cleaner (8 MB). 764 seglets (6112 MB) left in default pool.
1629490179.773299815 LogCleaner.cc:898 in FixedBalancer NOTICE[1]: Using fixed balancer with 50% disk cleaning
1629490179.774910848 MultiFileStorage.cc:1063 in MultiFileStorage NOTICE[1]: Backup storage opened with 4294967296 bytes available; allocated 512 frame(s) across 1 file(s) with 8388608 bytes per frame
1629490179.873532112 BackupStorage.cc:82 in benchmark NOTICE[1]: Backup storage speeds (min): 1006 MB/s read
1629490179.873542937 BackupStorage.cc:83 in benchmark NOTICE[1]: Backup storage speeds (avg): 1335 MB/s read,
1629490179.873543851 BackupStorage.cc:89 in benchmark NOTICE[1]: RANDOM_REFINE_AVG BackupStrategy selected
1629490179.873580991 MultiFileStorage.cc:1556 in tryLoadSuperblock NOTICE[1]: Stored superblock had a bad checksum: stored checksum was 0, but stored data had checksum 88a5c087
1629490179.873585051 MultiFileStorage.cc:1556 in tryLoadSuperblock NOTICE[1]: Stored superblock had a bad checksum: stored checksum was 0, but stored data had checksum 88a5c087
1629490179.873586077 MultiFileStorage.cc:1342 in loadSuperblock WARNING[1]: Backup couldn't find existing superblock; starting as fresh backup.
1629490179.873588877 PersistenceManager.cc:88 in PersistenceManager NOTICE[1]: Backup storing replicas with clusterName 'test'. Future backups must be restarted with the same clusterName for replicas stored on this backup to be reused.
1629490179.873590437 PersistenceManager.cc:102 in PersistenceManager NOTICE[1]: Replicas stored on disk have a different clusterName ('__unnamed__'). Scribbling storage to ensure any stale replicas left behind by old backups aren't used by future backups
socket() suceeded for pathname /tmp/plasma
Socket pathname is ok.
socket_fd: 11
1629490180.006412927 Server.cc:168 in createAndRegisterServices NOTICE[1]: Master service started
1629490180.006427969 Server.cc:180 in createAndRegisterServices NOTICE[1]: Starting admin service
1629490180.006431221 Server.cc:184 in createAndRegisterServices NOTICE[1]: Admin service started
1629490180.006432209 Server.cc:103 in run NOTICE[1]: Services started
1629490180.006433238 Server.cc:108 in run NOTICE[1]: Pinning memory
1629490181.028210772 Server.cc:110 in run NOTICE[1]: Memory pinned
1629490181.028383832 MemoryMonitor.cc:76 in handleTimerEvent NOTICE[5]: Memory usage now 7844 MB (increased 7197 MB)
1629490181.028405111 Server.cc:211 in enlist NOTICE[5]: Enlisting with coordinator
1629490181.044703448 CoordinatorSession.cc:119 in getSession NOTICE[5]: Opened session with coordinator at infrc:host=iris-079-ib0,port=11100,dev=mlx5_0
1629490181.044828091 Server.cc:218 in enlist NOTICE[5]: Enlisted; serverId 2.0
socket() suceeded for pathname /tmp/plasma
Socket pathname is ok.
socket_fd: 12
1629490181.045516643 Server.cc:229 in enlist NOTICE[5]: Created objectServerId 2 ? 1
1629490181.045539241 MasterService.cc:935 in initOnceEnlisted NOTICE[5]: My server ID is 2.0
1629490181.045545091 PersistenceManager.cc:150 in initOnceEnlisted NOTICE[5]: pm My server ID is 2.0
1629490181.046144290 PersistenceManager.cc:156 in initOnceEnlisted NOTICE[5]: PersistenceManager 2.0 will store replicas under cluster name 'test'
1629490181.051408793 ServerList.cc:200 in applyServerList NOTICE[6]: Server 1.0 is up (server list version 2)
1629490181.051427966 ServerList.cc:200 in applyServerList NOTICE[6]: Server 2.0 is up (server list

-- mode: markdown; mode: visual-line; fill-column: 80 --

Copyright (c) 2014-2017 UL HPC Team hpc-sysadmins@uni.lu

UL HPC Tutorial: Create and reproduce work environments using Vagrant

By ULHPC Licence GitHub issues Github Documentation Status GitHub forks

/!\ IMPORTANT Up-to-date instructions for Vagrant can be found in the "Reproducible Research at the Cloud Era" Tutorial. Below instructions are probably outdated but kept for archive purposes.

Vagrant is a tool that allows to easily and rapidly create and configure reproducible and portable work environments using Virtual Machines. This is especially useful if you want to test your work in a stable and controlled environment and minimize the various unintended or untrackable changes that may occur on a physical machine.

In this tutorial, we are going to explain the steps to install Vagrant and create your first basic Linux Virtual Machine with it.

Vagrant installation

Prerequisite:

Vagrant can use many Virtual Machine providers such as VirtualBox, VMware and Docker with VirtualBox being the easiest to use, and the default option in Vagrant.

Our first step is to install VirtualBox, you can download and install the correct version for your operating system from the official website. In many Linux distributions it is provided as a package from the standard repositories, thus you can use your usual package manager to install it.

Once this prerequisite is met, we can install Vagrant. Download the correct version for your operating system on the official website and install it.

Using Vagrant to create a Virtual Machine

The main advantage of Vagrant is that it lets you import and use pre-configured Virtual Machines (called boxes in this context) which can become bases for your own customizations (installed applications, libraries, etc). With Vagrant it becomes really fast and effortless to create and run a new Virtual Machine.

The Vagrant boxes contain the disk image of a VM without the virtual hardware details of the VM, which are initialized by Vagrant and can be edited by the user.

The first step is to choose a pre-configured box to use. It is possible to create your own from scratch yet this is not in the scope of the current tutorial. Freely available boxes can be found at the following two main sources:

Atlas corps box catalog
vagrantbox.es catalaog
The first catalog is the default box download location for Vagrant. This means that you can directly use the name of the boxes you find here with Vagrant (e.g. ubuntu/trusty64). To use the second catalog you would additionaly need to provide the source box URL, yet this catalog provides a much richer variety of boxes.

Adding a new box

To add a box and make it usable in Vagrant, we are going to use the vagrant box add command. In the example below we will add one box from each of the catalogs in order to present the different possibilities. We are going to add the ubuntu/trusty64 box from the Atlas catalog and the Ubuntu 14.04 box (by its url) from the vagrantbox.es catalog.

To add the first box, we use the following command (which may take some time due to the time needed to download the box):

    $> vagrant box add ubuntu/trusty64
    ==> box: Loading metadata for box 'ubuntu/trusty64'
        box: URL: https://vagrantcloud.com/ubuntu/trusty64
    ==> box: Adding box 'ubuntu/trusty64' (v14.04) for provider: virtualbox
        box: Downloading: https://vagrantcloud.com/ubuntu/boxes/trusty64/versions/14.04/providers/virtualbox.box
    ==> box: Successfully added box 'ubuntu/trusty64' (v14.04) for 'virtualbox'!
In this case, you just had to give the name of the box and Vagrant found the box by itself and added the box under the ubuntu/trusty64 name.

To list the local boxes available to Vagrant for initialization of new VMs, we use the vagrant box list command:

    $> vagrant box list
    ubuntu/trusty64    (virtualbox, 14.04)
To add the second box, you need to use a slightly different syntax since you need to precise the name you want to give to the box as well as its source URL:

    $> vagrant box add ubuntu14.04 https://github.com/kraksoft/vagrant-box-ubuntu/releases/download/14.04/ubuntu-14.04-amd64.box
    ==> box: Adding box 'ubuntu14.04' (v0) for provider:
        box: Downloading: https://github.com/kraksoft/vagrant-box-ubuntu/releases/download/14.04/ubuntu-14.04-amd64.box
    ==> box: Successfully added box 'ubuntu14.04' (v0) for 'virtualbox'!
Now a second box will be available to Vagrant under the name ubuntu14.04:

    $> vagrant box list
    ubuntu/trusty64    (virtualbox, 14.04)
    ubuntu14.04        (virtualbox, 0)
In the rest of the tutorial we are only going to use the first box. To remove a box we use the vagrant box remove command as follows:

    $> vagrant box remove ubuntu14.04
    Removing box 'ubuntu14.04' (v0) with provider 'virtualbox'...
Checking that it has been removed:

    $> vagran box list
    ubuntu/trusty64    (virtualbox, 14.04)
Creating a new Virtual Machine

Now we are going to create a new Virtual Machine using the ubuntu/trusty64 box. We will initialize it in an empty directory (which is not absolutely mandatory):

    $> mkdir vagrant && cd vagrant
Next, we make Vagrant prepare the configuration file describing the VM:

    $> vagrant init ubuntu/trusty64
    A `Vagrantfile` has been placed in this directory. You are now
    ready to `vagrant up` your first virtual environment! Please read
    the comments in the Vagrantfile as well as documentation on
    `vagrantup.com` for more information on using Vagrant.
You should now see a file named Vagrantfile in your directory. This file contains the minimal information for Vagrant to launch the VM. We could modify it to set up specific parameters of the VM (number of virtual cores, memory size, etc), but this constitutes advanced usage for which full documentation that can be found on the official site. However, it may be interesting to understand what is actually needed in this file, since it contains a lot of commented information. The minimal content of a Vagrantfile is as follows:

    VAGRANTFILE_API_VERSION = "2"
    Vagrant.configure("VAGRANTFILE_API_VERSION") do |config|
        config.vm.box = "hashicorp/trusty64"
    end
This basically defines which version of the Vagrant API will be used to build the VM using the box given as a base.

Now, to launch the VM you only need to use the single vagrant up command in the same directory where the Vagrantfile exists (this may take some time since Vagrant is going to boot the VM and set its basic configuration):

    $> vagrant up
    Bringing machine 'default' up with 'virtualbox' provider...
    ==> default: Importing base box 'ubuntu/trusty64'...
    ==> default: Matching MAC address for NAT networking...
    ==> default: Checking if box 'ubuntu/trusty64' is up to date...
    ==> default: Setting the name of the VM: vagrant_default_1425476252413_67101
    ==> default: Clearing any previously set forwarded ports...
    ==> default: Clearing any previously set network interfaces...
    ==> default: Preparing network interfaces based on configuration...
        default: Adapter 1: nat
    ==> default: Forwarding ports...
        default: 22 => 2222 (adapter 1)
    ==> default: Booting VM...
    ==> default: Waiting for machine to boot. This may take a few minutes...
        default: SSH address: 127.0.0.1:2222
        default: SSH username: vagrant
        default: SSH auth method: private key
        default: Warning: Connection timeout. Retrying...
        default: Warning: Remote connection disconnect. Retrying...
    ==> default: Machine booted and ready!
    ==> default: Checking for guest additions in VM...
    ==> default: Mounting shared folders...
        default: /vagrant => /tmp/vagrant
Your VM is now up and running at this point. To access it, use the vagrant ssh command within the same directory :

    $> vagrant ssh
You should now be connected to your VM and ready to work.

An interesting feature of Vagrant is that your computer (the "host") shares the directory that contains the Vagrantfile with your VM (the "guest"), where it is seen as /vagrant.

Assuming you have a script or data files you want to access from within the VM, you simply put them in the same directory as the Vagrantfile and then use them in the VM under /vagrant. The reverse is also true.

To learn more than the basics covered in this tutorial, we encourage you to refer to the official documentation.

Tensorflow/Keras programming for IPU

 Copyright (c) 2024 P. Pochelu and UL HPC Team <hpc-team@uni.lu>
This programming tutorial for IPU is designed for developers looking to accelerate training and evaluation of their model, tabular and computer vision.

Performance difference

The image below illustrates the difference of GPU (Tesla released in 2018) and IPU (released in 2017):

IPU GPU comparison

Experimental settings:

20 neural networks from keras.applications package with 1 and 32 batch size values. The identical code was executed on both IPU and GPU. The speedup ratio of the IPU over the GPU is depicted on the horizontal axis, and the arithmetic intensity (FLOPS per parameters) is the vertical axis. Each data point on the plot is labeled with the corresponding neural network architecture code name along with its associated batch size. For example, "eff1_32" corresponds to EfficientNetB1 with a batch size of 32. It is important to note that both axes are represented on a logarithmic scale.

In overall, training all those tasks one-by-one during 1 epoch takes 3h14 on GPU, 1h06 minutes on IPU.

Installation ML frameworks:

Local wheels in the ULHPC graphcore1 server: ~/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/

Tensorflow:

Tensorflow for AMD EPYC CPU: pip install ~/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/tensorflow-2.6.3+gc3.0.0+236842+d084e493702+amd_znver1-cp38-cp38-linux_x86_64.whl --no-index
Keras front-end: pip install ~/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/keras-2.6.0+gc3.0.0+236851+1744557f-py2.py3-none-any.whl --no-index
ipu addons for Tensorflow pip install ~/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/ipu_tensorflow_addons-2.6.3+gc3.0.0+236851+2e46901-py3-none-any.whl --no-index
JAX: pip install jax==0.3.16+ipu jaxlib==0.3.15+ipu.sdk300 -f https://graphcore-research.github.io/jax-experimental/wheels.html
HuggingFace: pip install 'transformers @ https://github.com/graphcore/transformers-fork@v4.18-gc#egg=transformers'
PyTorch for IPU (named "PopTorch"): https://docs.graphcore.ai/projects/pytorch-quick-start/en/latest/quick-start-beginners.html?highlight=poptorch-3.0.0%2B86945_163b7ce462_ubuntu_20_04-cp38-cp38-linux_x86_64.whl#install-the-poptorch-wheel-and-validate
Check your ML framework installation for ipu:

After installing Jax and Tensorflow you should see your favorite frameworks and the associated IPU addons.

JAX on IPU packages:

jax                      0.3.16+ipu       
jax-jumpy                1.0.0            
jaxlib                   0.3.15+ipu.sdk300
Tensorflow on IPU packages:

tensorflow               2.6.3
ipu-tensorflow-addons    2.6.3
Poptorch on IPU packages:

poptorch                 3.0.0+86945
torch                    1.10.0+cpu       
torchvision              0.11.1 
Tensorflow IPU code

Begin by importing Keras and TensorFlow. We'll encapsulate our training/testing loop within the 'scope' object. This object facilitates the definition of how AI accelerators (GPU or IPU) are utilized and sets the storage strategy for the model's parameters.

from tensorflow import keras
import tensorflow as tf
import numpy as np

scope = tf.distribute.get_strategy().scope # https://www.tensorflow.org/guide/distributed_training

Run the code below only if you use IPU. This is the only code difference between IPU and GPU implementation.

##############################IPU CONTEXT###############################
from tensorflow.python import ipu

# do not directly use tensorflow.compiler but ipu_compiler if the below application is Tensorflow (and not Keras).
from tensorflow.python.ipu import ipu_compiler as compiler

# Below code is detailed in:
# https://github.com/graphcore/tensorflow/blob/r2.6/sdk-release-3.2/tensorflow/python/ipu/config.py
cfg = ipu.config.IPUConfig()  # Create the IPU hardware configure
cfg.auto_select_ipus = 1  # Attach one IPU to the current process (or MPI rank)
# TODO: other settings include FP32, FP16, ...
cfg.configure_ipu_system()  # Running hardware configuration IPU
######################################################################

Define the main hyperparameter and deep learning architecture

BATCH_SIZE=2
IMG_SIZE=224
NUM_EPOCHS=3

def get_model():
    input_layer = tf.keras.layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3),batch_size=BATCH_SIZE)

    x = tf.keras.applications.ResNet50(weights=None, include_top=False, classes=10)(input_layer)
    x = tf.keras.layers.GlobalAveragePooling2D()(x)
    x = keras.layers.Flatten()(x)
    x = keras.layers.Dense(10, activation='softmax')(x)
    model = keras.Model(inputs=input_layer, outputs=x)

    return model
Read the data

Read the data, scale, and sub-samples the dataset to accelerate the demo

##############################
#### READ THE DATA      ######
###############################
keras.backend.set_image_data_format("channels_last")

# Reading raw images
(trainImages, trainLabels), (
    testImages,
    testLabels,
) = keras.datasets.cifar10.load_data()

# FOR DEBUGGING PURPOSE
MAX_IMG=1000
trainImages = trainImages[:MAX_IMG]
trainLabels = trainLabels[:MAX_IMG]
testImages = testImages[:MAX_IMG]
testLabels = testLabels[:MAX_IMG]


# Preprocessing data from [0;255] to [0;1.0]
trainImages = trainImages.astype(np.float32) / 255.0
testImages = testImages.astype(np.float32) / 255.0
trainLabels = trainLabels.astype(np.int32)
testLabels = testLabels.astype(np.int32)

# Selection of images. The nunber of images should be multiple of the batch size, otherwise remainding images are ignored.
training_images = int(
    (len(trainImages) // BATCH_SIZE) * BATCH_SIZE
)  # Force all steps to be the same size
testing_images = int((len(testImages) // BATCH_SIZE) * BATCH_SIZE)
trainImages = trainImages[:training_images]
trainLabels = trainLabels[:training_images]
testImages = testImages[:testing_images]
testLabels = testLabels[:testing_images]

Efficient data access

Design efficient Tensorflow I/O pipeline through asynchronous process between training and resizing images.

#  Some Keras versions display wrong alert messages "INVALID_ARGUMENT" . Please ignore them
train_dataset = tf.data.Dataset.from_tensor_slices(
    (tf.cast(trainImages, tf.float32), tf.cast(trainLabels, tf.float32))
)

eval_dataset = tf.data.Dataset.from_tensor_slices(
    (tf.cast(testImages, tf.float32), tf.cast(testLabels, tf.float32))
)
xy_train_gen = (
    train_dataset.shuffle(len(trainImages))
    .batch(BATCH_SIZE, drop_remainder=True)
    .map(lambda image, label: (tf.image.resize(image, (IMG_SIZE, IMG_SIZE)), label))
    .prefetch(tf.data.AUTOTUNE)
)
xy_test_gen = (
    eval_dataset.shuffle(len(testImages))
    .batch(BATCH_SIZE, drop_remainder=True)
    .map(lambda image, label: (tf.image.resize(image, (IMG_SIZE, IMG_SIZE)), label))
    .prefetch(tf.data.AUTOTUNE)
)

Buiding, training, evaluating it

######################################
##### KERAS MODEL DEFINITION #########
######################################
keras.backend.set_image_data_format("channels_last")

with scope():
  model=get_model()

  model.summary()

  # Call the Adam optimizer
  if hasattr(tf.keras.optimizers, "legacy"):  # Last TF2 version
      optimizer = tf.keras.optimizers.legacy.Adam(0.01)
  else:  # Older TF2 version
      optimizer = tf.keras.optimizers.Adam(0.01)

  # Compute the number of steps
  train_steps_per_exec = len(trainImages) // BATCH_SIZE
  eval_steps_per_exec = len(testImages) // BATCH_SIZE

  ############
  # TRAINING #
  ############
  # Keras computing graph construction. Plugs together : the model, the loss, the optimizer
  model.compile(
      optimizer=optimizer,
      loss="sparse_categorical_crossentropy",
      steps_per_execution=train_steps_per_exec
  )

  model.fit(
      xy_train_gen,
      epochs=NUM_EPOCHS,
      batch_size=BATCH_SIZE
  )

  ###############
  # EVALUATING #
  ###############
  model.compile(
      metrics=["accuracy"],
      loss="sparse_categorical_crossentropy",
      steps_per_execution=eval_steps_per_exec,
  )

  (loss, accuracy) = model.evaluate(xy_test_gen, batch_size=BATCH_SIZE)

  print(f"test loss: {round(loss, 4)}, test acc: {round(accuracy, 4)}")

The output is:

2024-02-08 09:35:38.053169: I tensorflow/compiler/plugin/poplar/driver/poplar_platform.cc:43] Poplar version: 3.0.0 (fa83d31c56) Poplar package: 1e179b3b85
2024-02-08 09:35:39.753460: I tensorflow/compiler/plugin/poplar/driver/poplar_executor.cc:1619] TensorFlow device /device:IPU:0 attached to 1 IPU with Poplar device ID: 0

Model: "model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(2, 224, 224, 3)]        0         
_________________________________________________________________
resnet50 (Functional)        (None, None, None, 2048)  23587712  
_________________________________________________________________
global_average_pooling2d (Gl (2, 2048)                 0         
_________________________________________________________________
flatten (Flatten)            (2, 2048)                 0         
_________________________________________________________________
dense (Dense)                (2, 10)                   20490     
=================================================================
Total params: 23,608,202
Trainable params: 23,555,082
Non-trainable params: 53,120
_________________________________________________________________
2024-02-08 09:35:41.557916: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
Epoch 1/3
2024-02-08 09:35:45.304415: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
Compiling module a_inference_train_function_18311__XlaMustCompile_true_config_proto___n_007_n_0...02_001_000__executor_type____.19234:
[##################################################] 100% Compilation Finished [Elapsed: 00:04:04.2]
2024-02-08 09:39:53.086734: I tensorflow/compiler/jit/xla_compilation_cache.cc:376] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
50/50 [==============================] - 255s 5s/step - loss: 12.2200
Epoch 2/3
50/50 [==============================] - 2s 32ms/step - loss: 2.8860
Epoch 3/3
50/50 [==============================] - 1s 23ms/step - loss: 2.5464
Compiling module a_inference_test_function_22828__XlaMustCompile_true_config_proto___n_007_n_0...02_001_000__executor_type____.2593:
[##################################################] 100% Compilation Finished [Elapsed: 00:01:01.2]
50/50 [==============================] - 65s 1s/step - loss: 51.4884 - accuracy: 0.0800
test loss: 51.4884, test acc: 0.08
Please note the presence of the line below:

[##################################################] 100% Compilation Finished

which show the successful compilation of the computing graph on IPU. This line is not present when ran on GPU.

Monitoring IPU activity

Don't forget gc-monitor and check that our process is actually running on the IPU. Illustration below:

ipuuser@graphcore1:~$ gc-monitor
+---------------+---------------------------------------------------------------------------------+
|  gc-monitor   |                Partition: p [active] has 16 reconfigurable IPUs                 |
+-------------+--------------------+--------+--------------+----------+-------+----+------+-------+
|    IPU-M    |       Serial       |IPU-M SW|Server version|  ICU FW  | Type  | ID | IPU# |Routing|
+-------------+--------------------+--------+--------------+----------+-------+----+------+-------+
|10.44.44.162 | 0011.0002.8222721  | 2.6.0  |    1.11.0    |  2.5.9   | M2000 | 0  |  3   |  DNC  |
|10.44.44.162 | 0011.0002.8222721  | 2.6.0  |    1.11.0    |  2.5.9   | M2000 | 1  |  2   |  DNC  |
|10.44.44.162 | 0011.0001.8222721  | 2.6.0  |    1.11.0    |  2.5.9   | M2000 | 2  |  1   |  DNC  |
|10.44.44.162 | 0011.0001.8222721  | 2.6.0  |    1.11.0    |  2.5.9   | M2000 | 3  |  0   |  DNC  |
+-------------+--------------------+--------+--------------+----------+-------+----+------+-------+
|10.44.44.130 | 0022.0002.8222721  | 2.6.0  |    1.11.0    |  2.5.9   | M2000 | 4  |  3   |  DNC  |
|10.44.44.130 | 0022.0002.8222721  | 2.6.0  |    1.11.0    |  2.5.9   | M2000 | 5  |  2   |  DNC  |
|10.44.44.130 | 0022.0001.8222721  | 2.6.0  |    1.11.0    |  2.5.9   | M2000 | 6  |  1   |  DNC  |
|10.44.44.130 | 0022.0001.8222721  | 2.6.0  |    1.11.0    |  2.5.9   | M2000 | 7  |  0   |  DNC  |
+-------------+--------------------+--------+--------------+----------+-------+----+------+-------+
|10.44.44.226 | 0029.0002.8222721  | 2.6.0  |    1.11.0    |  2.5.9   | M2000 | 8  |  3   |  DNC  |
|10.44.44.226 | 0029.0002.8222721  | 2.6.0  |    1.11.0    |  2.5.9   | M2000 | 9  |  2   |  DNC  |
|10.44.44.226 | 0029.0001.8222721  | 2.6.0  |    1.11.0    |  2.5.9   | M2000 | 10 |  1   |  DNC  |
|10.44.44.226 | 0029.0001.8222721  | 2.6.0  |    1.11.0    |  2.5.9   | M2000 | 11 |  0   |  DNC  |
+-------------+--------------------+--------+--------------+----------+-------+----+------+-------+
|10.44.44.194 | 0003.0002.8222721  | 2.6.0  |    1.11.0    |  2.5.9   | M2000 | 12 |  3   |  DNC  |
|10.44.44.194 | 0003.0002.8222721  | 2.6.0  |    1.11.0    |  2.5.9   | M2000 | 13 |  2   |  DNC  |
|10.44.44.194 | 0003.0001.8222721  | 2.6.0  |    1.11.0    |  2.5.9   | M2000 | 14 |  1   |  DNC  |
|10.44.44.194 | 0003.0001.8222721  | 2.6.0  |    1.11.0    |  2.5.9   | M2000 | 15 |  0   |  DNC  |
+-------------+--------------------+--------+--------------+----------+-------+----+------+-------+
+-------------------------------------------------------------------------------------------------------------------------------------------+------------------------+-----------------+
|                                                     Attached processes in partition p                                                     |          IPU           |      Board      |
+--------+------------------------------------------------------------------------------------------------------------+--------+------------+----+----------+--------+--------+--------+
|  PID   |                                                  Command                                                   |  Time  |    User    | ID |  Clock   |  Temp  |  Temp  | Power  |
+--------+------------------------------------------------------------------------------------------------------------+--------+------------+----+----------+--------+--------+--------+
|1653623 |                                                  python3                                                   |  40s   |  ipuuser   | 0  | 1850MHz  | 36.3 C | 30.4 C |130.7 W |
+--------+------------------------------------------------------------------------------------------------------------+--------+------------+----+----------+--------+--------+--------+
ipuuser@graphcore1:~$
Going further

Don't use Tensorflow for Graph Neural Network, use instead PopTorch. The performance are either very slow (https://keras.io/examples/graph/gnn_citations/) or the processing process is stuck (https://graphneural.network/layers/convolution/)
Switching beween Multi-IPU and Multi-GPU. Multi-IPU is backed by Popdist and Multi-GPU Horovod : https://github.com/PierrickPochelu/IPU_GPU_running_context

-- mode: markdown; mode: visual-line; --

Tutorial "Reproducible Research at the Cloud Era"

By svarrette LicenceGitHub Project GitHub issues Documentation Status Build Status GitHub forks

This tutorial is actually not part of the UL HPC Tutorial but it was prepared by S. Varrette and given during the IEEE CloudCom 2016 conference.



For more information: http://rr-tutorials.readthedocs.io/

The documentation for these tutorials is handled by Read the Docs, a web service dedicated to documentation management for the open source community.

Reference documentation
By default, the ULHPC/tutorials repository is bound to the ulhpc-tutorials project on Read the Docs.

You might wish to generate locally the docs:

Install mkdocs
Preview your documentation from the project root by running mkdocs serve and visite with your favorite browser the URL http://localhost:8000
build the full documentation locally by running mkdocs build to create the site/ directory.
If you want to know more of mkdocs and readthedocs (may be for your own projects), you can check this blog post

Proposing a new tutorial / Contributing to this repository

You're using a specific software on the UL HPC platform not listed in the above list? Then most probably you

developed a set of script to effectively run that software
used to face issues such that you're aware (eventually unconsciously) of tricks and tips for that specific usage.
Then your inputs are valuable for the other users and we would appreciate your help to complete this repository with new topics/entries.

To do that, the general approach is similar to the one proposed by Github via the Forking procedure. Since we use git-flow, your workflow for contributing to this repository should typically involve the following steps:

Fork it
Initialize your local copy of the repository (including git submodules etc.): make setup
Create your feature branch: git flow feature start <feature_name>
Commit your changes: git commit -am 'Added some feature'
Publish your feature branch: git flow feature publish <feature_name>
Create new Pull Request
More details are provided below.

git-flow

The Git branching model for this repository follows the guidelines of gitflow. In particular, the central repo (on github.com) holds two main branches with an infinite lifetime:

production: the production-ready tutorials
devel: the main branch where the latest developments interviene. This is the default branch you get when you clone the repo.
New tutorial layout

So assuming you have forked this repository to work freely on your own copy of it, you can now feed a new tutorial, assuming you follow the below guidelines.

Directory Layout

<topic>/<name>  # Select the appropriate root directory
├── README.md              # Main tutorial file, in Markdown
├── index.md -> README.md  # Symlink (for mkdocs)
├── slides.pdf             # Slides proposing an overview of the tutorial
├── cover_slides.png       # Picture of the cover of the slide
├── Makefile               # GNU Makefile offering the targets 'fetch', 'compile', 'run' and 'plot'
├── plots                  # Directory hosting the Gnuplots / R plots data
├── runs/                  # Directory hosting the data/logs of the runs
├── scripts/               # Eventually, a directory hosting some specific scripts
└── launcher-<name>.{slurm|oar}.sh # launcher script to be used in the tutorial

# Prepare the appropriate link for ReadtheDocs -- if needed
docs/<topic> -> ../<topic>
# such that 'docs/<topic>/<name>' points to '../<topic>/<name>'
You SHOULD stick to a single README.md file, (using the markdown format) if possible.

Kindly follow the following format for this file (adapt path/to accordingly):

[![By ULHPC](https://img.shields.io/badge/by-ULHPC-blue.svg)](https://hpc.uni.lu) [![Licence](https://img.shields.io/badge/license-GPL--3.0-blue.svg)](http://www.gnu.org/licenses/gpl-3.0.html) [![GitHub issues](https://img.shields.io/github/issues/ULHPC/tutorials.svg)](https://github.com/ULHPC/tutorials/issues/) [![](https://img.shields.io/badge/slides-PDF-red.svg)](https://github.com/ULHPC/tutorials/raw/devel/path/to/slides.pdf) [![Github](https://img.shields.io/badge/sources-github-green.svg)](https://github.com/ULHPC/tutorials/tree/devel/path/to/) [![Documentation Status](http://readthedocs.org/projects/ulhpc-tutorials/badge/?version=latest)](http://ulhpc-tutorials.readthedocs.io/en/latest/path/to/) [![GitHub forks](https://img.shields.io/github/stars/ULHPC/tutorials.svg?style=social&label=Star)](https://github.com/ULHPC/tutorials)

# <the title>

      Copyright (c) 2013-2018 [You name,] UL HPC Team  <hpc-sysadmins@uni.lu>

[![](https://github.com/ULHPC/tutorials/raw/devel/path/to/cover_slides.png)](https://github.com/ULHPC/tutorials/raw/devel/path/to/slides.pdf)

The objective of this tutorial is to cover XXX, in particular:

* objective 1
* objective 2

--------------------
## Pre-requisites ##

Ensure you are able to [connect to the UL HPC clusters](https://hpc-docs.uni.lu/connect/access/)
In particular, recall that the `module` command **is not** available on the access frontends.

Now you'll need to pull the latest changes in your working copy of the [ULHPC/tutorials](https://github.com/ULHPC/tutorials) you should have cloned in `~/git/github.com/ULHPC/tutorials` (see ["preliminaries" tutorial](../../preliminaries/))

``` bash
(access)$> cd ~/git/github.com/ULHPC/tutorials
(access)$> git pull
Now configure a dedicated directory ~/tutorials/<topic> for this session

# return to your home
(access)$> mkdir -p ~/tutorials/<topic>
(access)$> cd ~/tutorials/<topic>
# create a symbolic link to the reference material
(access)$> ln -s ~/git/github.com/ULHPC/tutorials/<path> ref.d
Advanced users (eventually yet strongly recommended), create a Tmux session (see Tmux cheat sheet and tutorial) or GNU Screen session you can recover later. See also "Getting Started" tutorial .

# /!\ Advanced (but recommended) best-practice:
#     Always work within a TMux or GNU Screen session named '<topic>' (Adapt accordingly)
(access-aion)$> tmux new -s HPC-school   # Tmux
(access-iris)$> screen -S HPC-school     # GNU Screen
#  TMux     | GNU Screen | Action
# ----------|------------|----------------------------------------------
#  CTRL+b c | CTRL+a c   | (create) creates a new Screen window. The default Screen number is zero.
#  CTRL+b n | CTRL+a n   | (next) switches to the next window.
#  CTRL+b p | CTRL+a p   | (prev) switches to the previous window.
#  CTRL+b , | CTRL+a A   | (title) rename the current window
#  CTRL+b d | CTRL+a d   | (detach) detaches from a Screen -
# Once detached:
#   tmux ls  | screen -ls : list available screen
#   tmux att | screen -x  : reattach to a past screen
Let's get an interactive jobs:

### Access to ULHPC cluster (if not yet done)
(laptop)$> ssh aion-cluster    # or iris-cluster
### Have an interactive job
# ... either directly
(access)$> si [...]
# ... or using the HPC School reservation 'hpcschool'if needed  - use 'sinfo -T' to check if active and its name
# (access)$> si --reservation=hpcschool [...]
(node)$>
Objective 1

instructions

Objective 2

instructions

Useful references

```

Remember that they shall be understandable for users having no or very few knowledge on your topic!

One proposal to organize the workflow of your tutorial:

Select a typical sample example that will be used throughout all the tutorial, that is easy to fetch from the official page of the software. Adapt the make fetch directive in your root Makefile to perform the corresponding actions.
(eventually) detail how to build the sources (using EasyBuild. Adapt the make build accordingly.
dedicate a section to the running of this example in an interactive job such that the reader has a better understanding of:
the involved modules to load
the classical way to execute the software
etc. Adapt also the make run_interactive accordingly
dedicate a second section to the running of the example in a passive job, typically providing a generic launcher script adapted to your software. You might adapt / extend the UL HPC launcher scripts the same way to extend these tutorials. Adapt also the make run accordingly.
a last section would typically involves hints / elements to benchmark the execution, add tips/tricks to improve the performances (and see the effects of those improvements) and have a way to plot the results. Adapt the make plot accordingly
Semantic Versionning

The operation consisting of releasing a new version of this repository is automated by a set of tasks within the Makefile at the root of this repository.

In this context, a version number have the following format:

  <major>.<minor>.<patch>
where:

< major > corresponds to the major version number
< minor > corresponds to the minor version number
< patch > corresponds to the patching version number
Example: 1.2.0

The current version number is stored in the file VERSION. DO NOT EDIT THIS FILE, use the below primitives to affect the number it contains. For more information on the version, run:

 $> make versioninfo
If a new version number such be bumped, you simply have to run:

 $> make start_bump_{major,minor,patch}
This will start the release process for you using git-flow. Then, to make the release effective, just run:

 $> make release
This will finalize the release using git-flow, create the appropriate tag and merge all things the way they should be.

These tutorials has been designed and implemented in the context of the UL HPC Platform of the University of Luxembourg by the UL HPC Team, i.e.

Dr. Sébastien Varrette, UL HPC Manager
Dr. Frederic Pinel
Dr. Emmanuel Kieffer
Dr. Ezhilmathi Krishnasamy
Dr. Loizos Koutsantonis
Sarah Peter, MSc. (born Diehl)
Hyacinthe Cartiaux
Abatcha Olloh
Teddy Valette
Yet we are grateful to the following (eventually former users/team alumni) persons who helped us to consolidate or complete these tutorials (in alphabetical order):

Dr. Xavier Besseron
Dr. Joseph Emeras
Dr. Aurélien Ginolhac
Maxime Schmitt, MSc.
Clément Parisot, MSc.
Valentin Plugaru, MSc.
P. Wohlschlegel
You can submit bug / issues / feature request using the ULHPC/tutorials Tracker. Alternatively, you can contact the UL HPC Management Team developing these tutorials under the responsibility of Dr. Varrette. Use in that case the following email address: hpc-team@uni.lu